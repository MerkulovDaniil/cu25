<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="ru" xml:lang="ru"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>üíÄ –î–æ–º–∞—à–∫–∞</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./favicon.svg" rel="icon" type="image/svg+xml">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-1a707f9c94ab67871d92b62367baa00b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "–ü–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
    "search-matching-documents-text": "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞",
    "search-copy-link-title": "–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Å—Å—ã–ª–∫—É",
    "search-hide-matches-text": "–°–∫—Ä—ã—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã",
    "search-more-match-text": "–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —ç—Ç–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ",
    "search-more-matches-text": "–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞(-–æ–≤) –≤ —ç—Ç–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ",
    "search-clear-button-title": "–û—á–∏—Å—Ç–∏—Ç—å",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "–û—Ç–º–µ–Ω–∏—Ç—å",
    "search-submit-button-title": "–ù–∞–π—Ç–∏",
    "search-label": "–ü–æ–∏—Å–∫"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js" integrity="sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A" crossorigin="anonymous"></script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="üíÄ –î–æ–º–∞—à–∫–∞">
<meta property="og:description" content="">
<meta property="og:image" content="https://cu25.fmin.xyz/linesearch.svg">
<meta name="twitter:title" content="üíÄ –î–æ–º–∞—à–∫–∞">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://cu25.fmin.xyz/linesearch.svg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./logo.svg" alt="cu25.fmin.xyz" class="navbar-logo light-content">
    <img src="./logo.svg" alt="cu25.fmin.xyz" class="navbar-logo dark-content">
    </a>
  </div>
            <div id="quarto-search" class="" title="–ü–æ–∏—Å–∫"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –Ω–∞–≤–∏–≥–∞—Ü–∏—é" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./program.html"> 
<span class="menu-text">üöÄ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/MerkulovDaniil/cu25" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://www.youtube.com/@fmin" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-youtube"></i></a>
    <a href="https://t.me/fminxyz" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-telegram"></i></a>
    <a href="https://fmin.xyz" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-gem"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ</h2>
   
  <ul>
  <li><a href="#linear-algebra-basics" id="toc-linear-algebra-basics" class="nav-link active" data-scroll-target="#linear-algebra-basics">Linear algebra basics</a></li>
  <li><a href="#convergence-rates" id="toc-convergence-rates" class="nav-link" data-scroll-target="#convergence-rates">Convergence rates</a></li>
  <li><a href="#line-search" id="toc-line-search" class="nav-link" data-scroll-target="#line-search">Line search</a></li>
  <li><a href="#matrix-calculus" id="toc-matrix-calculus" class="nav-link" data-scroll-target="#matrix-calculus">Matrix calculus</a></li>
  <li><a href="#automatic-differentiation-and-jax" id="toc-automatic-differentiation-and-jax" class="nav-link" data-scroll-target="#automatic-differentiation-and-jax">Automatic differentiation and jax</a></li>
  <li><a href="#convexity" id="toc-convexity" class="nav-link" data-scroll-target="#convexity">Convexity</a></li>
  <li><a href="#optimality-conditions.-kkt.-duality" id="toc-optimality-conditions.-kkt.-duality" class="nav-link" data-scroll-target="#optimality-conditions.-kkt.-duality">Optimality conditions. KKT. Duality</a></li>
  <li><a href="#linear-programming" id="toc-linear-programming" class="nav-link" data-scroll-target="#linear-programming">Linear programming</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient Descent</a></li>
  <li><a href="#accelerated-methods" id="toc-accelerated-methods" class="nav-link" data-scroll-target="#accelerated-methods">Accelerated methods</a></li>
  <li><a href="#conjugate-gradients" id="toc-conjugate-gradients" class="nav-link" data-scroll-target="#conjugate-gradients">Conjugate gradients</a></li>
  <li><a href="#newton-and-quasinewton-methods" id="toc-newton-and-quasinewton-methods" class="nav-link" data-scroll-target="#newton-and-quasinewton-methods">Newton and quasinewton methods</a></li>
  <li><a href="#conditional-gradient-methods" id="toc-conditional-gradient-methods" class="nav-link" data-scroll-target="#conditional-gradient-methods">Conditional gradient methods</a></li>
  <li><a href="#subgradient-method" id="toc-subgradient-method" class="nav-link" data-scroll-target="#subgradient-method">Subgradient method</a></li>
  <li><a href="#proximal-gradient-method" id="toc-proximal-gradient-method" class="nav-link" data-scroll-target="#proximal-gradient-method">Proximal gradient method</a></li>
  <li><a href="#stochastic-gradient-methods" id="toc-stochastic-gradient-methods" class="nav-link" data-scroll-target="#stochastic-gradient-methods">Stochastic gradient methods</a></li>
  <li><a href="#neural-network-training" id="toc-neural-network-training" class="nav-link" data-scroll-target="#neural-network-training">Neural network training</a></li>
  <li><a href="#big-models" id="toc-big-models" class="nav-link" data-scroll-target="#big-models">Big models</a></li>
  <li><a href="#admm-dual-methods" id="toc-admm-dual-methods" class="nav-link" data-scroll-target="#admm-dual-methods">ADMM (Dual methods)</a></li>
  <li><a href="#bonus-continuous-time-methods" id="toc-bonus-continuous-time-methods" class="nav-link" data-scroll-target="#bonus-continuous-time-methods">Bonus: Continuous time methods</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/MerkulovDaniil/cu25/edit/main/homework.md" class="toc-action"><i class="bi bi-github"></i>–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>

<header id="title-block-header">
</header>


<section id="linear-algebra-basics" class="level3">
<h3 class="anchored" data-anchor-id="linear-algebra-basics">Linear algebra basics</h3>
<ol type="1">
<li><p>[5 points] <strong>Sensitivity Analysis in Linear Systems</strong> Consider a nonsingular matrix <span class="math inline">A \in \mathbb{R}^{n \times n}</span> and a vector <span class="math inline">b \in \mathbb{R}^n</span>. Suppose that due to measurement or computational errors, the vector <span class="math inline">b</span> is perturbed to <span class="math inline">\tilde{b} = b + \delta b</span>.</p>
<ol type="1">
<li>Derive an upper bound for the relative error in the solution <span class="math inline">x</span> of the system <span class="math inline">Ax = b</span> in terms of the condition number <span class="math inline">\kappa(A)</span> and the relative error in <span class="math inline">b</span>.<br>
</li>
<li>Provide a concrete example using a <span class="math inline">2 \times 2</span> matrix where <span class="math inline">\kappa(A)</span> is large (say, <span class="math inline">\geq 100500</span>).</li>
</ol></li>
<li><p>[5 points] <strong>Effect of Diagonal Scaling on Rank</strong> Let <span class="math inline">A \in \mathbb{R}^{n \times n}</span> be a matrix with rank <span class="math inline">r</span>. Suppose <span class="math inline">D \in \mathbb{R}^{n \times n}</span> is a diagonal matrix. Determine the rank of the product <span class="math inline">DA</span>. Explain your reasoning.</p></li>
<li><p>[8 points] <strong>Unexpected SVD</strong> Compute the Singular Value Decomposition (SVD) of the following matrices:</p>
<ul>
<li><span class="math inline">A_1 = \begin{bmatrix} 2 \\ 2 \\ 8 \end{bmatrix}</span></li>
<li><span class="math inline">A_2 = \begin{bmatrix} 0 &amp; x \\ x &amp; 0 \\ 0 &amp; 0 \end{bmatrix}</span>, where <span class="math inline">x</span> is the sum of your birthdate numbers (day + month).</li>
</ul></li>
<li><p>[10 points] <strong>Effect of normalization on rank</strong> Assume we have a set of data points <span class="math inline">x^{(i)}\in\mathbb{R}^{n},\,i=1,\dots,m</span>, and decide to represent this data as a matrix <span class="math display">
X =
\begin{pmatrix}
  | &amp; &amp; | \\
  x^{(1)} &amp; \dots &amp; x^{(m)} \\
  | &amp; &amp; | \\
\end{pmatrix} \in \mathbb{R}^{n \times m}.
</span></p>
<p>We suppose that <span class="math inline">\text{rank}\,X = r</span>.</p>
<p>In the problem below, we ask you to find the rank of some matrix <span class="math inline">M</span> related to <span class="math inline">X</span>. In particular, you need to find relation between <span class="math inline">\text{rank}\,X = r</span> and <span class="math inline">\text{rank}\,M</span>, e.g., that the rank of <span class="math inline">M</span> is always larger/smaller than the rank of <span class="math inline">X</span> or that <span class="math inline">\text{rank}\,M = \text{rank}\,X \big / 35</span>. Please support your answer with legitimate arguments and make the answer as accurate as possible.</p>
<p>Note that border cases are possible depending on the structure of the matrix <span class="math inline">X</span>. Make sure to cover them in your answer correctly.</p>
<p>In applied statistics and machine learning, data is often normalized. One particularly popular strategy is to subtract the estimated mean <span class="math inline">\mu</span> and divide by the square root of the estimated variance <span class="math inline">\sigma^2</span>. i.e. <span class="math display">
x \rightarrow (x - \mu) \big / \sigma.
</span> After the normalization, we get a new matrix <span class="math display">
\begin{split}
Y &amp;:=
\begin{pmatrix}
  | &amp; &amp; | \\
  y^{(1)} &amp; \dots &amp; y^{(m)} \\
  | &amp; &amp; | \\
\end{pmatrix},\\
y^{(i)} &amp;:= \frac{x^{(i)} - \frac{1}{m}\sum_{j=1}^{m} x^{(j)}}{\sigma}.
\end{split}
</span> What is the rank of <span class="math inline">Y</span> if <span class="math inline">\text{rank} \; X = r</span>? Here <span class="math inline">\sigma</span> is a vector and the division is element-wise. The reason for this is that different features might have different scales. Specifically: <span class="math display">
\sigma_i = \sqrt{\frac{1}{m}\sum_{j=1}^{m} \left(x_i^{(j)}\right)^2 - \left(\frac{1}{m}\sum_{j=1}^{m} x_i^{(j)}\right)^2}.
</span></p></li>
<li><p>[20 points] <strong>Image Compression with Truncated SVD</strong> Explore image compression using Truncated Singular Value Decomposition (SVD). Understand how varying the number of singular values affects the quality of the compressed image. Implement a Python script to compress a grayscale image using Truncated SVD and visualize the compression quality.</p>
<ul>
<li><strong>Truncated SVD</strong>: Decomposes an image <span class="math inline">A</span> into <span class="math inline">U, S,</span> and <span class="math inline">V</span> matrices. The compressed image is reconstructed using a subset of singular values.</li>
<li><strong>Mathematical Representation</strong>: <span class="math display">
  A \approx U_k \Sigma_k V_k^T
  </span>
<ul>
<li><span class="math inline">U_k</span> and <span class="math inline">V_k</span> are the first <span class="math inline">k</span> columns of <span class="math inline">U</span> and <span class="math inline">V</span>, respectively.</li>
<li><span class="math inline">\Sigma_k</span> is a diagonal matrix with the top <span class="math inline">k</span> singular values.</li>
<li><strong>Relative Error</strong>: Measures the fidelity of the compressed image compared to the original. <span class="math display">
  \text{Relative Error} = \frac{\| A - A_k \|}{\| A \|}
  </span></li>
</ul></li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.animation <span class="im">as</span> animation</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skimage <span class="im">import</span> io, color</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> io <span class="im">import</span> BytesIO</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> download_image(url):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> requests.get(url)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> io.imread(BytesIO(response.content))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> color.rgb2gray(img)  <span class="co"># Convert to grayscale</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_plot(i, img_plot, error_plot, U, S, V, original_img, errors, ranks, ax1, ax2):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adjust rank based on the frame index</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">70</span>:</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        rank <span class="op">=</span> i <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        rank <span class="op">=</span> <span class="dv">70</span> <span class="op">+</span> (i <span class="op">-</span> <span class="dv">69</span>) <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    reconstructed_img <span class="op">=</span> ... <span class="co"># YOUR CODE HERE </span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate relative error</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    relative_error <span class="op">=</span> ... <span class="co"># YOUR CODE HERE</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    errors.append(relative_error)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    ranks.append(rank)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update the image plot and title</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    img_plot.set_data(reconstructed_img)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    ax1.set_title(<span class="ss">f"Image compression with SVD</span><span class="ch">\n</span><span class="ss"> Rank </span><span class="sc">{</span>rank<span class="sc">}</span><span class="ss">; Relative error </span><span class="sc">{</span>relative_error<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove axis ticks and labels from the first subplot (ax1)</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    ax1.set_xticks([])</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    ax1.set_yticks([])</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update the error plot</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    error_plot.set_data(ranks, errors)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlim(<span class="dv">1</span>, <span class="bu">len</span>(S))</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    ax2.grid(linestyle<span class="op">=</span><span class="st">":"</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylim(<span class="fl">1e-4</span>, <span class="fl">0.5</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">'Relative Error'</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlabel(<span class="st">'Rank'</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    ax2.set_title(<span class="st">'Relative Error over Rank'</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    ax2.semilogy()</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set xticks to show rank numbers</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    ax2.set_xticks(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(S)<span class="op">+</span><span class="dv">1</span>, <span class="bu">max</span>(<span class="bu">len</span>(S)<span class="op">//</span><span class="dv">10</span>, <span class="dv">1</span>)))  <span class="co"># Adjust the step size as needed</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> img_plot, error_plot</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_animation(image, filename<span class="op">=</span><span class="st">'svd_animation.mp4'</span>):</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    U, S, V <span class="op">=</span> np.linalg.svd(image, full_matrices<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    errors <span class="op">=</span> []</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    ranks <span class="op">=</span> []</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">8</span>))</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    img_plot <span class="op">=</span> ax1.imshow(image, cmap<span class="op">=</span><span class="st">'gray'</span>, animated<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    error_plot, <span class="op">=</span> ax2.plot([], [], <span class="st">'r-'</span>, animated<span class="op">=</span><span class="va">True</span>)  <span class="co"># Initial empty plot for errors</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add watermark</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    ax1.text(<span class="dv">1</span>, <span class="fl">1.02</span>, <span class="st">'@fminxyz'</span>, transform<span class="op">=</span>ax1.transAxes, color<span class="op">=</span><span class="st">'gray'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, ha<span class="op">=</span><span class="st">'right'</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Determine frames for the animation</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    initial_frames <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">70</span>))  <span class="co"># First 70 ranks</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    subsequent_frames <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">70</span>, <span class="bu">len</span>(S), <span class="dv">10</span>))  <span class="co"># Every 10th rank after 70</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    frames <span class="op">=</span> initial_frames <span class="op">+</span> subsequent_frames</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>    ani <span class="op">=</span> animation.FuncAnimation(fig, update_plot, frames<span class="op">=</span><span class="bu">len</span>(frames), fargs<span class="op">=</span>(img_plot, error_plot, U, S, V, image, errors, ranks, ax1, ax2), interval<span class="op">=</span><span class="dv">50</span>, blit<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    ani.save(filename, writer<span class="op">=</span><span class="st">'ffmpeg'</span>, fps<span class="op">=</span><span class="dv">8</span>, dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># URL of the image</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>    url <span class="op">=</span> <span class="st">""</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Download the image and create the animation</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> download_image(url)</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    create_animation(image)</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div></li>
</ol>
</section>
<section id="convergence-rates" class="level3">
<h3 class="anchored" data-anchor-id="convergence-rates">Convergence rates</h3>
<ol type="1">
<li><p>[6 points] Determine (it means to prove the character of convergence if it is convergent) the convergence or divergence of a given sequences</p>
<ul>
<li><span class="math inline">r_{k} = \frac{1}{\sqrt{k+5}}</span>.</li>
<li><span class="math inline">r_{k} = 0.101^k</span>.</li>
<li><span class="math inline">r_{k} = 0.101^{2^k}</span>.</li>
</ul></li>
<li><p>[8 points] Let the sequence <span class="math inline">\{r_k\}</span> be defined by <span class="math display">
r_{k+1} =
\begin{cases}
\frac{1}{2}\,r_k, &amp; \text{if } k \text{ is even}, \\
r_k^2, &amp; \text{if } k \text{ is odd},
\end{cases}
</span> with initial value <span class="math inline">0 &lt; r_0 &lt; 1</span>. Prove that <span class="math inline">\{r_k\}</span> converges to 0 and analyze its convergence rate. In your answer, determine whether the overall convergence is linear, superlinear, or quadratic.</p></li>
<li><p>[6 points] Determine the following sequence <span class="math inline">\{r_k\}</span> by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, determine whether there is quadratic convergence. <span class="math display">
r_k = \dfrac{1}{k!}
</span></p></li>
<li><p>[8 points] Consider the recursive sequence defined by <span class="math display">
r_{k+1} = \lambda\,r_k + (1-\lambda)\,r_k^p,\quad k\ge0,
</span> where <span class="math inline">\lambda\in [0,1)</span> and <span class="math inline">p&gt;1</span>. Which additional conditions on <span class="math inline">r_0</span> should be satisfied for the sequence to converge? Show that when <span class="math inline">\lambda&gt;0</span> the sequence converges to 0 with a linear rate (with asymptotic constant <span class="math inline">\lambda</span>), and when <span class="math inline">\lambda=0</span> determine the convergence rate in terms of <span class="math inline">p</span>. In particular, for <span class="math inline">p=2</span> decide whether the convergence is quadratic.</p></li>
</ol>
</section>
<section id="line-search" class="level3">
<h3 class="anchored" data-anchor-id="line-search">Line search</h3>
<ol type="1">
<li><p>[10 points] Consider a strongly convex quadratic function <span class="math inline">f: \mathbb{R}^n \rightarrow \mathbb{R}</span>, and let us start from a point <span class="math inline">x_k \in \mathbb{R}^n</span> moving in the direction of the antigradient <span class="math inline">-\nabla f(x_k)</span>, note that <span class="math inline">\nabla f(x_k)\neq 0</span>. Show that the minimum of <span class="math inline">f</span> along this direction as a function of the step size <span class="math inline">\alpha</span>, for a decreasing function at <span class="math inline">x_k</span>, satisfies Armijo‚Äôs condition for any <span class="math inline">c_1</span> in the range <span class="math inline">0 \leq c_1 \leq \frac{1}{2}</span>. Specifically, demonstrate that the following inequality holds at the optimal <span class="math inline">\alpha^*</span>: <span class="math display">
\varphi(\alpha) = f(x_{k+1}) = f(x_k - \alpha \nabla f(x_k)) \leq f(x_k) - c_1 \alpha \|\nabla f(x_k)\|_2^2
</span></p></li>
<li><p><strong>Implementing and Testing Line Search Conditions in Gradient Descent</strong> [36 points] <span class="math display">
x_{k+1} = x_k - \alpha \nabla f(x_k)
</span> In this assignment, you will modify an existing Python code for gradient descent to include various line search conditions. You will test these modifications on two functions: a quadratic function and the Rosenbrock function. The main objectives are to understand how different line search strategies influence the convergence of the gradient descent algorithm and to compare their efficiencies based on the number of function evaluations.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize_scalar</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">214</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the quadratic function and its gradient</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quadratic_function(x, A, b):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> np.dot(x.T, np.dot(A, x)) <span class="op">-</span> np.dot(b.T, x)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad_quadratic(x, A, b):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(A, x) <span class="op">-</span> b</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a 2D quadratic problem with a specified condition number</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_quadratic_problem(cond_number):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Random symmetric matrix</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> np.random.randn(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> np.dot(M, M.T)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ensure the matrix has the desired condition number</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    U, s, V <span class="op">=</span> np.linalg.svd(M)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> np.linspace(cond_number, <span class="dv">1</span>, <span class="bu">len</span>(s))  <span class="co"># Spread the singular values</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.dot(U, np.dot(np.diag(s), V))</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Random b</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> np.random.randn(<span class="dv">2</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> A, b</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient descent function</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(start_point, A, b, stepsize_func, max_iter<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> start_point.copy()</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    trajectory <span class="op">=</span> [x.copy()]</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> grad_quadratic(x, A, b)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        step_size <span class="op">=</span> stepsize_func(x, grad)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">-=</span> step_size <span class="op">*</span> grad</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        trajectory.append(x.copy())</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(trajectory)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Backtracking line search strategy using scipy</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backtracking_line_search(x, grad, A, b, alpha<span class="op">=</span><span class="fl">0.3</span>, beta<span class="op">=</span><span class="fl">0.8</span>):</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> objective(t):</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> quadratic_function(x <span class="op">-</span> t <span class="op">*</span> grad, A, b)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> minimize_scalar(objective, method<span class="op">=</span><span class="st">'golden'</span>)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res.x</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ill-posed problem</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>cond_number <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>A, b <span class="op">=</span> generate_quadratic_problem(cond_number)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Starting point</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>start_point <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">1.8</span>])</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform gradient descent with both strategies</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>trajectory_fixed <span class="op">=</span> gradient_descent(start_point, A, b, <span class="kw">lambda</span> x, g: <span class="fl">5e-2</span>)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>trajectory_backtracking <span class="op">=</span> gradient_descent(start_point, A, b, <span class="kw">lambda</span> x, g: backtracking_line_search(x, g, A, b))</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the trajectories on a contour plot</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>x1, x2 <span class="op">=</span> np.meshgrid(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">400</span>), np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">400</span>))</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.array([quadratic_function(np.array([x, y]), A, b) <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(x1.flatten(), x2.flatten())]).reshape(x1.shape)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>plt.contour(x1, x2, Z, levels<span class="op">=</span><span class="dv">50</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>plt.plot(trajectory_fixed[:, <span class="dv">0</span>], trajectory_fixed[:, <span class="dv">1</span>], <span class="st">'o-'</span>, label<span class="op">=</span><span class="st">'Fixed Step Size'</span>)</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>plt.plot(trajectory_backtracking[:, <span class="dv">0</span>], trajectory_backtracking[:, <span class="dv">1</span>], <span class="st">'o-'</span>, label<span class="op">=</span><span class="st">'Backtracking Line Search'</span>)</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a><span class="co"># Add markers for start and optimal points</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>plt.plot(start_point[<span class="dv">0</span>], start_point[<span class="dv">1</span>], <span class="st">'ro'</span>, label<span class="op">=</span><span class="st">'Start Point'</span>)</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>optimal_point <span class="op">=</span> np.linalg.solve(A, b)</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>plt.plot(optimal_point[<span class="dv">0</span>], optimal_point[<span class="dv">1</span>], <span class="st">'y*'</span>, markersize<span class="op">=</span><span class="dv">15</span>, label<span class="op">=</span><span class="st">'Optimal Point'</span>)</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Gradient Descent Trajectories on Quadratic Function'</span>)</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x1'</span>)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'x2'</span>)</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">"linesearch.svg"</span>)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="linesearch.svg" class="img-fluid figure-img"></p>
<figcaption>The code above plots this</figcaption>
</figure>
</div>
<p>Start by reviewing the provided Python code. This code implements gradient descent with a fixed step size and a backtracking line search on a quadratic function. Familiarize yourself with how the gradient descent function and the step size strategies are implemented.</p>
<ol type="1">
<li><p>[10/36 points] Modify the gradient descent function to include the following line search conditions:</p>
<ol type="1">
<li>Dichotomy</li>
<li>Sufficient Decrease Condition</li>
<li>Wolfe Condition</li>
<li>Polyak step size <span class="math display">
\alpha_k = \frac{f(x_k) - f^*}{\|\nabla f(x_k)\|_2^2},
</span> where <span class="math inline">f^*</span> is the optimal value of the function. It seems strange to use the optimal value of the function in the step size, but there are options to estimate it even without knowing the optimal value.<br>
</li>
<li>Sign Gradient Method: <span class="math display">
\alpha_k = \frac{1}{\|\nabla f(x_k)\|_2},
</span> Test your modified gradient descent algorithm with the implemented step size search conditions on the provided quadratic function. Plot the trajectories over iterations for each condition. Choose and specify hyperparameters for inexact line search conditions. Choose and specify the <strong>termination criterion</strong>. Start from the point <span class="math inline">x_0 = (-1, 2)^T</span>.</li>
</ol></li>
<li><p>[8/36 points] Compare these 7 methods from the budget perspective. Plot the graph of function value from the number of function evaluations for each method on the same graph.</p></li>
<li><p>[10/36 points] Plot trajectory for another function with the same set of methods <span class="math display">
f(x_1, x_2) =  10(x_2 ‚àí x_1^2)^2 + (x_1 ‚àí 1)^2
</span> with <span class="math inline">x_0 = (-1, 2)^T</span>. You might need to adjust hyperparameters.</p></li>
<li><p>[8/36 points] Plot the same function value from the number of function calls for this experiment.</p></li>
</ol></li>
</ol>
</section>
<section id="matrix-calculus" class="level3">
<h3 class="anchored" data-anchor-id="matrix-calculus">Matrix calculus</h3>
<ol type="1">
<li><p>[6 points] Find the gradient <span class="math inline">\nabla f(x)</span> and hessian <span class="math inline">f^{\prime\prime}(x)</span>, if <span class="math inline">f(x) = \frac{1}{2}\Vert A - xx^T\Vert ^2_F, A \in \mathbb{S}^n</span></p></li>
<li><p>[6 points] Find the gradient <span class="math inline">\nabla f(x)</span> and hessian <span class="math inline">f''(x)</span>, if <span class="math inline">f(x) = \dfrac{1}{2} \Vert Ax - b\Vert^2_2</span>.</p></li>
<li><p>[8 points] Find the gradient <span class="math inline">\nabla f(x)</span> and hessian <span class="math inline">f''(x)</span>, if <span class="math display">
f(x) = \frac1m \sum\limits_{i=1}^m \log \left( 1 + \exp(a_i^{T}x) \right) + \frac{\mu}{2}\Vert x\Vert _2^2, \; a_i, x \in \mathbb R^n, \; \mu&gt;0
</span></p></li>
<li><p>[8 points] Compute the gradient <span class="math inline">\nabla_A f(A)</span> of the trace of the matrix exponential function <span class="math inline">f(A) = \text{tr}(e^A)</span> with respect to <span class="math inline">A</span>. Hint: Use the definition of the matrix exponential. Use the definition of the differential <span class="math inline">df = f(A + dA) - f(A) + o(\Vert dA \Vert)</span> with the limit <span class="math inline">\Vert dA \Vert \to 0</span>.</p></li>
<li><p>[20 points] <strong>Principal Component Analysis through gradient calculation.</strong> Let there be a dataset <span class="math inline">\{x_i\}_{i=1}^N, x_i \in \mathbb{R}^D</span>, which we want to transform into a dataset of reduced dimensionality <span class="math inline">d</span> using projection onto a linear subspace defined by the matrix <span class="math inline">P \in \mathbb{R}^{D \times d}</span>. The orthogonal projection of a vector <span class="math inline">x</span> onto this subspace can be computed as <span class="math inline">P(P^TP)^{-1}P^Tx</span>. To find the optimal matrix <span class="math inline">P</span>, consider the following optimization problem: <span class="math display">
F(P) = \sum_{i=1}^N \|x_i - P(P^TP)^{-1}P^Tx_i\|^2 = N \cdot \text{tr}\left((I - P(P^TP)^{-1}P^T)^2 S\right) \to \min_{P \in \mathbb{R}^{D \times d}},
</span> where <span class="math inline">S = \frac{1}{N} \sum_{i=1}^N x_i x_i^T</span> is the sample covariance matrix for the normalized dataset.</p>
<ol type="1">
<li><p>Find the gradient <span class="math inline">\nabla_P F(P)</span>, calculated for an arbitrary matrix <span class="math inline">P</span> with orthogonal columns, i.e., <span class="math inline">P : P^T P = I</span>.</p>
<p><em>Hint: When calculating the differential <span class="math inline">dF(P)</span>, first treat <span class="math inline">P</span> as an arbitrary matrix, and then use the orthogonality property of the columns of <span class="math inline">P</span> in the resulting expression.</em></p></li>
<li><p>Consider the eigendecomposition of the matrix <span class="math inline">S</span>: <span class="math display">
S = Q \Lambda Q^T,
</span> where <span class="math inline">\Lambda</span> is a diagonal matrix with eigenvalues on the diagonal, and <span class="math inline">Q = [q_1 | q_2 | \ldots | q_D] \in \mathbb{R}^{D \times D}</span> is an orthogonal matrix consisting of eigenvectors <span class="math inline">q_i</span> as columns. Prove the following:</p>
<ol type="1">
<li>The gradient <span class="math inline">\nabla_P F(P)</span> equals zero for any matrix <span class="math inline">P</span> composed of <span class="math inline">d</span> distinct eigenvectors <span class="math inline">q_i</span> as its columns.</li>
<li>The minimum value of <span class="math inline">F(P)</span> is achieved for the matrix <span class="math inline">P</span> composed of eigenvectors <span class="math inline">q_i</span> corresponding to the largest eigenvalues of <span class="math inline">S</span>.</li>
</ol></li>
</ol></li>
</ol>
</section>
<section id="automatic-differentiation-and-jax" class="level3">
<h3 class="anchored" data-anchor-id="automatic-differentiation-and-jax">Automatic differentiation and jax</h3>
<ol type="1">
<li><p><strong>Benchmarking Hessian-Vector Product (HVP) Computation in a Neural Network via JAX</strong> [22 points]</p>
<p>You are given a simple neural network model (an MLP with several hidden layers using a nonlinearity such as GELU). The model‚Äôs parameters are defined by the weights of its layers. Your task is to compare different approaches for computing the Hessian-vector product (HVP) with respect to the model‚Äôs loss and to study how the computation time scales as the model grows in size.</p>
<p><strong>Model and Loss Definition:</strong> [2/22 points] Here is the code for the model and loss definition. Write a method <code>get_params()</code> that returns the flattened vector of all model weights.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax.nn <span class="im">import</span> gelu</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ MLP –º–æ–¥–µ–ª–∏</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP:</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, key, layer_sizes):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_sizes <span class="op">=</span> layer_sizes</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        keys <span class="op">=</span> jax.random.split(key, <span class="bu">len</span>(layer_sizes) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> [</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>            jax.random.normal(k, (layer_sizes[i], layer_sizes[i <span class="op">+</span> <span class="dv">1</span>]))</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i, k <span class="kw">in</span> <span class="bu">enumerate</span>(keys)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> w <span class="kw">in</span> <span class="va">self</span>.weights[:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> gelu(jnp.dot(x, w))</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jnp.dot(x, <span class="va">self</span>.weights[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_params(<span class="va">self</span>):</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">### YOUR CODE HERE </span><span class="al">###</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Hessian and HVP Implementations:</strong> [2/22 points] Write a function</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ì–µ—Å—Å–∏–∞–Ω–∞</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_hessian(model, params):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss_fn(p):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> jnp.ones((<span class="dv">1</span>, model.layer_sizes[<span class="dv">0</span>]))  <span class="co"># –ó–∞–≥–ª—É—à–∫–∞ –≤—Ö–æ–¥–∞</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jnp.<span class="bu">sum</span>(model.forward(x))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">### YOUR CODE HERE </span><span class="al">###</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">#hessian_fn =           </span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hessian_fn(params)</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div>
<p>that computes the full Hessian <span class="math inline">H</span> of the loss function with respect to the model parameters using JAX‚Äôs automatic differentiation.</p>
<p><strong>Naive HVP via Full Hessian:</strong> [2/22 points] Write a function <code>naive_hvp(hessian, vector)</code> that, given a precomputed Hessian <span class="math inline">H</span> and a vector <span class="math inline">v</span> (of the same shape as the parameters), computes the Hessian-vector product using a straightforward matrix-vector multiplication.</p>
<p><strong>Efficient HVP Using Autograd:</strong> [4/22 points] Write a function <code>python   def hvp(f, x, v):       return jax.grad(lambda x: jnp.vdot(jax.grad(f)(x), v))(x)</code> that directly computes the HVP without explicitly forming the full Hessian. This leverages the reverse-mode differentiation capabilities of JAX.</p>
<p><strong>Timing Experiment:</strong> Consider a family of models with an increasing number of hidden layers.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>ns <span class="op">=</span> np.linspace(<span class="dv">50</span>, <span class="dv">1000</span>, <span class="dv">15</span>, dtype<span class="op">=</span><span class="bu">int</span>)  <span class="co"># The number of hidden layers</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>num_runs <span class="op">=</span> <span class="dv">10</span>  <span class="co"># The number of runs for averaging</span></span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div>
<p>For each model configuration:</p>
<ul>
<li>Generate the model and extract its parameter vector.</li>
<li>Generate a random vector <span class="math inline">v</span> of the same dimension as the parameters.</li>
<li>Measure (do not forget to use <code>.block_until_ready()</code> to ensure accurate timing and proper synchronization) the following:
<ol type="1">
<li><strong>Combined Time (Full Hessian + Naive HVP):</strong> The total time required to compute the full Hessian and then perform the matrix-vector multiplication.</li>
<li><strong>Naive HVP Time (Excluding Hessian Computation):</strong> The time required to perform the matrix-vector multiplication given a precomputed Hessian.</li>
<li><strong>Efficient HVP Time:</strong> The time required to compute the HVP using the autograd-based function.</li>
</ol></li>
<li>Repeat each timing measurement for a fixed number of runs (e.g., 10 runs) and record both the mean and standard deviation of the computation times.</li>
</ul>
<p><strong>Visualization and Analysis:</strong> [12/22 points]</p>
<ul>
<li>Plot the timing results for the three methods on the same graph. For each method, display error bars corresponding to the standard deviation over the runs.</li>
<li>Label the axes clearly (e.g., ‚ÄúNumber of Layers‚Äù vs.&nbsp;‚ÄúComputation Time (seconds)‚Äù) and include a legend indicating which curve corresponds to which method.</li>
<li>Analyze the scaling behavior. Try analytically derive the scaling of the methods and compare it with the experimental results.</li>
</ul></li>
<li><p>[15 points] We can use automatic differentiation not only to calculate necessary gradients but also for tuning hyperparameters of the algorithm like learning rate in gradient descent (with gradient descent ü§Ø). Suppose, we have the following function <span class="math inline">f(x) = \frac{1}{2}\Vert x\Vert^2</span>, select a random point <span class="math inline">x_0 \in \mathbb{B}^{1000} = \{0 \leq x_i \leq 1 \mid \forall i\}</span>. Consider <span class="math inline">10</span> steps of the gradient descent starting from the point <span class="math inline">x_0</span>: <span class="math display">
x_{k+1} = x_k - \alpha_k \nabla f(x_k)
</span> Your goal in this problem is to write the function, that takes <span class="math inline">10</span> scalar values <span class="math inline">\alpha_i</span> and return the result of the gradient descent on function <span class="math inline">L = f(x_{10})</span>. And optimize this function using gradient descent on <span class="math inline">\alpha \in \mathbb{R}^{10}</span>. Suppose that each of <span class="math inline">10</span> components of <span class="math inline">\alpha</span> is uniformly distributed on <span class="math inline">[0; 0.1]</span>. <span class="math display">
\alpha_{k+1} = \alpha_k - \beta \frac{\partial L}{\partial \alpha}
</span> Choose any constant <span class="math inline">\beta</span> and the number of steps you need. Describe the obtained results. How would you understand, that the obtained schedule (<span class="math inline">\alpha \in \mathbb{R}^{10}</span>) becomes better than it was at the start? How do you check numerically local optimality in this problem?</p></li>
</ol>
</section>
<section id="convexity" class="level3">
<h3 class="anchored" data-anchor-id="convexity">Convexity</h3>
<ol type="1">
<li><p>[10 points] Show that this function is convex.: <span class="math display">
f(x, y, z) = z \log \left(e^{\frac{x}{z}} + e^{\frac{y}{z}}\right) + (z - 2)^2 + e^{\frac{1}{x + y}}
</span> where the function <span class="math inline">f : \mathbb{R}^3 \to \mathbb{R}</span> has its domain defined as: <span class="math display">
\text{dom } f = \{ (x, y, z) \in \mathbb{R}^3 : x + y &gt; 0, \, z &gt; 0 \}.
</span></p></li>
<li><p>[5 points] The center of mass of a body is an important concept in physics (mechanics). For a system of material points with masses <span class="math inline">m_i</span> and coordinates <span class="math inline">x_i</span>, the center of mass is given by: <span class="math display">
x_c = \frac{\sum_{i=1}^k m_i x_i}{\sum_{i=1}^k m_i}
</span> The center of mass of a body does not always lie inside the body. For example, the center of mass of a doughnut is located in its hole. Prove that the center of mass of a system of material points lies in the convex hull of the set of these points.</p></li>
<li><p>[8 points] Show, that <span class="math inline">\mathbf{conv}\{xx^\top: x \in \mathbb{R}^n, \Vert x\Vert  = 1\} = \{A \in \mathbb{S}^n_+: \text{tr}(A) = 1\}</span>.</p></li>
<li><p>[5 points] Prove that the set of <span class="math inline">\{x \in \mathbb{R}^2 \mid e^{x_1}\le x_2\}</span> is convex.</p></li>
<li><p>[8 points] Consider the function <span class="math inline">f(x) = x^d</span>, where <span class="math inline">x \in \mathbb{R}_{+}</span>. Fill the following table with ‚úÖ or ‚ùé. Explain your answers (with proofs).</p>
<div class="table-responsive">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">d</span></th>
<th style="text-align: center;">Convex</th>
<th style="text-align: center;">Concave</th>
<th style="text-align: center;">Strictly Convex</th>
<th style="text-align: center;"><span class="math inline">\mu</span>-strongly convex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">-2, x \in \mathbb{R}_{++}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">-1, x \in \mathbb{R}_{++}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">0</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">0.5</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">1</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\in (1; 2)</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">2</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">&gt; 2</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div></li>
<li><p>[6 points] Prove that the entropy function, defined as <span class="math display">
f(x) = -\sum_{i=1}^n x_i \log(x_i),
</span> with <span class="math inline">\text{dom}(f) = \{x \in \R^n_{++} : \sum_{i=1}^n x_i = 1\}</span>, is strictly concave.</p></li>
<li><p>[8 points] Show that the maximum of a convex function <span class="math inline">f</span> over the polyhedron <span class="math inline">P = \text{conv}\{v_1, \ldots, v_k\}</span> is achieved at one of its vertices, i.e., <span class="math display">
\sup_{x \in P} f(x) = \max_{i=1, \ldots, k} f(v_i).
</span></p>
<p>A stronger statement is: the maximum of a convex function over a closed bounded convex set is achieved at an extreme point, i.e., a point in the set that is not a convex combination of any other points in the set. (you do not have to prove it). <em>Hint:</em> Assume the statement is false, and use Jensen‚Äôs inequality.</p></li>
<li><p>[6 points] Show, that the two definitions of <span class="math inline">\mu</span>-strongly convex functions are equivalent:</p>
<ol type="1">
<li><p><span class="math inline">f(x)</span> is <span class="math inline">\mu</span>-strongly convex <span class="math inline">\iff</span> for any <span class="math inline">x_1, x_2 \in S</span> and <span class="math inline">0 \le \lambda \le 1</span> for some <span class="math inline">\mu &gt; 0</span>: <span class="math display">
f(\lambda x_1 + (1 - \lambda)x_2) \le \lambda f(x_1) + (1 - \lambda)f(x_2) - \frac{\mu}{2} \lambda (1 - \lambda)\|x_1 - x_2\|^2
</span></p></li>
<li><p><span class="math inline">f(x)</span> is <span class="math inline">\mu</span>-strongly convex <span class="math inline">\iff</span> if there exists <span class="math inline">\mu&gt;0</span> such that the function <span class="math inline">f(x) - \dfrac{\mu}{2}\Vert x\Vert^2</span> is convex.</p></li>
</ol></li>
</ol>
</section>
<section id="optimality-conditions.-kkt.-duality" class="level3">
<h3 class="anchored" data-anchor-id="optimality-conditions.-kkt.-duality">Optimality conditions. KKT. Duality</h3>
<p>In this section, you can consider either the arbitrary norm or the Euclidian norm if nothing else is specified.</p>
<ol type="1">
<li><p><strong>Toy example</strong> [10 points] <span class="math display">
\begin{split}
&amp; x^2 + 1 \to \min\limits_{x \in \mathbb{R} }\\
\text{s.t. } &amp; (x-2)(x-4) \leq 0
\end{split}
</span></p>
<ol type="1">
<li>Give the feasible set, the optimal value, and the optimal solution.</li>
<li>Plot the objective <span class="math inline">x^2 +1</span> versus <span class="math inline">x</span>. On the same plot, show the feasible set, optimal point, and value, and plot the Lagrangian <span class="math inline">L(x,\mu)</span> versus <span class="math inline">x</span> for a few positive values of <span class="math inline">\mu</span>. Verify the lower bound property (<span class="math inline">p^* \geq \inf_x L(x, \mu)</span>for <span class="math inline">\mu \geq 0</span>). Derive and sketch the Lagrange dual function <span class="math inline">g</span>.</li>
<li>State the dual problem, and verify that it is a concave maximization problem. Find the dual optimal value and dual optimal solution <span class="math inline">\mu^*</span>. Does strong duality hold?</li>
<li>Let <span class="math inline">p^*(u)</span> denote the optimal value of the problem</li>
</ol>
<p><span class="math display">
\begin{split}
&amp; x^2 + 1 \to \min\limits_{x \in \mathbb{R} }\\
\text{s.t. } &amp; (x-2)(x-4) \leq u
\end{split}
</span></p>
<p>as a function of the parameter <span class="math inline">u</span>. Plot <span class="math inline">p^*(u)</span>. Verify that <span class="math inline">\dfrac{dp^*(0)}{du} = -\mu^*</span></p></li>
<li><p>Consider a smooth convex function <span class="math inline">f(x)</span> at some point <span class="math inline">x_k</span>. One can define the first-order Taylor expansion of the function as: <span class="math display">
f^I_{x_k}(x) = f(x_k) + \nabla f(x_k)^\top (x - x_k),
</span> where we can define <span class="math inline">\delta x = x - x_k</span> and <span class="math inline">g = \nabla f(x_k)</span>. Thus, the expansion can be rewritten as: <span class="math display">
f^I_{x_k}(\delta x) = f(x_k) + g^\top \delta x.
</span> Suppose, we would like to design the family of optimization methods that will be defined as: <span class="math display">
x_{k+1} = \text{arg}\min_{x} \left\{f^I_{x_k}(\delta x) + \frac{\lambda}{2} \|\delta x\|^2\right\},
</span> where <span class="math inline">\lambda &gt; 0</span> is a parameter.</p>
<ol type="1">
<li>[5 points] Show, that this method is equivalent to the gradient descent method with the choice of Euclidean norm of the vector <span class="math inline">\|\delta x\| = \|\delta x\|_2</span>. Find the corresponding learning rate.</li>
<li>[5 points] Prove, that the following holds: <span class="math display">
\text{arg}\min_{\delta x \in \mathbb{R}^n} \left\{ g^T\delta x + \frac{\lambda}{2} \|\delta x\|^2\right\} = - \frac{\|g\|_*}{\lambda} \text{arg}\max_{\|t\|=1} \left\{ t^T g \right\},
</span> where <span class="math inline">\|g\|_*</span> is the <a href="https://fmin.xyz/docs/theory/Dual%20norm.html">dual norm</a> of <span class="math inline">g</span>.</li>
<li>[3 points] Consider another vector norm <span class="math inline">\|\delta x\| = \|\delta x\|_\infty</span>. Write down explicit expression for the corresponding method.</li>
<li>[2 points] Consider induced operator matrix norm for any matrix <span class="math inline">W \in \mathbb{R}^{d_{out} \times d_{in}}</span> <span class="math display">
\|W\|_{\alpha \to \beta} = \max_{x \in \mathbb{R}^{d_{in}}} \frac{\|Wx\|_{\beta}}{\|x\|_{\alpha}}.
</span> Typically, when we solve optimization problems in deep learning, we stack the weight matrices for all layers <span class="math inline">l = [1, L]</span> into a single vector. <span class="math display">
w = \text{vec}(W_1, W_2, \ldots, W_L) \in \mathbb{R}^{n},
</span> Can you write down the explicit expression, that relates <span class="math display">
\|w\|_\infty \qquad \text{ and } \qquad \|W_l\|_{\alpha \to \beta}, \; l = [1, L]?
</span></li>
</ol></li>
<li><p>[10 points] Derive the dual problem for the Ridge regression problem with <span class="math inline">A \in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m, \lambda &gt; 0</span>:</p>
<p><span class="math display">
\begin{split}
\dfrac{1}{2}\|y-b\|^2 + \dfrac{\lambda}{2}\|x\|^2 &amp;\to \min\limits_{x \in \mathbb{R}^n, y \in \mathbb{R}^m }\\
\text{s.t. } &amp; y = Ax
\end{split}
</span></p></li>
<li><p>[20 points] Derive the dual problem for the support vector machine problem with <span class="math inline">A \in \mathbb{R}^{m \times n}, \mathbf{1} \in \mathbb{R}^m \in \mathbb{R}^m, \lambda &gt; 0</span>:</p>
<p><span class="math display">
\begin{split}
\langle \mathbf{1}, t\rangle + \dfrac{\lambda}{2}\|x\|^2 &amp;\to \min\limits_{x \in \mathbb{R}^n, t \in \mathbb{R}^m }\\
\text{s.t. } &amp; Ax \succeq \mathbf{1} - t \\
&amp; t \succeq 0
\end{split}
</span></p></li>
<li><p>[10 points] Give an explicit solution to the following LP.</p>
<p><span class="math display">
\begin{split}
&amp; c^\top x \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; 1^\top x = 1, \\
&amp; x \succeq 0
\end{split}
</span></p>
<p>This problem can be considered the simplest portfolio optimization problem.</p></li>
<li><p>[20 points] Show, that the following problem has a unique solution and find it:</p>
<p><span class="math display">
\begin{split}
&amp; \langle C^{-1}, X\rangle - \log \det X \to \min\limits_{x \in \mathbb{R}^{n \times n} }\\
\text{s.t. } &amp; \langle Xa, a\rangle \leq 1,
\end{split}
</span></p>
<p>where <span class="math inline">C \in \mathbb{S}^n_{++}, a \in \mathbb{R}^n \neq 0</span>. The answer should not involve inversion of the matrix <span class="math inline">C</span>.</p></li>
<li><p>[20 points] Give an explicit solution to the following QP.</p>
<p><span class="math display">
\begin{split}
&amp; c^\top x \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; (x - x_c)^\top A (x - x_c) \leq 1,
\end{split}
</span></p>
<p>where <span class="math inline">A \in \mathbb{S}^n_{++}, c \neq 0, x_c \in \mathbb{R}^n</span>.</p></li>
<li><p>[10 points] Consider the equality-constrained least-squares problem</p>
<p><span class="math display">
\begin{split}
&amp; \|Ax - b\|_2^2 \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; Cx = d,
\end{split}
</span></p>
<p>where <span class="math inline">A \in \mathbb{R}^{m \times n}</span> with <span class="math inline">\mathbf{rank }A = n</span>, and <span class="math inline">C \in \mathbb{R}^{k \times n}</span> with <span class="math inline">\mathbf{rank }C = k</span>. Give the KKT conditions, and derive expressions for the primal solution <span class="math inline">x^*</span> and the dual solution <span class="math inline">\lambda^*</span>.</p></li>
<li><p><strong>Supporting hyperplane interpretation of KKT conditions</strong>. [10 points] Consider a <strong>convex</strong> problem with no equality constraints</p>
<p><span class="math display">
\begin{split}
&amp; f_0(x) \to \min\limits_{x \in \mathbb{R}^n }\\
\text{s.t. } &amp; f_i(x) \leq 0, \quad i = [1,m]
\end{split}
</span></p>
<p>Assume, that <span class="math inline">\exists x^* \in \mathbb{R}^n, \mu^* \in \mathbb{R}^m</span> satisfy the KKT conditions</p>
<p><span class="math display">
\begin{split}
&amp; \nabla_x L (x^*, \mu^*) = \nabla f_0(x^*) + \sum\limits_{i=1}^m\mu_i^*\nabla f_i(x^*) = 0 \\
&amp; \mu^*_i \geq 0, \quad i = [1,m] \\
&amp; \mu^*_i f_i(x^*) = 0, \quad i = [1,m]\\
&amp; f_i(x^*) \leq 0, \quad i = [1,m]
\end{split}
</span></p>
<p>Show that</p>
<p><span class="math display">
\nabla f_0(x^*)^\top (x - x^*) \geq 0
</span></p>
<p>for all feasible <span class="math inline">x</span>. In other words, the KKT conditions imply the simple optimality criterion or <span class="math inline">\nabla f_0(x^*)</span> defines a supporting hyperplane to the feasible set at <span class="math inline">x^*</span>.</p></li>
<li><p><strong>A penalty method for equality constraints.</strong> [10 points] We consider the problem of minimization</p>
<p><span class="math display">
\begin{split}
&amp; f_0(x) \to \min\limits_{x \in \mathbb{R}^{n} }\\
\text{s.t. } &amp; Ax = b,
\end{split}
</span></p>
<p>where $f_0(x): ^n $ is convex and differentiable, and <span class="math inline">A \in \mathbb{R}^{m \times n}</span> with <span class="math inline">\mathbf{rank }A = m</span>. In a quadratic penalty method, we form an auxiliary function</p>
<p><span class="math display">
\phi(x) = f_0(x) + \alpha \|Ax - b\|_2^2,
</span></p>
<p>where <span class="math inline">\alpha &gt; 0</span> is a parameter. This auxiliary function consists of the objective plus the penalty term <span class="math inline">\alpha \Vert Ax - b\Vert_2^2</span>. The idea is that a minimizer of the auxiliary function, <span class="math inline">\tilde{x}</span>, should be an approximate solution to the original problem. Intuition suggests that the larger the penalty weight <span class="math inline">\alpha</span>, the better the approximation <span class="math inline">\tilde{x}</span> to a solution of the original problem. Suppose <span class="math inline">\tilde{x}</span> is a minimizer of <span class="math inline">\phi(x)</span>. Show how to find, from <span class="math inline">\tilde{x}</span>, a dual feasible point for the original problem. Find the corresponding lower bound on the optimal value of the original problem.</p></li>
</ol>
</section>
<section id="linear-programming" class="level3">
<h3 class="anchored" data-anchor-id="linear-programming">Linear programming</h3>
<ol type="1">
<li><p><strong>üì±üéßüíª Covers manufacturing.</strong> [20 points] Lyzard Corp is producing covers for the following products:</p>
<ul>
<li>üì± phones</li>
<li>üéß headphones</li>
<li>üíª laptops</li>
</ul>
<p>The company‚Äôs production facilities are such that if we devote the entire production to headphone covers, we can produce 5000 of them in one day. If we devote the entire production to phone covers or laptop covers, we can produce 4000 or 2000 of them in one day.</p>
<p>The production schedule is one week (6 working days), and the week‚Äôs production must be stored before distribution. Storing 1000 headphone covers (packaging included) takes up 30 cubic feet of space. Storing 1000 phone covers (packaging included) takes up 50 cubic feet of space, and storing 1000 laptop covers (packaging included) takes up 200 cubic feet of space. The total storage space available is 1500 cubic feet.</p>
<p>Due to commercial agreements with Lyzard Corp has to deliver at least 6000 headphone covers and 4000 laptop covers per week to strengthen the product‚Äôs diffusion.</p>
<p>The marketing department estimates that the weekly demand for headphones covers, phone, and laptop covers does not exceed 15000, 12000 and 8000 units, therefore the company does not want to produce more than these amounts for headphones, phone, and laptop covers.</p>
<p>Finally, the net profit per headphone cover, phone cover, and laptop cover are $5, $7, and $12, respectively.</p>
<p>The aim is to determine a weekly production schedule that maximizes the total net profit.</p>
<ol type="1">
<li><p>Write a Linear Programming formulation for the problem. Use the following variables:</p>
<ul>
<li><span class="math inline">y_1</span> = number of headphones covers produced over the week,<br>
</li>
<li><span class="math inline">y_2</span> = number of phone covers produced over the week,<br>
</li>
<li><span class="math inline">y_3</span> = number of laptop covers produced over the week.</li>
</ul></li>
<li><p>Find the solution to the problem using <a href="http://www.pyomo.org">PyOMO</a></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pyomo</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> sudo apt<span class="op">-</span>get install glpk<span class="op">-</span>utils <span class="op">--</span>quiet  <span class="co"># GLPK</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> sudo apt<span class="op">-</span>get install coinor<span class="op">-</span>cbc <span class="op">--</span>quiet  <span class="co"># CoinOR</span></span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Perform the sensitivity analysis. Which constraint could be relaxed to increase the profit the most? Prove it numerically.</p></li>
</ol></li>
<li><p>Prove the optimality of the solution [10 points]</p>
<p><span class="math display">
x = \left(\frac{7}{3} , 0, \frac{1}{3}\right)^T
</span></p>
<p>to the following linear programming problem:</p>
<p><span class="math display">
\begin{split}
&amp; 9x_1 + 3x_2 + 7x_3 \to \max\limits_{x \in \mathbb{R}^3 }\\
\text{s.t. } &amp; 2x_1 + x_2 + 3x_3 \leq 6 \\
&amp; 5x_1 + 4x_2 + x_3 \leq 12 \\
&amp; 3x_3 \leq 1,\\
&amp; x_1, x_2, x_3 \geq 0
\end{split}
</span></p>
<p>but you cannot use any numerical algorithm here.</p></li>
<li><p>[10 points] Economic interpretation of the dual problem: Suppose a small shop makes wooden toys, where each toy train requires one piece of wood and <span class="math inline">2</span> tins of paint, while each toy boat requires one piece of wood and <span class="math inline">1</span> tin of paint. The profit on each toy train is <span class="math inline">\$30</span>, and the profit on each toy boat is <span class="math inline">\$20</span>. Given an inventory of <span class="math inline">80</span> pieces of wood and <span class="math inline">100</span> tins of paint, how many of each toy should be made to maximize the profit?</p>
<ol type="1">
<li>Write out the optimization problem in standard form, writing all constraints as inequalities.</li>
<li>Sketch the feasible set and determine <span class="math inline">p^*</span> and <span class="math inline">x^*</span></li>
<li>Find the dual problem, then determine <span class="math inline">d^*</span> and <span class="math inline">\lambda^*</span>. Note that we can interpret the Lagrange multipliers <span class="math inline">\lambda_k</span> associated with the constraints on wood and paint as the prices for each piece of wood and tin of paint, so that <span class="math inline">‚àíd^*</span> is how much money would be obtained from selling the inventory for those prices. Strong duality says a buyer should not pay more for the inventory than what the toy store would make by producing and selling toys from it, and that the toy store should not sell the inventory for less than that.</li>
<li>The other interpretation of the Lagrange multipliers is as sensitivities to changes in the constraints. Suppose the toymaker found some more pieces of wood; the <span class="math inline">\lambda_k</span> associated with the wood constraint will equal the partial derivative of <span class="math inline">‚àíp^*</span> with respect to how much more wood became available. Suppose the inventory increases by one piece of wood. Use <span class="math inline">\lambda^*</span> to estimate how much the profit would increase, without solving the updated optimization problem. How is this consistent with the price interpretation given above for the Lagrange multipliers? <a href="https://tleise.people.amherst.edu/Math294Spring2017/TeXfiles/LagrangeDualityHW.pdf">source</a></li>
</ol></li>
</ol>
</section>
<section id="gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent">Gradient Descent</h3>
<ol type="1">
<li><p><strong>Convergence of Gradient Descent in non-convex smooth case</strong> [10 points]</p>
<p>We will assume nothing about the convexity of <span class="math inline">f</span>. We will show that gradient descent reaches an <span class="math inline">\varepsilon</span>-substationary point <span class="math inline">x</span>, such that <span class="math inline">\|\nabla f(x)\|_2 \leq \varepsilon</span>, in <span class="math inline">O(1/\varepsilon^2)</span> iterations. Important note: you may use here Lipschitz parabolic upper bound:</p>
<p><span id="eq-quad_ub"><span class="math display">
f(y) \leq f(x) + \nabla f(x)^T (y-x) + \frac{L}{2} \|y-x\|_2^2, \;\;\;
\text{for all $x,y$}.  
  \tag{1}</span></span></p>
<ul>
<li><p>Plug in <span class="math inline">y = x^{k+1} = x^{k} - \alpha \nabla f(x^k), x = x^k</span> to (<a href="#eq-quad_ub" class="quarto-xref">–£—Ä–∞–≤–Ω–µ–Ω–∏–µ&nbsp;1</a>) to show that</p>
<p><span class="math display">
  f(x^{k+1}) \leq f(x^k) - \Big (1-\frac{L\alpha}{2} \Big) \alpha \|\nabla f(x^k)\|_2^2.
  </span></p></li>
<li><p>Use <span class="math inline">\alpha \leq 1/L</span>, and rearrange the previous result, to get</p>
<p><span class="math display">
  \|\nabla f(x^k)\|_2^2 \leq \frac{2}{\alpha} \left( f(x^k) - f(x^{k+1}) \right).
  </span></p></li>
<li><p>Sum the previous result over all iterations from <span class="math inline">1,\ldots,k+1</span> to establish</p>
<p><span class="math display">
  \sum_{i=0}^k \|\nabla f(x^{i})\|_2^2 \leq
  \frac{2}{\alpha} ( f(x^{0}) - f^*).
  </span></p></li>
<li><p>Lower bound the sum in the previous result to get</p>
<p><span class="math display">
  \min_{i=0,\ldots,k} \|\nabla f(x^{i}) \|_2
  \leq \sqrt{\frac{2}{\alpha(k+1)} (f(x^{0}) - f^*)},
  </span> which establishes the desired <span class="math inline">O(1/\varepsilon^2)</span> rate for achieving <span class="math inline">\varepsilon</span>-substationarity.</p></li>
</ul></li>
<li><p><strong>How gradient descent convergence depends on the condition number and dimensionality.</strong> [20 points] Investigate how the number of iterations required for gradient descent to converge depends on the following two parameters: the condition number <span class="math inline">\kappa \geq 1</span> of the function being optimized, and the dimensionality <span class="math inline">n</span> of the space of variables being optimized.</p>
<p>To do this, for given parameters <span class="math inline">n</span> and <span class="math inline">\kappa</span>, randomly generate a quadratic problem of size <span class="math inline">n</span> with condition number <span class="math inline">\kappa</span> and run gradient descent on it with some fixed required precision. Measure the number of iterations <span class="math inline">T(n, \kappa)</span> that the method required for convergence (successful termination based on the stopping criterion).</p>
<p>Recommendation: The simplest way to generate a random quadratic problem of size <span class="math inline">n</span> with a given condition number <span class="math inline">\kappa</span> is as follows. It is convenient to take a diagonal matrix <span class="math inline">A \in S_{n}^{++}</span> as simply the diagonal matrix <span class="math inline">A = \text{Diag}(a)</span>, whose diagonal elements are randomly generated within <span class="math inline">[1, \kappa]</span>, and where <span class="math inline">\min(a) = 1</span>, <span class="math inline">\max(a) = \kappa</span>. As the vector <span class="math inline">b \in \mathbb{R}^n</span>, you can take a vector with random elements. Diagonal matrices are convenient to consider since they can be efficiently processed with even for large values of <span class="math inline">n</span>.</p>
<p>Fix a certain value of the dimensionality <span class="math inline">n</span>. Iterate over different condition numbers <span class="math inline">\kappa</span> on a grid and plot the dependence of <span class="math inline">T(n,\kappa)</span> against <span class="math inline">\kappa</span>. Since the quadratic problem is generated randomly each time, repeat this experiment several times. As a result, for a fixed value of <span class="math inline">n</span>, you should obtain a whole family of curves showing the dependence of <span class="math inline">T(n, \kappa)</span> on <span class="math inline">\kappa</span>. Draw all these curves in the same color for clarity (for example, red).</p>
<p>Now increase the value of <span class="math inline">n</span> and repeat the experiment. You should obtain a new family of curves <span class="math inline">T(n',\kappa)</span> against <span class="math inline">\kappa</span>. Draw all these curves in the same color but different from the previous one (for example, blue).</p>
<p>Repeat this procedure several times for other values of <span class="math inline">n</span>. Eventually, you should have several different families of curves - some red (corresponding to one value of <span class="math inline">n</span>), some blue (corresponding to another value of <span class="math inline">n</span>), some green, etc.</p>
<p>Note that it makes sense to iterate over the values of the dimensionality <span class="math inline">n</span> on a logarithmic grid (for example, <span class="math inline">n = 10, n = 100, n = 1000</span>, etc.). Use the following stopping criterion: <span class="math inline">\|\nabla f(x_k)\|_2^2 \leq \varepsilon \|\nabla f(x_0)\|_2^2</span> with <span class="math inline">\varepsilon = 10^{-5}</span>. Select the starting point <span class="math inline">x_0 = (1, \ldots, 1)^T</span></p>
<p>What conclusions can be drawn from the resulting picture?</p></li>
</ol>
</section>
<section id="accelerated-methods" class="level3">
<h3 class="anchored" data-anchor-id="accelerated-methods">Accelerated methods</h3>
<ol type="1">
<li><p><strong>Local Convergence of Heavy Ball Method.</strong> [20 points] We will work with the heavy ball method in this problem</p>
<p><span class="math display">
\tag{HB}
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1})
</span></p>
<p>It is known, that for the quadratics the best choice of hyperparameters is <span class="math inline">\alpha^* = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \beta^* = \dfrac{(\sqrt{L} - \sqrt{\mu})^2}{(\sqrt{L} + \sqrt{\mu})^2}</span>, which ensures accelerated linear convergence for a strongly convex quadratic function.</p>
<p>Consider the following continuously differentiable, strongly convex with parameter <span class="math inline">\mu</span>, and smooth function with parameter <span class="math inline">L</span>:</p>
<p><span class="math display">
f(x) =
\begin{cases}
\frac{25}{2}x^2, &amp; \text{if } x &lt; 1 \\
\frac12x^2 + 24x - 12, &amp; \text{if } 1 \leq x &lt; 2 \\
\frac{25}{2}x^2 - 24x + 36, &amp; \text{if } x \geq 2
\end{cases}
\quad
\nabla f(x) =
\begin{cases}
25x, &amp; \text{if } x &lt; 1 \\
x + 24, &amp; \text{if } 1 \leq x &lt; 2 \\
25x - 24, &amp; \text{if } x \geq 2
\end{cases}
</span></p>
<ol type="1">
<li><p>How to prove, that the given function is convex? Strongly convex? Smooth?</p></li>
<li><p>Find the constants <span class="math inline">\mu</span> and <span class="math inline">L</span> for a given function.</p></li>
<li><p>Plot the function value for <span class="math inline">x \in [-4, 4]</span>.</p></li>
<li><p>Run the Heavy Ball method for the function with optimal hyperparameters <span class="math inline">\alpha^* = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \beta^* = \dfrac{(\sqrt{L} - \sqrt{\mu})^2}{(\sqrt{L} + \sqrt{\mu})^2}</span> for quadratic function, starting from <span class="math inline">x_0 = 3.5</span>. If you have done everything above correctly, you should receive something like</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="heavy_ball_conv.mp4"></video></div>
<p>You can use the following code for plotting:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.animation <span class="im">as</span> animation</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient of the function</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad_f(x):</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Heavy Ball method implementation</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> heavy_ball_method(alpha, beta, x0, num_iterations):</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.zeros(num_iterations <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    x_prev <span class="op">=</span> x0</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    x_curr <span class="op">=</span> x0  <span class="co"># Initialize x[1] same as x[0] to start the algorithm</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        x[i] <span class="op">=</span> x_curr</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        x_new <span class="op">=</span> x_curr <span class="op">-</span> alpha <span class="op">*</span> grad_f(x_curr) <span class="op">+</span> beta <span class="op">*</span> (x_curr <span class="op">-</span> x_prev)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        x_prev <span class="op">=</span> x_curr</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        x_curr <span class="op">=</span> x_new</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    x[num_iterations] <span class="op">=</span> x_curr</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> ...</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> ...</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>alpha_star <span class="op">=</span> ...</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>beta_star <span class="op">=</span> ...</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> ...</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>num_iterations <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the trajectory of the method</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> heavy_ball_method(alpha_star, beta_star, x0, num_iterations)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup the figure and axes for the animation</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="fl">3.5</span>))</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">"Heavy ball method with optimal hyperparameters Œ±* Œ≤*"</span>)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Function for updating the animation</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update(i):</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>    ax1.clear()</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>    ax2.clear()</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot f(x) and trajectory</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>    x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">100</span>)</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>    f_vals <span class="op">=</span> np.piecewise(x_vals, [x_vals <span class="op">&lt;</span> <span class="dv">1</span>, (x_vals <span class="op">&gt;=</span> <span class="dv">1</span>) <span class="op">&amp;</span> (x_vals <span class="op">&lt;</span> <span class="dv">2</span>), x_vals <span class="op">&gt;=</span> <span class="dv">2</span>],</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>                        [<span class="kw">lambda</span> x: <span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span>, <span class="kw">lambda</span> x: <span class="fl">.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">-</span> <span class="dv">12</span>, <span class="kw">lambda</span> x: <span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">+</span> <span class="dv">36</span>])</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>    ax1.plot(x_vals, f_vals, <span class="st">'b-'</span>)</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>    ax1.plot(trajectory[:i], [<span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="fl">.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">-</span> <span class="dv">12</span> <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">2</span> <span class="cf">else</span> <span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">+</span> <span class="dv">36</span> <span class="cf">for</span> x <span class="kw">in</span> trajectory[:i]], <span class="st">'ro-'</span>)</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add vertical dashed lines at x=1 and x=2 on the left subplot</span></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>    ax1.axvline(x<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'navy'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>    ax1.axvline(x<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'navy'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot function value from iteration</span></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>    f_trajectory <span class="op">=</span> [<span class="va">None</span> <span class="cf">for</span> x <span class="kw">in</span> trajectory]</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>    f_trajectory[:i] <span class="op">=</span> [<span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="fl">.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">-</span> <span class="dv">12</span> <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">2</span> <span class="cf">else</span> <span class="fl">12.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">24</span> <span class="op">*</span> x <span class="op">+</span> <span class="dv">36</span> <span class="cf">for</span> x <span class="kw">in</span> trajectory[:i]]</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>    ax2.plot(<span class="bu">range</span>(<span class="bu">len</span>(trajectory)), f_trajectory, <span class="st">'ro-'</span>)</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlim(<span class="dv">0</span>, <span class="bu">len</span>(trajectory))</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylim(<span class="bu">min</span>(f_vals), <span class="bu">max</span>(f_vals))</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add horizontal dashed lines at f(1) and f(2) on the right subplot</span></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>    f_1 <span class="op">=</span> <span class="fl">12.5</span> <span class="op">*</span> <span class="fl">1.0</span><span class="op">**</span><span class="dv">2</span></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>    f_2 <span class="op">=</span> <span class="fl">.5</span> <span class="op">*</span> <span class="fl">2.</span><span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">24</span> <span class="op">*</span> <span class="fl">2.</span> <span class="op">-</span> <span class="dv">12</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>    ax2.axhline(y<span class="op">=</span>f_1, color<span class="op">=</span><span class="st">'navy'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>    ax2.axhline(y<span class="op">=</span>f_2, color<span class="op">=</span><span class="st">'navy'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ax1.set_title("Function f(x) and Trajectory")</span></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlabel(<span class="st">"x"</span>)</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylabel(<span class="st">"f(x)"</span>)</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>    ax1.grid(linestyle<span class="op">=</span><span class="st">":"</span>)</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ax2.set_title("Function Value from Iteration")</span></span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlabel(<span class="st">"Iteration"</span>)</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">"f(x)"</span>)</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>    ax2.grid(linestyle<span class="op">=</span><span class="st">":"</span>)</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the animation</span></span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>ani <span class="op">=</span> animation.FuncAnimation(fig, update, frames<span class="op">=</span>num_iterations, repeat<span class="op">=</span><span class="va">False</span>, interval<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>HTML(ani.to_jshtml())</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Change the starting point to <span class="math inline">x_0 = 3.4</span>. What do you see? How could you name such a behavior of the method?</p></li>
<li><p>Change the hyperparameter <span class="math inline">\alpha^{\text{Global}} = \frac2L, \beta^{\text{Global}} = \frac{\mu}{L}</span> and run the method again from <span class="math inline">x_0 = 3.4</span>. Check whether you have accelerated convergence here.</p></li>
</ol>
<p>Context: this counterexample was provided in the <a href="https://arxiv.org/pdf/1408.3595.pdf">paper</a>, while the global convergence of the heavy ball method for general smooth strongly convex function was introduced in another <a href="https://arxiv.org/pdf/1412.7457.pdf">paper</a>. Recently, it was <a href="https://arxiv.org/pdf/2307.11291.pdf">suggested</a>, that the heavy-ball (HB) method provably does not reach an accelerated convergence rate on smooth strongly convex problems.</p></li>
<li><p>[40 points] In this problem we will work with accelerated methods applied to the logistic regression problem. A good visual introduction to the topic is available <a href="https://mlu-explain.github.io/logistic-regression/">here</a>.</p>
<p>Logistic regression is a standard model in classification tasks. For simplicity, consider only the case of binary classification. Informally, the problem is formulated as follows: There is a training sample <span class="math inline">\{(a_i, b_i)\}_{i=1}^m</span>, consisting of <span class="math inline">m</span> vectors <span class="math inline">a_i \in \mathbb{R}^n</span> (referred to as features) and corresponding numbers <span class="math inline">b_i \in \{-1, 1\}</span> (referred to as classes or labels). The goal is to construct an algorithm <span class="math inline">b(\cdot)</span>, which for any new feature vector <span class="math inline">a</span> automatically determines its class <span class="math inline">b(a) \in \{-1, 1\}</span>.</p>
<p>In the logistic regression model, the class determination is performed based on the sign of the linear combination of the components of the vector <span class="math inline">a</span> with some fixed coefficients <span class="math inline">x \in \mathbb{R}^n</span>:</p>
<p><span class="math display">
b(a) := \text{sign}(\langle a, x \rangle).
</span></p>
<p>The coefficients <span class="math inline">x</span> are the parameters of the model and are adjusted by solving the following optimization problem:</p>
<p><span class="math display">
\tag{LogReg}
\min_{x \in \mathbb{R}^n} \left( \frac{1}{m} \sum_{i=1}^m \ln(1 + \exp(-b_i \langle a_i, x \rangle)) + \frac{\lambda}{2} \|x\|^2 \right),
</span></p>
<p>where <span class="math inline">\lambda \geq 0</span> is the regularization coefficient (a model parameter).</p>
<ol type="1">
<li><p>Will the LogReg problem be convex for <span class="math inline">\lambda = 0</span>? What is the gradient of the objective function? Will it be strongly convex? What if you will add regularization with <span class="math inline">\lambda &gt; 0</span>?</p></li>
<li><p>We will work with the real-world data for <span class="math inline">A</span> and <span class="math inline">b</span>: take the mushroom dataset. Be careful, you will need to predict if the mushroom is poisonous or edible. A poor model can cause death in this exercise.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_svmlight_file</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># URL of the file to download</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://hse24.fmin.xyz/files/mushrooms.txt'</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the file and save it locally</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> requests.get(url)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> <span class="st">'mushrooms.txt'</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure the request was successful</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> response.status_code <span class="op">==</span> <span class="dv">200</span>:</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(dataset, <span class="st">'wb'</span>) <span class="im">as</span> f:</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        f.write(response.content)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load the dataset from the downloaded file</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> load_svmlight_file(dataset)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    A, b <span class="op">=</span> data[<span class="dv">0</span>].toarray(), data[<span class="dv">1</span>]</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    n, d <span class="op">=</span> A.shape</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Data loaded successfully."</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Number of samples: </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">, Number of features: </span><span class="sc">{</span>d<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Failed to download the file. Status code: </span><span class="sc">{</span>response<span class="sc">.</span>status_code<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Divide the data into two parts: training and test. We will train the model on the <span class="math inline">A_{train}</span>, <span class="math inline">b_{train}</span> and measure the accuracy of the model on the <span class="math inline">A_{test}</span>, <span class="math inline">b_{test}</span>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and test sets</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>A_train, A_test, b_train, b_test <span class="op">=</span> train_test_split(A, b, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">214</span>)</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>For the training part <span class="math inline">A_{train}</span>, <span class="math inline">b_{train}</span>, estimate the constants <span class="math inline">\mu, L</span> of the training/optimization problem. Use the same small value <span class="math inline">\lambda</span> for all experiments</p></li>
<li><p>Using gradient descent with the step <span class="math inline">\frac{1}{L}</span>, train a model. Plot: accuracy versus iteration number.</p>
<p><span class="math display">
\tag{HB}
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1})
</span></p>
<p>Fix a step <span class="math inline">\alpha = \frac{1}{L}</span> and search for different values of the momentum <span class="math inline">\beta</span> from <span class="math inline">-1</span> to <span class="math inline">1</span>. Choose your own convergence criterion and plot convergence for several values of momentum on the same graph. Is the convergence always monotonic?</p></li>
<li><p>For the best value of momentum <span class="math inline">\beta</span>, plot the dependence of the model accuracy on the test sample on the running time of the method. Add to the same graph the convergence of gradient descent with step <span class="math inline">\frac{1}{L}</span>. Draw a conclusion. Ensure, that you use the same value of <span class="math inline">\lambda</span> for both methods.</p></li>
<li><p>Solve the logistic regression problem using the Nesterov method.</p>
<p><span class="math display">
\tag{NAG}
x_{k+1} = x_k - \alpha \nabla f(x_k + \beta (x_k - x_{k-1})) + \beta (x_k - x_{k-1})  
</span></p>
<p>Fix a step <span class="math inline">\frac{1}{L}</span> and search for different values of momentum <span class="math inline">\beta</span> from <span class="math inline">-1</span> to <span class="math inline">1</span>. Check also the momentum values equal to <span class="math inline">\frac{k}{k+3}</span>, <span class="math inline">\frac{k}{k+2}</span>, <span class="math inline">\frac{k}{k+1}</span> (<span class="math inline">k</span> is the number of iterations), and if you are solving a strongly convex problem, also <span class="math inline">\frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}</span>. Plot the convergence of the method as a function of the number of iterations (choose the convergence criterion yourself) for different values of the momentum. Is the convergence always monotonic?</p></li>
<li><p>For the best value of momentum <span class="math inline">\beta</span>, plot the dependence of the model accuracy on the test sample on the running time of the method. Add this graph to the graphs for the heavy ball and gradient descent from the previous steps. Make a conclusion.</p></li>
<li><p>Now we drop the estimated value of <span class="math inline">L</span> and will try to do it adaptively. Let us make the selection of the constant <span class="math inline">L</span> adaptive.</p>
<p><span class="math display">
f(y) \leq f(x^k) + \langle \nabla f(x^k), y - x^k \rangle + \frac{L}{2}\|x^k - y\|_2^2
</span></p>
<p>In particular, the procedure might work:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backtracking_L(f, grad, x, h, L0, rho, maxiter<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> L0</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    fx <span class="op">=</span> f(x)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    gradx <span class="op">=</span> grad(x)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">iter</span> <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">iter</span> <span class="op">&lt;</span> maxiter :</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> x <span class="op">-</span> <span class="dv">1</span> <span class="op">/</span> L <span class="op">*</span> h</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> f(y) <span class="op">&lt;=</span> fx <span class="op">-</span> <span class="dv">1</span> <span class="op">/</span> L gradx.dot(h) <span class="op">+</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> L) h.dot(h):</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>            L <span class="op">=</span> L <span class="op">*</span> rho</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">iter</span> <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> L</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div>
<p>What should <span class="math inline">h</span> be taken as? Should <span class="math inline">\rho</span> be greater or less than <span class="math inline">1</span>? Should <span class="math inline">L_0</span> be taken as large or small? Draw a similar figure as it was in the previous step for L computed adaptively (6 lines - GD, HB, NAG, GD adaptive L, HB adaptive L, NAG adaptive L)</p></li>
</ol></li>
</ol>
</section>
<section id="conjugate-gradients" class="level3">
<h3 class="anchored" data-anchor-id="conjugate-gradients">Conjugate gradients</h3>
<ol type="1">
<li><p><strong><a href="https://web.stanford.edu/class/ee364b/364b_exercises.pdf">Randomized Preconditioners for Conjugate Gradient Methods.</a></strong> (20 points)</p>
<p><em>Linear least squares</em></p>
<p>In this task, we explore the use of some randomization methods for solving overdetermined least-squares problems, focusing on conjugate gradient methods. Let <span class="math inline">\hat{A} \in \mathbb{R}^{m \times n}</span> be a matrix (we assume that <span class="math inline">m \gg n</span>) and <span class="math inline">\hat{b} \in \mathbb{R}^m</span>, we aim to minimize</p>
<p><span class="math display">
f(x) = \frac{1}{2} \|\hat{A}x - \hat{b}\|^2_2 = \frac{1}{2} \sum_{i=1}^m (\hat{a}_i^T x - \hat{b}_i)^2,
</span></p>
<p>where the <span class="math inline">\hat{a}_i \in \mathbb{R}^n</span> denote the rows of <span class="math inline">\hat{A}</span>.</p>
<p><em>Preconditioners</em></p>
<p>We know, that the convergence bound of the CG applied for the problem depends on the condition number of the matrix. Note, that for the problem above we have the matrix <span class="math inline">\hat{A}^T \hat{A}</span> and the condition number is squared after this operation (<span class="math inline">\kappa (X^T X) =  \kappa^2 \left(X \right)</span>). That is the reason, why we typically need to use <em>preconditioners</em> (<a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">read 12. for more details</a>) with CG.</p>
<p>The general idea of using preconditioners implies switchwing from solving <span class="math inline">Ax = b</span> to <span class="math inline">MAx = Mb</span> with hope, that <span class="math inline">\kappa \left( MA\right) \ll \kappa \left( A\right)</span> or eigenvalues of <span class="math inline">MA</span> are better clustered than those of <span class="math inline">A</span> (note, that matrix <span class="math inline">A</span> here is for the general case, here we have <span class="math inline">\hat{A}^T\hat{A}</span> instead).</p>
<p>This idea can also be viewed as coordinate change <span class="math inline">x = T \hat{x}, \; \hat{x} = T^{-1}x</span>, which leads to the problem <span class="math inline">T^T A T \hat{x} = T^Tb</span>. Note, that the spectrum of <span class="math inline">T^TAT</span> is the same as the spectrum of <span class="math inline">MA</span>.</p>
<p>The best choice of <span class="math inline">M</span> is <span class="math inline">A^{-1}</span>, because <span class="math inline">\kappa (A^{-1} A) = \kappa (I) = 1</span>. However, if we know <span class="math inline">A^{-1}</span>, the original problem is already solved, that is why we need to find some trade-off between enhanced convergence, and extra cost of working with <span class="math inline">M</span>. The goal is to find <span class="math inline">M</span> that is cheap to multiply, and approximate inverse of <span class="math inline">A</span> (or at least has a more clustered spectrum than <span class="math inline">A</span>).</p>
<p>Note, that for the linear least squares problem the matrix of quadratic form is <span class="math inline">A = \hat{A}^T\hat{A} \in \mathbb{R}^{n \times n}</span> and the rhs vector is <span class="math inline">b = \hat{A}^T\hat{b} \in \mathbb{R}^n</span>. Below you can find Vanilla CG algorithm (on the left) and preconditioned CG algorithm (on the right):</p>
<p><span class="math display">
\begin{aligned}
&amp; \mathbf{r}_0 := \mathbf{b} - \mathbf{A x}_0 \\
&amp; \hbox{if } \mathbf{r}_{0} \text{ is sufficiently small, then return } \mathbf{x}_{0} \text{ as the result}\\
&amp; \mathbf{d}_0 := \mathbf{r}_0 \\
&amp; k := 0 \\
&amp; \text{repeat} \\
&amp; \qquad \alpha_k := \frac{\mathbf{r}_k^\mathsf{T} \mathbf{r}_k}{\mathbf{d}_k^\mathsf{T} \mathbf{A d}_k}  \\
&amp; \qquad \mathbf{x}_{k+1} := \mathbf{x}_k + \alpha_k \mathbf{d}_k \\
&amp; \qquad \mathbf{r}_{k+1} := \mathbf{r}_k - \alpha_k \mathbf{A d}_k \\
&amp; \qquad \hbox{if } \mathbf{r}_{k+1} \text{ is sufficiently small, then exit loop} \\
&amp; \qquad \beta_k := \frac{\mathbf{r}_{k+1}^\mathsf{T} \mathbf{r}_{k+1}}{\mathbf{r}_k^\mathsf{T} \mathbf{r}_k} \\
&amp; \qquad \mathbf{d}_{k+1} := \mathbf{r}_{k+1} + \beta_k \mathbf{d}_k \\
&amp; \qquad k := k + 1 \\
&amp; \text{end repeat} \\
&amp; \text{return } \mathbf{x}_{k+1} \text{ as the result}
\end{aligned} \qquad
\begin{aligned}
&amp; \mathbf{r}_0 := \mathbf{b} - \mathbf{A x}_0 \\
&amp; \text{if } \mathbf{r}_0 \text{ is sufficiently small, then return } \mathbf{x}_0 \text{ as the result} \\
&amp; \mathbf{z}_0 := \mathbf{M} \mathbf{r}_0 \\
&amp; \mathbf{d}_0 := \mathbf{z}_0 \\
&amp; k := 0 \\
&amp; \text{repeat} \\
&amp; \qquad \alpha_k := \frac{\mathbf{r}_k^\mathsf{T} \mathbf{z}_k}{\mathbf{d}_k^\mathsf{T} \mathbf{A d}_k} \\
&amp; \qquad \mathbf{x}_{k+1} := \mathbf{x}_k + \alpha_k \mathbf{d}_k \\
&amp; \qquad \mathbf{r}_{k+1} := \mathbf{r}_k - \alpha_k \mathbf{A d}_k \\
&amp; \qquad \text{if } \mathbf{r}_{k+1} \text{ is sufficiently small, then exit loop} \\
&amp; \qquad \mathbf{z}_{k+1} := \mathbf{M} \mathbf{r}_{k+1} \\
&amp; \qquad \beta_k := \frac{\mathbf{r}_{k+1}^\mathsf{T} \mathbf{z}_{k+1}}{\mathbf{r}_k^\mathsf{T} \mathbf{z}_k} \\
&amp; \qquad \mathbf{d}_{k+1} := \mathbf{z}_{k+1} + \beta_k \mathbf{d}_k \\
&amp; \qquad k := k + 1 \\
&amp; \text{end repeat} \\
&amp; \text{return } \mathbf{x}_{k+1} \text{ as the result}
\end{aligned}
</span></p>
<p><em>Hadamard matrix</em></p>
<p>Given <span class="math inline">m \in \{2^i, i = 1, 2, \ldots\}</span>, the (unnormalized) Hadamard matrix of order <span class="math inline">m</span> is defined recursively as</p>
<p><span class="math display">
H_2 = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix}, \quad \text{and} \quad H_m = \begin{bmatrix} H_{m/2} &amp; H_{m/2} \\ H_{m/2} &amp; -H_{m/2} \end{bmatrix}.
</span></p>
<p>The associated normalized Hadamard matrix is given by <span class="math inline">H^{(\text{norm})}_m = \frac{1}{\sqrt{m}} H_m</span>, which evidently satisfies <span class="math inline">H^{(\text{norm})T}_m H^{(\text{norm})}_m = I_{m \times m}</span>. Moreover, via a recursive algorithm, it is possible to compute matvec <span class="math inline">H_m x</span> in time <span class="math inline">O(m \log m)</span>, which is much faster than <span class="math inline">m^2</span> for a general matrix.</p>
<p>To solve the least squares minimization problem using conjugate gradients, we must solve <span class="math inline">\hat{A}^T \hat{A} x = \hat{A}^T b</span>. Using a preconditioner <span class="math inline">M</span> such that <span class="math inline">M \approx A^{-1}</span> can give substantial speedup in computing solutions to large problems.</p>
<p>Consider the following scheme to generate a randomized preconditioner, assuming that <span class="math inline">m = 2^i</span> for some <span class="math inline">i</span>:</p>
<ol type="1">
<li><p>Let <span class="math inline">S = \text{diag}(S_{11}, \ldots, S_{mm})</span>, where <span class="math inline">S_{jj}</span> are random <span class="math inline">\{-1,+1\}</span> signs</p></li>
<li><p>Let <span class="math inline">p \in \mathbb{Z}^+</span> be a small positive integer, say <span class="math inline">20</span> for this problem.</p></li>
<li><p>Let <span class="math inline">R \in \{0, 1\}^{n+p \times m}</span> be a <em>row selection matrix</em>, meaning that each row of <span class="math inline">R</span> has only 1 non-zero entry, chosen uniformly at random. (The location of these non-zero columns is distinct.)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> random</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_row_selection_matrix_jax(m, n, p, key):</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># m is the number of columns in the original matrix A</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># n+p is the number of rows in the row selection matrix R</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># key is a PRNGKey needed for randomness in JAX</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    inds <span class="op">=</span> random.permutation(key, m)[:n<span class="op">+</span>p]  <span class="co"># Generate a random permutation and select the first n+p indices</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> jnp.zeros((n<span class="op">+</span>p, m), dtype<span class="op">=</span>jnp.int32)  <span class="co"># Create a zero matrix of shape (n+p, m)</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> R.at[np.arange(n<span class="op">+</span>p), inds].<span class="bu">set</span>(<span class="dv">1</span>)     <span class="co"># Use JAX's indexed update to set the entries corresponding to inds to 1</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> R</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>Define <span class="math inline">\Phi = R H^{(\text{norm})}_m S \in \mathbb{R}^{n+p \times m}</span></p></li>
</ol>
<p>We then define the matrix <span class="math inline">M</span> via its inverse <span class="math inline">M^{-1} = \hat{A}^T \Phi^T \Phi \hat{A} \in \mathbb{R}^{n \times n}</span>.</p>
<p><em>Questions</em></p>
<ol type="1">
<li><p><strong>(2 point)</strong> How many FLOPs (floating point operations, i.e.&nbsp;multiplication and additions) are required to compute the matrices <span class="math inline">M^{-1}</span> and <span class="math inline">M</span>, respectively, assuming that you can compute the matrix-vector product <span class="math inline">H_mv</span> in time <span class="math inline">m \log m</span> for any vector <span class="math inline">v \in \mathbb{R}^m</span>?</p></li>
<li><p><strong>(2 point)</strong> How many FLOPs are required to naively compute <span class="math inline">\hat{A}^T \hat{A}</span>, assuming <span class="math inline">\hat{A}</span> is dense (using standard matrix algorithms)?</p></li>
<li><p><strong>(2 point)</strong> How many FLOPs are required to compute <span class="math inline">\hat{A}^T \hat{A} v</span> for a vector <span class="math inline">v \in \mathbb{R}^n</span> by first computing <span class="math inline">u = \hat{A}v</span> and then computing <span class="math inline">\hat{A}^T u</span>?</p></li>
<li><p><strong>(4 poins)</strong> Suppose that conjugate gradients runs for <span class="math inline">k</span> iterations. Using the preconditioned conjugate gradient algorithm with <span class="math inline">M = (\hat{A}^T \Phi^T \Phi \hat{A})^{-1}</span>, how many total floating point operations have been performed? How many would be required to directly solve <span class="math inline">\hat{A}^T \hat{A} x = \hat{A}^T b</span>? How large must <span class="math inline">k</span> be to make the conjugate gradient method slower?</p></li>
<li><p><strong>(10 points)</strong> Implement the conjugate gradient algorithm for solving the positive definite linear system <span class="math inline">\hat{A}^T \hat{A} x = \hat{A}^T b</span> both with and without the preconditioner <span class="math inline">M</span>. To generate data for your problem, set <span class="math inline">m = 2^{12}</span> and <span class="math inline">n = 400</span>, then generate the matrix <span class="math inline">A</span> and the vector <span class="math inline">b</span>. For simplicity in implementation, you may directly pass <span class="math inline">\hat{A}^T \hat{A}</span> and <span class="math inline">\hat{A}^T b</span> into your conjugate gradient solver, as we only wish to explore how the methods work.</p></li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.sparse <span class="im">import</span> diags</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">2</span><span class="op">**</span><span class="dv">12</span>  <span class="co"># 4096</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">400</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a linear space of values from 0.001 to 100</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> np.linspace(<span class="fl">0.001</span>, <span class="dv">100</span>, n)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the matrix A</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.random.randn(m, n) <span class="op">*</span> diags(values).toarray()</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.random.randn(m, <span class="dv">1</span>)</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div>
<p>Plot the norm of the residual <span class="math inline">r_k = \hat{A}^T b - \hat{A}^T \hat{A} x_k</span> (relative to <span class="math inline">\|\hat{A}^T b\|_2</span>) as a function of iteration <span class="math inline">k</span> for each of your conjugate gradient procedures. Additionally, compute and print the condition numbers <span class="math inline">\kappa(\hat{A}^T \hat{A})</span> and <span class="math inline">\kappa(M^{1/2} \hat{A}^T \hat{A} M^{1/2})</span>.</p></li>
</ol>
</section>
<section id="newton-and-quasinewton-methods" class="level3">
<h3 class="anchored" data-anchor-id="newton-and-quasinewton-methods">Newton and quasinewton methods</h3>
<ol type="1">
<li><p><strong>üò± Newton convergence issue</strong> (10 points)</p>
<p>Consider the following function:</p>
<p><span class="math display">
f(x,y) = \dfrac{x^4}{4} - x^2 + 2x + (y-1)^2
</span></p>
<p>And the starting point is <span class="math inline">x_0 = (0,2)^\top</span>. How does Newton‚Äôs method behave when started from this point? How can this be explained? How does the gradient descent with fixed step <span class="math inline">\alpha = 0.01</span> and the steepest descent method behave under the same conditions? (It is not necessary to show numerical simulations in this problem).</p></li>
<li><p><strong>Hessian-Free Newton method</strong> (20 points) In this exercise, we‚Äôll explore the optimization of a binary logistic regression problem using various methods. Don‚Äôt worry about the size of the problem description, first 5 bullets out of 7 could be done pretty quickly. In this problem you should start with this <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Hessian_free_Newton.ipynb">colab notebook</a></p>
<p>Given a dataset with <span class="math inline">n</span> observations, where each observation consists of a feature vector <span class="math inline">x_i</span> and an associated binary target variable <span class="math inline">y_i \in \{0,1\}</span>, the logistic regression model predicts the probability that <span class="math inline">y_i = 1</span> given <span class="math inline">x_i</span> using the logistic function. The loss function to be minimized is the negative log-likelihood of the observed outcomes under this model, summed across all observations. It has a high value when the model outputs differ significantly from the data <span class="math inline">y</span>.</p>
<p>The binary cross-entropy loss function for a single observation <span class="math inline">(x_i, y_i)</span> is given by: <span class="math display">
\text{Loss}(w; x_i, y_i) = -\left[ y_i \log(p(y_i=1 | x_i; w)) + (1-y_i) \log(1-p(y_i=1 | x_i; w)) \right]
</span></p>
<p>Here, <span class="math inline">p(y=1 | x;w)</span> is defined as: <span class="math display">
p(y=1 | x;w) = \frac{1}{1 + e^{-w^T x}}
</span></p>
<p>To define the total loss over the dataset, we sum up the individual losses: <span class="math display">
f(w) = -\sum_{i=1}^n \left[ y_i \log(p(y_i=1 | x_i; w)) + (1-y_i) \log(1-p(y_i=1 | x_i; w)) \right]
</span></p>
<p>Therefore, the optimization problem in logistic regression is: <span class="math display">
\min_w f(w) = \min_w -\sum_{i=1}^n \left[ y_i \log\left(p\left(y_i=1 | x_i; w\right)\right) + \left(1-y_i\right) \log\left(1-p(y_i=1 | x_i; w)\right) \right]
</span></p>
<p>This is a convex optimization problem and can be solved using gradient-based methods such as gradient descent, Newton‚Äôs method, or more sophisticated optimization algorithms often available in machine learning libraries. However, it is the problem is often together with <span class="math inline">l_2</span> regularization:</p>
<p><span class="math display">
\min_w f(w) = \min_w -\sum_{i=1}^n \left[ y_i \log\left(p\left(y_i=1 | x_i; w\right)\right) + \left(1-y_i\right) \log\left(1-p(y_i=1 | x_i; w)\right) \right] + \frac{\mu}{2} \|w\|_2^2
</span></p>
<ol type="1">
<li><p>(2 points) Firstly, we address the optimization with Gradient Descent (GD) in a strongly convex setting, with <span class="math inline">\mu = 1</span>. Use a constant learning rate <span class="math inline">\alpha</span>. Run the gradient descent algorithm. Report the highest learning rate that ensures convergence of the algorithm. Plot the convergence graph in terms of both domain (parameter values) and function value (loss). Describe the type of convergence observed.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="dv">1</span>,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">1000</span>,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">100</span>,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"GD"</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">3e-2</span>,</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">550</span>,</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>results, params <span class="op">=</span> run_experiments(params)</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>(2 points) Run Newton‚Äôs method under the same conditions, using the second derivatives to guide the optimization. Describe and analyze the convergence properties observed.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="dv">1</span>,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">1000</span>,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">100</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"GD"</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">3e-2</span>,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">550</span>,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"Newton"</span>,</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">20</span>,</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>results, params <span class="op">=</span> run_experiments(params)</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>(2 points) In cases where Newton‚Äôs method may converge too rapidly or overshoot, a damped version can be more stable. Run the damped Newton method. Adjust the damping factor as a learning rate. Report the highest learning rate ensuring stability and convergence. Plot the convergence graph.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="dv">1</span>,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">1000</span>,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">100</span>,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"GD"</span>,</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">3e-2</span>,</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">550</span>,</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"Newton"</span>,</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">20</span>,</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"Newton"</span>,</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">5e-1</span>,</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">50</span>,</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>results, params <span class="op">=</span> run_experiments(params)</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>(2 points) Now turn off the regularization by setting <span class="math inline">\mu=0</span>. Try to find the largest learning rate, which ensures convergence of the Gradient Descent. Use a constant learning rate <span class="math inline">\alpha</span>. Run the gradient descent algorithm. Report the highest learning rate that ensures convergence of the algorithm. Plot the convergence graph in terms of both domain (parameter values) and function value (loss). Describe the type of convergence observed. How can you describe an idea to run this method for the problem to reach tight primal gap <span class="math inline">f(x_k) - f^* \approx 10^{-2}</span> or <span class="math inline">10^{-3}</span>, <span class="math inline">10^{-4}</span>?</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="dv">0</span>,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">1000</span>,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">100</span>,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"GD"</span>,</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">3e-2</span>,</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">200</span>,</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"GD"</span>,</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">7e-2</span>,</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">200</span>,</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>results, params <span class="op">=</span> run_experiments(params)</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>(2 points) What can you say about Newton‚Äôs method convergence in the same setting <span class="math inline">\mu=0</span>? Try several learning rates smaller, than <span class="math inline">1</span> for the damped Newton method. Does it work? Write your conclusions about the second-order method convergence for a binary logistic regression problem.</p></li>
<li><p>(5 points) Now switch back to the strongly convex setting <span class="math inline">\mu=1</span>. To avoid directly computing the Hessian matrix in Newton‚Äôs method, use the Conjugate Gradient (CG) method to solve the linear system in the Newton step. Develop the <code>newton_method_cg</code> function, which computes the Newton step using CG to solve the system <span class="math inline">\nabla^2 f(x_k) d_k = - \nabla f(x_k), \; x_{k+1} = x_k + \alpha d_k</span> defined by the Hessian. You have to use <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.sparse.linalg.cg.html"><code>jax.scipy.sparse.linalg.cg</code></a> function here. So, firstly compute the hessian as it was done in the code, then put it into this linear solver. Compare its performance in terms of computational efficiency and convergence rate to the standard Newton method.</p></li>
<li><p>(5 points) Finally, implement a Hessian-free version of Newton‚Äôs method (HFN) which utilizes Hessian-vector products derived via automatic differentiation. Note, that <code>jax.scipy.sparse.linalg.cg</code> function can take the matvec function, which directly produces the multiplication of any input vector <span class="math inline">x</span>. Implement the HFN method without explicitly forming or storing the Hessian matrix in function <code>newton_method_hfn</code>. Use autograd to compute Hessian-vector products as it is described <a href="https://iclr-blogposts.github.io/2024/blog/bench-hvp/">here</a>. Compare this method‚Äôs time complexity and memory requirements against previous implementations.</p></li>
</ol></li>
</ol>
</section>
<section id="conditional-gradient-methods" class="level3">
<h3 class="anchored" data-anchor-id="conditional-gradient-methods">Conditional gradient methods</h3>
<ol type="1">
<li><p><strong>Projection onto the Birkhoff Polytope using Frank-Wolfe</strong> [20 points]</p>
<p>In a recent <a href="https://arxiv.org/pdf/2211.14103">book</a> authors presented the following comparison table with complexities of linear minimizations and projections on some convex sets up to an additive error <span class="math inline">\epsilon</span> in the Euclidean norm. When <span class="math inline">\epsilon</span> is missing, there is no additive error. The <span class="math inline">\tilde{\mathcal{O}}</span> hides polylogarithmic factors in the dimensions and polynomial factors in constants related to thedistancetothe optimum. For the nuclear norm ball, i.e., the spectrahedron, <span class="math inline">\nu</span> denotes the number of non-zero entries and <span class="math inline">\sigma_1</span> denotes the top singular value of the projected matrix.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 45%">
<col style="width: 37%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Set</strong></th>
<th><strong>Linear minimization</strong></th>
<th><strong>Projection</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">n</span>-dimensional <span class="math inline">\ell_p</span>-ball, <span class="math inline">p \neq 1,2,\infty</span></td>
<td><span class="math inline">\mathcal{O}(n)</span></td>
<td><span class="math inline">\tilde{\mathcal{O}}\!\bigl(\tfrac{n}{\epsilon^2}\bigr)</span></td>
</tr>
<tr class="even">
<td>Nuclear norm ball of <span class="math inline">n\times m</span> matrices</td>
<td><span class="math inline">\mathcal{O}\!\Bigl(\nu\,\ln(m + n)\,\tfrac{\sqrt{\sigma_1}}{\sqrt{\epsilon}}\Bigr)</span></td>
<td><span class="math inline">\mathcal{O}\!\bigl(m\,n\,\min\{m,n\}\bigr)</span></td>
</tr>
<tr class="odd">
<td>Flow polytope on a graph with <span class="math inline">m</span> vertices and <span class="math inline">n</span> edges (capacity bound on edges)</td>
<td><span class="math inline">\mathcal{O}\!\Bigl((n \log m)\bigl(n + m\,\log m\bigr)\Bigr)</span></td>
<td><span class="math inline">\tilde{\mathcal{O}}\!\bigl(\tfrac{n}{\epsilon^2}\bigr)\ \text{or}\ \mathcal{O}(n^4\,\log n)</span></td>
</tr>
<tr class="even">
<td>Birkhoff polytope (<span class="math inline">n \times n</span> doubly stochastic matrices)</td>
<td><span class="math inline">\mathcal{O}(n^3)</span></td>
<td><span class="math inline">\tilde{\mathcal{O}}\!\bigl(\tfrac{n^2}{\epsilon^2}\bigr)</span></td>
</tr>
</tbody>
</table>
<p>The Birkhoff polytope, denoted as <span class="math inline">B_n</span>, is the set of <span class="math inline">n \times n</span> doubly stochastic matrices: <span class="math display">
B_n = \{ X \in \mathbb{R}^{n \times n} \mid X_{ij} \ge 0 \;\forall i,j, \quad X \mathbf{1} = \mathbf{1}, \quad X^T \mathbf{1} = \mathbf{1} \}
</span> where <span class="math inline">\mathbf{1}</span> is the vector of all ones. This set is convex and compact. Its extreme points are the permutation matrices.</p>
<p>Given an arbitrary matrix <span class="math inline">Y \in \mathbb{R}^{n \times n}</span>, we want to find its projection onto <span class="math inline">B_n</span>, which is the solution to the optimization problem: <span class="math display">
\min_{X \in B_n} f(X) = \frac{1}{2} \| X - Y \|_F^2
</span> where <span class="math inline">\| \cdot \|_F</span> is the Frobenius norm.</p>
<p>We will use the Frank-Wolfe (Conditional Gradient) algorithm to solve this problem. Recall the steps of the Frank-Wolfe algorithm:</p>
<ul>
<li>Initialize <span class="math inline">X_0 \in B_n</span>.</li>
<li>For <span class="math inline">k = 0, 1, 2, \ldots</span>:
<ul>
<li>Compute the gradient <span class="math inline">\nabla f(X_k)</span>.</li>
<li>Solve the Linear Minimization Oracle (LMO): <span class="math inline">S_k = \arg\min_{S \in B_n} \langle \nabla f(X_k), S \rangle</span>.</li>
<li>Determine the step size <span class="math inline">\gamma_k \in [0, 1]</span>.</li>
<li>Update <span class="math inline">X_{k+1} = (1-\gamma_k) X_k + \gamma_k S_k</span>.</li>
</ul></li>
</ul>
<p><strong>Tasks:</strong></p>
<ol type="1">
<li><p>[5 points] Explicitly write down the gradient <span class="math inline">\nabla f(X_k)</span>. Explain how to solve the LMO step <span class="math inline">\min_{S \in B_n} \langle \nabla f(X_k), S \rangle</span>. What kind of matrix is the solution <span class="math inline">S_k</span>? <em>Hint: Consider the connection to the linear assignment problem (Hungarian algorithm).</em></p></li>
<li><p>[10 points] Implement the Frank-Wolfe algorithm in Python to solve the projection problem. Use <code>scipy.optimize.linear_sum_assignment</code> to solve the LMO. For the step size, you can use the optimal closed-form solution for projection: <span class="math inline">\gamma_k = \frac{\langle X_k - Y, X_k - S_k \rangle}{\| X_k - S_k \|_F^2}</span>, clipped to <span class="math inline">[0, 1]</span>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> linear_sum_assignment</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> project_to_birkhoff_frank_wolfe(Y, max_iter<span class="op">=</span><span class="dv">100</span>, tol<span class="op">=</span><span class="fl">1e-6</span>):</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Projects matrix Y onto the Birkhoff polytope using the Frank-Wolfe algorithm.</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">        Y (np.ndarray): The matrix to project.</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">        max_iter (int): Maximum number of iterations.</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">        tol (float): Tolerance for convergence (change in objective value).</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co">        np.ndarray: The projection of Y onto the Birkhoff polytope.</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co">        list: History of objective function values.</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> Y.shape[<span class="dv">0</span>]</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> Y.shape[<span class="dv">0</span>] <span class="op">==</span> Y.shape[<span class="dv">1</span>], <span class="st">"Input matrix must be square"</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize with a feasible point (e.g., uniform matrix)</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    Xk <span class="op">=</span> np.ones((n, n)) <span class="op">/</span> n</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    objective_history <span class="op">=</span> []</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Objective function value</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>        obj_val <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> np.linalg.norm(Xk <span class="op">-</span> Y, <span class="st">'fro'</span>)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>        objective_history.append(obj_val)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> <span class="bu">abs</span>(objective_history[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> objective_history[<span class="op">-</span><span class="dv">2</span>]) <span class="op">&lt;</span> tol:</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Converged after </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss"> iterations."</span>)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Compute gradient</span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>        grad_fk <span class="op">=</span> ... <span class="co"># YOUR CODE HERE </span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Solve the LMO: S_k = argmin_{S in Birkhoff} &lt;grad_fk, S&gt;</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use linear_sum_assignment on the cost matrix grad_fk</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>        row_ind, col_ind <span class="op">=</span> ... <span class="co"># YOUR CODE HERE using linear_sum_assignment</span></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>        Sk <span class="op">=</span> np.zeros((n, n))</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Construct permutation matrix Sk based on row_ind, col_ind</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>        ... <span class="co"># YOUR CODE HERE </span></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Compute step size gamma_k </span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optimal step size for projection, clipped to [0, 1]</span></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>        delta_k <span class="op">=</span> Xk <span class="op">-</span> Sk</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>        denom <span class="op">=</span> np.linalg.norm(delta_k, <span class="st">'fro'</span>)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> denom <span class="op">&lt;</span> <span class="fl">1e-12</span>: <span class="co"># Avoid division by zero if Xk is already the vertex Sk</span></span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>            gamma_k <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>            gamma_k <span class="op">=</span> ... <span class="co"># YOUR CODE HERE for optimal step size</span></span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>            gamma_k <span class="op">=</span> np.clip(gamma_k, <span class="fl">0.0</span>, <span class="fl">1.0</span>) </span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Update</span></span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a>        Xk <span class="op">=</span> ... <span class="co"># YOUR CODE HERE </span></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: <span class="co"># If loop finishes without breaking</span></span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>         <span class="bu">print</span>(<span class="ss">f"Reached max iterations (</span><span class="sc">{</span>max_iter<span class="sc">}</span><span class="ss">)."</span>)</span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Xk, objective_history</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p>[5 points] Test your implementation with <span class="math inline">n=5</span> and a randomly generated matrix <span class="math inline">Y = \text{np.random.rand}(5, 5)</span>. Run the algorithm for 200 iterations. Plot the objective function value <span class="math inline">f(X_k)</span> versus the iteration number <span class="math inline">k</span>. Verify numerically that the final matrix <span class="math inline">X_{200}</span> approximately satisfies the conditions for being in <span class="math inline">B_5</span> (non-negative entries, row sums equal to 1, column sums equal to 1).</p></li>
</ol></li>
<li><p><strong>[Minimizing a Quadratic over the Simplex]</strong> [20 points] Consider the problem of minimizing a quadratic function over the standard probability simplex: <span class="math display">
\min_{x \in \Delta_n} f(x) = \frac{1}{2} x^T Q x + c^T x
</span> where <span class="math inline">\Delta_n = \{x \in \mathbb{R}^n \mid \sum_{i=1}^n x_i = 1, x_i \ge 0\}</span> is the standard simplex in <span class="math inline">\mathbb{R}^n</span>, <span class="math inline">Q \in \mathbb{S}^n_{++}</span> is a positive definite matrix, and <span class="math inline">c \in \mathbb{R}^n</span>.</p>
<ul>
<li>[5 points] Generate the problem data: Choose a dimension <span class="math inline">n</span> (e.g., <span class="math inline">n=20</span>). Create a random positive definite matrix <span class="math inline">Q</span> with a given spectrum <span class="math inline">[\mu; L]</span> and a random vector <span class="math inline">x^* \in \Delta_n</span>, so <span class="math inline">c = -Q x^*</span> (e.g., with standard normal entries).
<ul>
<li>Specify and consider <span class="math inline">2</span> different starting points (you will use them for another algorithm as well)</li>
<li>Calculate <span class="math inline">f(x^*)</span> and <span class="math inline">f(x_0)</span>, you will have to track <span class="math inline">|f(x_k) - f(x^*)|</span> for both algorithms</li>
</ul></li>
<li>[7 points] Implement the Frank-Wolfe (Conditional Gradient) algorithm to solve this problem. Do not forget to start from a feasible point.</li>
<li>[8 points] Implement the Projected Gradient Descent algorithm.
<ul>
<li>Use the same starting points <span class="math inline">x_0</span>.</li>
<li>Justify learning rate selection.</li>
<li>We do not have an explicit formula for Euclidean projection onto the standard simplex. You will need an algorithm for projection onto the standard simplex (e.g., see <a href="https://stanford.edu/~jduchi/projects/DuchiShSiCh08.pdf">Duchi et al., 2008</a> or use available implementations).</li>
</ul></li>
<li>Plot the objective function value <span class="math inline">f(x_k)</span> versus the iteration number <span class="math inline">k</span> for both Frank-Wolfe and Projected Gradient Descent on the same graph. Compare their convergence behavior. Discuss which method appears to converge faster in terms of iterations for this problem.</li>
</ul></li>
</ol>
</section>
<section id="subgradient-method" class="level3">
<h3 class="anchored" data-anchor-id="subgradient-method">Subgradient method</h3>
<ol type="1">
<li><p><strong>Finding a point in the intersection of convex sets.</strong> [30 points] Let <span class="math inline">A \in \mathbb{R}^{n \times n}</span> be some non-degenerate matrix and let <span class="math inline">\Sigma</span> be an <span class="math inline">n \times n</span> diagonal matrix with diagonal entries <span class="math inline">\sigma_1,...,\sigma_n &gt; 0</span>, and <span class="math inline">y</span> a given vector in <span class="math inline">\mathbb{R}^n</span>. Consider the compact convex sets <span class="math inline">U = \{x \in \mathbb{R}^n \mid \|A(x-y)\|_2 \leq 1\}</span> and <span class="math inline">V = \{x \in \mathbb{R}^n \mid \|\Sigma x\|_\infty \leq 1\}</span>.</p>
<ul>
<li><p>[10 points] Minimize maximum distance from the current point to the convex sets.</p>
<p><span class="math display">
  \min_{x\in\mathbb{R}^n} f(x) =  \min_{x\in\mathbb{R}^n} \max\{\mathbf{dist}(x, U), \mathbf{dist}(x, V)\}
  </span></p>
<p>propose an algorithm to find a point <span class="math inline">x \in U \cap V</span>. You can assume that <span class="math inline">U \cap V</span> is not empty. Your algorithm must be specific and provably converging (although you do not need to prove it and you can simply refer to the lecture slides).</p></li>
<li><p>[15 points] Implement your algorithm with the following data: <span class="math inline">n = 2</span>, <span class="math inline">y = (3, 2)</span>, <span class="math inline">\sigma_1 = 0.5</span>, <span class="math inline">\sigma_2 = 1</span>,</p>
<p><span class="math display">
  A = \begin{bmatrix}
  1 &amp; 0 \\
  -1 &amp; 1
  \end{bmatrix},
  </span></p>
<p>Plot the objective value of your optimization problem versus the number of iterations. Choose the following initial points <span class="math inline">x_0 = [(2, -1), (0, 0), (1, 2)]</span>.</p></li>
<li><p>[5 points] Discussion: compare the three curves. Describe the properties of this optimization problem.</p>
<ul>
<li>Is it convex/strongly convex?</li>
<li>Is it smooth?</li>
<li>Do we have a unique solution here?</li>
<li>Which start converges fastest / slowest and why? Relate your observations to the initial distance to <span class="math inline">U \cap V</span> and to the contact angle between the two sets at the solution.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="convex_intersection.png" class="img-fluid figure-img"></p>
<figcaption>Illustration of the problem</figcaption>
</figure>
</div></li>
<li><p><strong>Subgradient methods for Lasso.</strong> (10 points)</p>
<p>Consider the optimization problem</p>
<p><span class="math display">
\min_{x \in \mathbb{R}^n} f(x) := \frac12 \|Ax - b\|^2 + \lambda \|x\|_1,
</span></p>
<p>with variables <span class="math inline">x \in \mathbb{R}^n</span> and problem data <span class="math inline">A \in \mathbb{R}^{m \times n}</span>, <span class="math inline">b \in \mathbb{R}^m</span> and <span class="math inline">\lambda &gt; 0</span>. This model is known as Lasso, or Least Squares with <span class="math inline">l_1</span> regularization, which encourages sparsity in the solution via the non-smooth penalty <span class="math inline">\|x\|_1 := \sum_{j=1}^n |x_j|</span>. In this problem, we will explore various subgradient methods for fitting this model.</p>
<ul>
<li><p>Derive the subdifferential <span class="math inline">\partial f(x)</span> of the objective.</p></li>
<li><p>Find the update rule of the subgradient method and state the computational complexity of applying one update using big O notation in terms of the dimensions.</p></li>
<li><p>Let <span class="math inline">n = 1000</span>, <span class="math inline">m = 200</span> and <span class="math inline">\lambda = 0.01</span>. Generate a random matrix <span class="math inline">A \in \mathbb{R}^{m \times n}</span> with independent Gaussian entries with mean 0 and variance <span class="math inline">1/m</span>, and a fixed vector <span class="math inline">x^* = {\underbrace{[1, \ldots, 1}_{\text{k times}}, \underbrace{0, \ldots, 0]}_{\text{n-k times}}}^T \in \mathbb{R}^n</span>. Let <span class="math inline">k = 5</span> and then set <span class="math inline">b = Ax^*</span>. Implement the subgradient method to minimize <span class="math inline">f(x)</span>, initialized at the all-zeros vector. Try different step size rules, including:</p>
<ul>
<li><p>constant step size <span class="math inline">\alpha_k = \alpha</span></p></li>
<li><p>constant step length <span class="math inline">\alpha_k = \frac{\gamma}{\|g_k\|_2}</span> (so <span class="math inline">\|x^{k+1} - x^k\|_2 = \gamma</span>)</p></li>
<li><p>Inverse square root <span class="math inline">\frac{1}{\sqrt{k}}</span></p></li>
<li><p>Inverse <span class="math inline">\frac1k</span></p></li>
<li><p>Polyak‚Äôs step length with estimated objective value:</p>
<p><span class="math display">
  \alpha_k = \frac{f(x_k) - f_k^{\text{best}} + \gamma_k}{\|g_k\|_2^2}, \quad \text{ with} \sum_{k=1}^\infty \gamma_k = \infty, \quad \sum_{k=1}^\infty \gamma_k^2 &lt; \infty
  </span></p>
<p>For example, one can use <span class="math inline">\gamma_k = \frac{10}{10 + k}</span>. Here <span class="math inline">f_k^{\text{best}} - \gamma_k</span> serves as estimate of <span class="math inline">f^*</span>. It is better to take <span class="math inline">\gamma_k</span> in the same scale as the objective value. One can show, that <span class="math inline">f_k^{\text{best}} \to f^*</span>.</p></li>
</ul>
<p>Plot objective value versus iteration curves of different step size rules on the same figure.</p></li>
<li><p>Repeat previous part using a heavy ball term, <span class="math inline">\beta_k(x^k - x^{k-1})</span>, added to the subgradient. Try different step size rules as in the previous part and tune the heavy ball parameter <span class="math inline">\beta_k = \beta</span> for faster convergence.</p></li>
</ul></li>
</ol>
</section>
<section id="proximal-gradient-method" class="level3">
<h3 class="anchored" data-anchor-id="proximal-gradient-method">Proximal gradient method</h3>
<ol type="1">
<li><p>[20 points] <strong>Proximal Method for Sparse Softmax Regression</strong> Softmax regression, also known as multinomial logistic regression, is a generalization of logistic regression to multiple classes. It is used to model categorical outcome variables where each category is mutually exclusive. The softmax function transforms any input vector to the probability-like vector as follows:</p>
<p><span class="math display">
P(y = j | x; W) = \frac{e^{W_j^T x}}{\sum\limits_{i=1}^{c} e^{W_i^T x}}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Softmax.svg" class="img-fluid figure-img"></p>
<figcaption>Scheme of softmax regression</figcaption>
</figure>
</div>
<p>where <span class="math inline">x</span> is the input vector, <span class="math inline">W</span> is the weight matrix, <span class="math inline">c</span> is the number of classes, and <span class="math inline">P(y = j | x; W)</span> is the probability that the input <span class="math inline">x</span> belongs to class <span class="math inline">j</span>.</p>
<p>The optimization problem for softmax regression is to minimize the negative log-likelihood:</p>
<p><span class="math display">
\min_{W \in \mathbb{R}^{c \times d}} -\sum_{i=1}^{N} \log P(y_i | x_i; W) + \lambda \| W \|_1
</span></p>
<p>where <span class="math inline">N</span> is the number of training examples, <span class="math inline">\lambda</span> is the regularization parameter, and <span class="math inline">\| W \|_1</span> is the L1 norm of the weight matrix, which promotes sparsity in the solution. I suggest you to vectorize matrix and add <span class="math inline">1</span>-vector norm.</p>
<p>We will solve the sparse softmax regression problem using the subgradient method and the proximal gradient method, both incorporating L1 regularization. The proximal gradient method is particularly useful for optimization problems involving non-smooth regularizers like the L1 norm. We will use 3 class classification problem of <a href="https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success">Predicting Students‚Äô Dropout and Academic Success</a>. In this problem you should start with this <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Proximal_softmax_regression.ipynb">colab notebook</a></p>
<ol type="1">
<li>[4 points] Write down exact formulation of subgradient method and proximal gradient method here (you can not use any optimization problems in this formulation).</li>
<li>[6 points] Choose <span class="math inline">\lambda = 0</span>. Solve the softmax regression problem using subgradient method and proximal gradient descent. Find the highest learning (individually), acceptable for both methods to converge. Report convergence curves and report final sparsity of both methods. Draw you conclusions.</li>
<li>[10 points] Solve non-smooth problem and fill the following table. For each value of <span class="math inline">\lambda</span> provide convergence curves.</li>
</ol>
<p>Report the number of iterations needed to reach specified primal gaps for each method. Present the results in the following markdown table:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 13%">
<col style="width: 12%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 16%">
<col style="width: 5%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Learning Rate (<span class="math inline">\eta</span>)</th>
<th style="text-align: right;">Tolerance (<span class="math inline">\epsilon</span>)</th>
<th style="text-align: center;">Number of Iterations</th>
<th style="text-align: center;">Comment(if any)</th>
<th style="text-align: center;">Final Sparsity of the solution</th>
<th style="text-align: left;"><span class="math inline">\lambda</span></th>
<th style="text-align: center;">Final test accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">subgradient method</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-1}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">subgradient method</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">subgradient method</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-3}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">subgradient method</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-4}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">subgradient method</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-5}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-1}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-3}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-4}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-5}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-2</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">subgradient method</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-3</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-3</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">subgradient method</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-1</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1e-1</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">subgradient method</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Proximal Gradient Descent</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"><span class="math inline">10^{-2}</span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"><code>1</code></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table></li>
</ol>
</section>
<section id="stochastic-gradient-methods" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-gradient-methods">Stochastic gradient methods</h3>
<ol type="1">
<li><p><strong>Variance reduction for stochastic gradient methods</strong>. [20 points]</p>
<p>[5 points]Open <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/VR_exercise.ipynb">colab notebook</a>. Implement SAG and SVRG method. Consider Linear least squares problem with the following setup</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="dv">0</span>,</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">50</span>,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">100</span>,</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">1000</span>,</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">50</span>,</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">1000</span>,</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SAG"</span>,</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">1000</span>,</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SVRG"</span>,</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">"epoch_length"</span>: <span class="dv">2</span>,</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">1000</span>,</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiments(params)</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div>
<p>[5 points] Then, consider strongly convex case with:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="fl">1e-1</span>,</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">50</span>,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">100</span>,</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">50</span>,</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SAG"</span>,</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SVRG"</span>,</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">"epoch_length"</span>: <span class="dv">2</span>,</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div>
<p>[5 points] And for the convex binary logistic regression:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="dv">0</span>,</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">100</span>,</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">200</span>,</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SAG"</span>,</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SVRG"</span>,</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>            <span class="st">"epoch_length"</span>: <span class="dv">3</span>,</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">1e-2</span>,</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">100</span>,</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">2000</span>,</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div>
<p>[5 points] and strongly convex case</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mu"</span>: <span class="fl">1e-1</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"m"</span>: <span class="dv">100</span>,</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n"</span>: <span class="dv">200</span>,</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"methods"</span>: [</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">2e-2</span>,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">3000</span>,</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SAG"</span>,</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">2e-2</span>,</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">3000</span>,</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SVRG"</span>,</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">2e-2</span>,</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>            <span class="st">"epoch_length"</span>: <span class="dv">3</span>,</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">2</span>,</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">3000</span>,</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>            <span class="st">"method"</span>: <span class="st">"SGD"</span>,</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>            <span class="st">"learning_rate"</span>: <span class="fl">2e-2</span>,</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">"batch_size"</span>: <span class="dv">100</span>,</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">"iterations"</span>: <span class="dv">3000</span>,</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div>
<p>Describe the obtained convergence and compare methods.</p>
<p><img src="lls_VR.svg" class="img-fluid"></p>
<p><img src="logreg_VR.svg" class="img-fluid"></p></li>
</ol>
<!-- 1. **Do we need to tune hyperparameters for stochastic gradient methods?** [15 points]

    The performance of stochastic gradient-based optimizers, such as Adam and AdamW, is highly dependent on their hyperparameters. While the learning rate together with the momentum term are often the most tuned parameters, other hyperparameters like the exponential decay rates for the moment estimates, `beta1` and `beta2`, can also have an impact on training dynamics and final model performance. In this problem, you will investigate the sensitivity of Adam and AdamW to these beta parameters when training a small Transformer model.

    Your task is to train a small Transformer on the TinyStories dataset and perform a grid search over `beta1` and `beta2` for both `optax.adam` and `optax.adamw`. You will then visualize the results as heatmaps to analyze the performance landscape of these hyperparameters.

    **Tasks:**

    1.  **[5 points] Implement the basic training loop:**
        *   Define a small Transformer model. You can use the tutorial [here](https://docs.jaxstack.ai/en/latest/JAX_for_LLM_pretraining.html).
        *   Load and preprocess a subset of the TinyStories dataset.
        *   Create a training loop that, for a given set of hyperparameters (`beta1`, `beta2`), trains the model for a fixed number of epochs (e.g., 1).
        *   For each method find a set of hyperparameters (try the default values) and training length, that satisfies the loss value on the test set to be less than $3.14$.

    2.  **[10 points] Visualize and Analyze:**
        *   Perform a grid search for `beta1` and `beta2` for both `optax.adam` and `optax.adamw`. Suggested grid:
            *   `beta1_values = jnp.linspace(0.8, 0.99, 10)`
            *   `beta2_values = jnp.linspace(0.8, 0.9999, 10)`
        *   For each pair of (`beta1`, `beta2`), record the final training loss and test loss.
        *   For each optimizer (Adam and AdamW), create two 2D heatmaps (4 in total):
            1.  A heatmap showing the final **training loss** for each (`beta1`, `beta2`) pair.
            2.  A heatmap showing the final **test loss** for each (`beta1`, `beta2`) pair.
        *   The axes of the heatmaps should correspond to `beta1` and `beta2` values. The color of each cell should represent the loss.
        *   Analyze the heatmaps. Which hyperparameter settings yield the best performance for each optimizer? Are the optimizers sensitive to changes in `beta1` and `beta2`? Do you observe significant differences between Adam and AdamW in terms of their sensitivity or optimal beta values?

    You can use the following code as a starting point. You can use any other libraries you want, i.e. you are not restricted to JAX. 

    ```python
    import jax
    import jax.numpy as jnp
    import optax
    import flax.nnx as nnx
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from tqdm.auto import tqdm
    import requests
    import tiktoken
    from sklearn.model_selection import train_test_split

    # --- Model Definition ---
    def causal_attention_mask(seq_len):
        return jnp.tril(jnp.ones((seq_len, seq_len)))

    class TransformerBlock(nnx.Module):
        def __init__(self, embed_dim, num_heads, ff_dim, rngs: nnx.Rngs, rate: float = 0.1):
            self.mha = nnx.MultiHeadAttention(num_heads=num_heads, in_features=embed_dim, rngs=rngs)
            self.dropout1 = nnx.Dropout(rate=rate, rngs=rngs)
            self.layer_norm1 = nnx.LayerNorm(num_features=embed_dim, rngs=rngs)
            self.linear1 = nnx.Linear(in_features=embed_dim, out_features=ff_dim, rngs=rngs)
            self.linear2 = nnx.Linear(in_features=ff_dim, out_features=embed_dim, rngs=rngs)
            self.dropout2 = nnx.Dropout(rate=rate, rngs=rngs)
            self.layer_norm2 = nnx.LayerNorm(num_features=embed_dim, rngs=rngs)

        def __call__(self, inputs, training: bool = False):
            mask = causal_attention_mask(inputs.shape[1])
            attention_output = self.mha(inputs_q=inputs, mask=mask, decode=False)
            attention_output = self.dropout1(attention_output, deterministic=not training)
            out1 = self.layer_norm1(inputs + attention_output)
            ffn_output = self.linear2(nnx.relu(self.linear1(out1)))
            ffn_output = self.dropout2(ffn_output, deterministic=not training)
            return self.layer_norm2(out1 + ffn_output)

    class TokenAndPositionEmbedding(nnx.Module):
        def __init__(self, maxlen, vocab_size, embed_dim, rngs: nnx.Rngs):
            self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)
            self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)

        def __call__(self, x):
            positions = jnp.arange(0, x.shape[1])[None, :]
            return self.token_emb(x) + self.pos_emb(positions)

    class MiniGPT(nnx.Module):
        def __init__(self, maxlen, vocab_size, embed_dim, num_heads, ff_dim, num_blocks, rngs: nnx.Rngs):
            self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim, rngs=rngs)
            self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim, rngs=rngs) for _ in range(num_blocks)]
            self.output_layer = nnx.Linear(in_features=embed_dim, out_features=vocab_size, rngs=rngs)

        def __call__(self, inputs, training: bool = False):
            x = self.embedding_layer(inputs)
            for block in self.transformer_blocks:
                x = block(x, training=training)
            return self.output_layer(x)

    # --- Data Loading ---
    def load_and_preprocess_data(maxlen, num_samples=5000):
        url = 'https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true'
        response = requests.get(url)
        text = response.text
        stories = text.split('<|endoftext|>')
        stories = [story.strip() for story in stories if story.strip()]
        
        tokenizer = tiktoken.get_encoding("gpt2")
        
        # Use a subset of data for speed
        data = [tokenizer.encode(s, allowed_special={'<|endoftext|>'}) for s in stories[:num_samples]]
        
        # Pad sequences
        padded_data = np.zeros((len(data), maxlen), dtype=int)
        for i, d in enumerate(data):
            seq_len = min(len(d), maxlen)
            padded_data[i, :seq_len] = d[:seq_len]

        train_data, test_data = train_test_split(padded_data, test_size=0.2, random_state=42)
        return train_data, test_data, tokenizer

    # --- Training and Evaluation ---
    def loss_fn(model, batch):
        inputs, targets = batch
        logits = model(inputs, training=True)
        loss = optax.softmax_cross_entropy_with_integer_labels(
            logits=logits, labels=targets).mean()
        return loss

    @nnx.jit
    def train_step(model: MiniGPT, optimizer: nnx.Optimizer, batch):
        grad_fn = nnx.value_and_grad(loss_fn)
        loss, grads = grad_fn(model, batch)
        optimizer.update(grads)
        return loss

    @nnx.jit
    def eval_step(model: MiniGPT, batch):
        inputs, targets = batch
        logits = model(inputs, training=False)
        return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=targets).mean()

    def create_batches(data, batch_size):
        num_batches = len(data) // batch_size
        for i in range(num_batches):
            batch_data = data[i*batch_size:(i+1)*batch_size]
            inputs = batch_data[:, :-1]
            targets = batch_data[:, 1:]
            yield inputs, targets
    
    def get_full_ds_loss(model, data, batch_size):
        losses = []
        for batch in create_batches(data, batch_size):
            losses.append(eval_step(model, batch))
        return jnp.mean(jnp.array(losses))


    def train_and_evaluate(optimizer_name, beta1, beta2, num_epochs, model_params, data, batch_size):
        rngs = nnx.Rngs(0)
        model = MiniGPT(**model_params, rngs=rngs)
        
        if optimizer_name == 'adam':
            optimizer = nnx.Optimizer(model, optax.adam(learning_rate=1e-3, b1=beta1, b2=beta2))
        else:
            optimizer = nnx.Optimizer(model, optax.adamw(learning_rate=1e-3, b1=beta1, b2=beta2))
        
        train_data, test_data = data

        for epoch in range(num_epochs):
            # Simple shuffling
            key = jax.random.PRNGKey(epoch)
            perm = jax.random.permutation(key, len(train_data))
            train_data_shuffled = train_data[perm]
            
            for batch in create_batches(train_data_shuffled, batch_size):
                train_step(model, optimizer, batch)

        final_train_loss = get_full_ds_loss(model, train_data, batch_size)
        final_test_loss = get_full_ds_loss(model, test_data, batch_size)
        
        return final_train_loss, final_test_loss

    def plot_heatmap(data, title, xlabel, ylabel, xticklabels, yticklabels):
        ### YOUR CODE HERE ###
        plt.show()

    # --- Main Execution ---
    if __name__ == '__main__':
        # Model and training parameters
        maxlen = 128
        embed_dim = 128
        num_heads = 4
        ff_dim = 128
        num_blocks = 2
        num_epochs = 3
        batch_size = 64
        
        train_data, test_data, tokenizer = load_and_preprocess_data(maxlen)
        vocab_size = tokenizer.n_vocab

        model_params = {
            "maxlen": maxlen, "vocab_size": vocab_size, "embed_dim": embed_dim,
            "num_heads": num_heads, "ff_dim": ff_dim, "num_blocks": num_blocks
        }

        beta1_values = jnp.linspace(0.8, 0.99, 10)
        beta2_values = jnp.linspace(0.8, 0.9999, 10)

        for optimizer_name in ['adam', 'adamw']:
            print(f"--- Tuning {optimizer_name.upper()} ---")
            train_losses = np.zeros((len(beta2_values), len(beta1_values)))
            test_losses = np.zeros((len(beta2_values), len(beta1_values)))

            for i, beta2 in enumerate(tqdm(beta2_values, desc=f'{optimizer_name} beta2')):
                for j, beta1 in enumerate(tqdm(beta1_values, desc=f'{optimizer_name} beta1')):
                    train_loss, test_loss = train_and_evaluate(
                        optimizer_name, beta1, beta2, num_epochs, 
                        model_params, (train_data, test_data), batch_size
                    )
                    train_losses[i, j] = train_loss
                    test_losses[i, j] = test_loss

            plot_heatmap(train_losses, f'{optimizer_name.upper()} - Final Train Loss',
                         'beta1', 'beta2', beta1_values, beta2_values)
            plot_heatmap(test_losses, f'{optimizer_name.upper()} - Final Test Loss',
                         'beta1', 'beta2', beta1_values, beta2_values)
    ``` -->
</section>
<section id="neural-network-training" class="level3">
<h3 class="anchored" data-anchor-id="neural-network-training">Neural network training</h3>
<ol type="1">
<li><p><strong>Anomaly detection with neural network.</strong> [30 points]</p>
<p>In this problem we will try to detect anomalies in time series with neural network.</p>
<p><strong>[Plotly Filter Error: Could not read file: anomaly_detection.html. Error: anomaly_detection.html: No such file or directory]</strong></p>
<p>We will train the model to reconstruct normal data and when the reconstruction error for the actual data on trained model is high, we report an anomaly. Start with this notebook <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/time_series_anomaly.ipynb">colab notebook</a>. The default solution is adam and after training it can detect 4 out of 5 anomalies. Train and compare several methods on the same problem. For each method try to find hyperparameters, which ensures at least 3 out of 5 anomalies detection. Present learning curves and anomaly predictions for each method.</p>
<ul>
<li>SGD with momentum [5 points] from optax</li>
<li>Adadelta [5 points] from optax</li>
<li>BFGS [10 points] implemented manually</li>
<li><a href="https://github.com/KellerJordan/Muon">Muon</a> <a href="https://arxiv.org/pdf/2502.16982">optimizer</a> [10 points] implemented manually</li>
</ul></li>
</ol>
</section>
<section id="big-models" class="level3">
<h3 class="anchored" data-anchor-id="big-models">Big models</h3>
<ol type="1">
<li><p><strong>Fit the largest model you can on a single GPU.</strong> [15 points]</p>
<p>In this assignment, you will train a language model (LM) using the TinyStories dataset, focusing on optimizing model performance within the constraints of Google Colab‚Äôs hardware. For the sake of speed, we will do it on the part of the dataset.</p>
<pre class="tiny_stories"><code>Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. 
Beep was a healthy car because he always had good fuel....</code></pre>
<p>Your objective is to maximize the size of the model without exceeding the available computational resources (~ 16GB VRAM). You could start with the Hugging Face Transformers library and experiment with various memory optimization techniques, such as (but not limited to):</p>
<ul>
<li>Different batch size</li>
<li>Different optimizer</li>
<li>Gradient accumulation</li>
<li>Activation checkpointing</li>
<li>CPU offloading</li>
<li>8bit optimizers</li>
</ul>
<p>You have a baseline of training <code>gpt-2</code> model prepared at the following <a href="https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/TinyStories_baseline.ipynb">colab notebook</a>. You can easily switch it to <code>opt-350m</code>, <code>opt-1.3b</code>, <code>gpt2</code> etc. You can find a great beginner-level guide on the topic <a href="https://huggingface.co/docs/transformers/v4.18.0/en/performance">here</a>.</p>
<pre class="gpt2"><code>A long time ago in a galaxy far far away... a little girl named Lily was playing in the garden. She was so excited! She wanted to explore the garden and see what was around her.
Suddenly, she heard a loud noise. Lily looked up and saw a big, hairy creature. Lily was so excited! She ran to the creature and grabbed it by the arm. The creature was so big and hairy that Lily couldn't help but laugh. </code></pre>
<p><img src="gpt2_generation.jpeg" class="img-fluid"></p>
<p>You have to fill this table with your description/observations.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 21%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Setup</th>
<th style="text-align: center;"># of parameters</th>
<th style="text-align: center;">GPU peak memory, MB</th>
<th style="text-align: center;">Final eval loss</th>
<th style="text-align: center;">Batch Size</th>
<th style="text-align: center;">Time to run 5 epochs, s</th>
<th style="text-align: center;">Generation example</th>
<th style="text-align: center;">Comment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Baseline (OPT-125M)</td>
<td style="text-align: center;">125 M</td>
<td style="text-align: center;">9044</td>
<td style="text-align: center;">1.928</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">442.34</td>
<td style="text-align: center;"><code>A long time ago in a galaxy far far away... there was a little girl named Lily. She was three years old and loved to explore. One day, she decided to go for a walk in the park. Lily was so excited to go for a walk. She asked her mom, "What do you want to do?" Her mom smiled and said, "I want to explore the galaxy." Lily was so excited to explore the galaxy.</code></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Baseline (GPT2-S)</td>
<td style="text-align: center;">124 M</td>
<td style="text-align: center;">13016</td>
<td style="text-align: center;">2.001</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">487.75</td>
<td style="text-align: center;"><code>A long time ago in a galaxy far far away... a little girl named Lily was playing in the garden. She was so excited! She wanted to explore the garden and see what was around her. Suddenly, she heard a loud noise. Lily looked up and saw a big, hairy creature. Lily was so excited! She ran to the creature and grabbed it by the arm. The creature was so big and hairy that Lily couldn't help but laugh.</code></td>
<td style="text-align: center;">The generation seems more interesting, despite the fact, that eval loss is higher.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>For each unique trick for memory optimization, you will get 3 points (maximum 15 points). A combination of tricks is not counted as a unique trick, but will, probably, be necessary to train big models. The maximum grade is bounded with the size of the trained model:</p>
<ul>
<li>If the model size you train is &lt;= 125M - you can get a maximum of 6 points.</li>
<li>If the model size you train is 126M &lt;= 350M - you can get a maximum of 8 points.</li>
<li>If the model size you train is 350M &lt;= 1B - you can get a maximum of 12 points.</li>
<li>If you fit 1B model or more - you can get a maximum 15 points.</li>
</ul></li>
</ol>
</section>
<section id="admm-dual-methods" class="level3">
<h3 class="anchored" data-anchor-id="admm-dual-methods">ADMM (Dual methods)</h3>
<ol type="1">
<li><p><strong>Low‚ÄëRank Matrix Completion via ADMM</strong> [25 points]</p>
<p><strong>Background.</strong> In many applications such as recommender systems, computer vision and system identification, the data matrix is approximately low‚Äërank but only a subset of its entries are observed. Recovering the missing entries can be posed as a convex program that combines a data‚Äëfitting term with the nuclear norm, a convex surrogate for rank.</p>
<p>We are given a partially observed matrix <span class="math inline">M \in \mathbb{R}^{m\times n}</span> and the index set of observed entries <span class="math inline">\Omega \subseteq \{1,\dots,m\} \times \{1,\dots,n\}</span>. Define the sampling operator <span class="math inline">P_\Omega : \mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}</span> by <span class="math inline">(P_\Omega(X))_{ij}= X_{ij}</span> if <span class="math inline">(i,j)\in\Omega</span> and <span class="math inline">0</span> otherwise.</p>
<p>We consider the optimization problem <span class="math display">
\min_{X\in\mathbb{R}^{m\times n}}\;\frac12\|P_\Omega(X-M)\|_F^2\; + \;\lambda\|X\|_*,
</span> where <span class="math inline">\|X\|_* = \sum_k \sigma_k(X)</span> is the nuclear norm.</p>
<ul>
<li><p><strong>(a) [10 points] Derive a two‚Äëblock ADMM algorithm.</strong><br>
Introduce an auxiliary variable <span class="math inline">Z</span> and rewrite the problem in the form<br>
<span class="math display">
\min_{X,Z}\; \frac12\|P_\Omega(Z-M)\|_F^2 + \lambda\|X\|_* \quad\text{s.t. } X-Z = 0.
</span> Derive explicit closed‚Äëform expressions for each ADMM update:</p>
<ul>
<li><strong><span class="math inline">X</span>‚Äëupdate:</strong> singular‚Äëvalue soft‚Äëthresholding (SVT);</li>
<li><strong><span class="math inline">Z</span>‚Äëupdate:</strong> projection onto the observed entries (keep <span class="math inline">M</span> on <span class="math inline">\Omega</span>, average with <span class="math inline">X</span> elsewhere);</li>
<li><strong>dual‚Äëvariable update.</strong></li>
</ul>
<p>State a practical stopping rule based on the primal and dual residuals.</p></li>
<li><p><strong>(b) [10 points] Implement the algorithm on synthetic data.</strong><br>
Use the following set‚Äëup (in Python):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>m, n, r <span class="op">=</span> <span class="dv">50</span>, <span class="dv">40</span>, <span class="dv">3</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> np.random.randn(m, r)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> np.random.randn(n, r)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>M_star <span class="op">=</span> U <span class="op">@</span> V.T                      <span class="co"># ground‚Äëtruth low‚Äërank matrix</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.random.rand(m, n) <span class="op">&lt;</span> <span class="fl">0.3</span>     <span class="co"># 30 % observations</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> <span class="fl">0.01</span> <span class="op">*</span> np.random.randn(m, n)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> mask <span class="op">*</span> (M_star <span class="op">+</span> noise)           <span class="co"># observed matrix (zeros elsewhere)</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>lambda_ <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> np.sqrt(<span class="bu">max</span>(m, n))</span></code></pre></div><button title="–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç" class="code-copy-button"><i class="bi"></i></button></div>
<ol type="1">
<li>Implement the ADMM algorithm derived in part (a).</li>
<li>Run it from <span class="math inline">X^0 = 0</span> for three penalty parameters <span class="math inline">\rho \in \{0.1, 1, 10\}</span>.</li>
<li>For each <span class="math inline">\rho</span>:
<ul>
<li>plot <strong>(i)</strong> the objective value and <strong>(ii)</strong> the relative reconstruction error <span class="math inline">\frac{\|X^k - M_\star\|_F}{\|M_\star\|_F}</span> versus iteration number;</li>
<li>report the number of iterations required until <span class="math inline">\max(\|r_{\mathrm p}^k\|_F,\|r_{\mathrm d}^k\|_F) \le 10^{-3}</span>.</li>
</ul></li>
</ol></li>
<li><p><strong>(c) [5 points] Discussion.</strong><br>
Compare the convergence behaviour across the three values of <span class="math inline">\rho</span>. How does <span class="math inline">\rho</span> influence the rate at which the primal and dual residuals decrease? Comment on</p>
<ul>
<li>the rank of the iterates (after SVT);</li>
<li>the trade‚Äëoff between data‚Äëfit and nuclear‚Äënorm penalty as <span class="math inline">\lambda</span> varies;</li>
<li>the quality of the reconstruction once the stopping criterion is met.</li>
</ul>
<p>Relate your observations to the theory of ADMM and to the sensitivity of singular‚Äëvalue thresholding to the choice of <span class="math inline">\rho</span>.</p></li>
</ul></li>
</ol>
</section>
<section id="bonus-continuous-time-methods" class="level3">
<h3 class="anchored" data-anchor-id="bonus-continuous-time-methods">Bonus: Continuous time methods</h3>
<ol type="1">
<li><p><strong>SGD as a splitting scheme and the importance of batches order</strong> [30 points]</p>
<p><strong>Background: (to be honest you can do the task without reading it)</strong></p>
<p>The standard Gradient Descent (GD) method for minimizing <span class="math inline">f(x) = \frac{1}{n} \sum_{i=1}^n f_i(x)</span> can be viewed as an Euler discretization of the gradient flow Ordinary Differential Equation (ODE): <span class="math display">
\frac{d x}{d t} = -\nabla f(x) = -\frac{1}{n} \sum_{i=1}^n \nabla f_i(x)
</span> Stochastic Gradient Descent (SGD), particularly with cycling through mini-batches without replacement, can be interpreted as a <em>splitting scheme</em> applied to this ODE. In a first-order splitting scheme for <span class="math inline">\frac{dx}{dt} = A x = \sum_{i=1}^m A_i x</span>, we approximate the solution <span class="math inline">x(h)</span> by sequentially applying the flows corresponding to each <span class="math inline">A_i</span>: <span class="math inline">x(h) \approx e^{A_{\sigma(m)} h} \ldots e^{A_{\sigma(1)} h} x_0</span> for some permutation <span class="math inline">\sigma</span>.</p>
<p>In the <a href="https://arxiv.org/abs/2004.08981">paper</a> authors show that for the linear least squares problem <span class="math inline">f(x) = \frac{1}{2n}\|X x - y\|^2</span>, where <span class="math inline">X</span> is split into <span class="math inline">m</span> row blocks <span class="math inline">X_i</span>, the corresponding ODE involves matrices <span class="math inline">A_i = -\frac{1}{n} X_i^T X_i</span>. If <span class="math inline">X_i^T = Q_i R_i</span> is the QR decomposition (<span class="math inline">Q_i</span> has orthonormal columns), let <span class="math inline">\Pi_i = I - Q_i Q_i^*</span> be the projector onto the null space of <span class="math inline">X_i</span>. The paper presents the following result for the asymptotic global error of the splitting scheme:</p>
<p><strong>Theorem:</strong> Let <span class="math inline">A_i = -\frac{1}{n} X_i^T X_i</span> for <span class="math inline">i=1,\dots,m</span>. Assume each <span class="math inline">A_i</span> is negative semidefinite and does not have full rank, but their sum <span class="math inline">A = \sum A_i</span> does have full rank. Then, for any permutation <span class="math inline">\sigma</span> of <span class="math inline">\{1, \dots, m\}</span>: <span class="math display">
\lim_{t \to \infty}\| e^{A_{\sigma(m)}t} \cdots e^{A_{\sigma(1)}t} - e^{At}\| = \left\|\prod_{i=1}^m \Pi_{\sigma(i)}\right\|
</span></p>
<p>This error bound depends on the product of projectors <span class="math inline">\Pi_i</span> and thus <em>on the order</em> specified by the permutation <span class="math inline">\sigma</span>. Since one epoch of SGD corresponds to applying the Euler discretization of each local problem <span class="math inline">\frac{dx}{dt} = A_i x</span> sequentially, this suggests that the order in which batches are processed in SGD might affect convergence, especially over many epochs.</p>
<p><strong>Tasks:</strong></p>
<ol type="1">
<li><strong>Investigating the Bound Distribution</strong> [5 points]
<ul>
<li>Consider a simple linear least squares problem. <span class="math display">
\frac{1}{2n}\|X \theta - y\|^2 \to \min_{\theta \in \mathbb{R}^{d}}, X \in \mathbb{R}^{n \times d}, y \in \mathbb{R}^n
</span> For example, generate a random matrix <span class="math inline">X \in \mathbb{R}^{80 \times 20}</span> and a random vector <span class="math inline">y \in \mathbb{R}^{80}</span>.</li>
<li>Split <span class="math inline">X</span> into <span class="math inline">m=8</span> batches (row blocks) sequentially <span class="math inline">X_1, \ldots, X_8</span>, where each <span class="math inline">X_i \in \mathbb{R}^{10 \times 20}</span>.</li>
<li>For each batch <span class="math inline">X_i \in \mathbb{R}^{10 \times 20}</span>, you have to compute the projector matrix <span class="math inline">\Pi_i = I - Q_i Q_i^* \in \mathbb{R}^{20 \times 20}</span>, where <span class="math inline">X_i^T = Q_i R_i</span> is the (thin) QR decomposition of <span class="math inline">X_i^T \in \mathbb{R}^{20 \times 10}</span> (<span class="math inline">Q_i \in \mathbb{R}^{20 \times r_i}, R_i \in \mathbb{R}^{r_i \times 10}</span>, with <span class="math inline">r_i = \text{rank}(X_i) \le 10</span>).</li>
<li>Calculate the error bound norm <span class="math inline">E(\sigma) = \|\prod_{j=1}^m \Pi_{\sigma(j)}\|_2</span> (where the product is a <span class="math inline">20 \times 20</span> matrix) for <em>all</em> <span class="math inline">m! = 8! = 40320</span> possible permutations <span class="math inline">\sigma</span>. Note, that this quantity is a scalar and depends on the order of batches in multiplication (permutation <span class="math inline">\sigma</span>), i.e.&nbsp;<span class="math inline">\|\Pi_1 \Pi_2\| \neq \|\Pi_2 \Pi_1\|</span>.</li>
<li>Plot a histogram of the distribution of these scalar <span class="math inline">E(\sigma)</span> values. Does the order seem to matter significantly in this random case?</li>
</ul></li>
<li><strong>Maximizing Order Dependence with adversarial dataset construction</strong> [20 points]
<ul>
<li>Modify the structure of the matrix <span class="math inline">X</span> (or the way it is split into <span class="math inline">X_i</span>, but you cannot change the number of batches and their size) from Task 1 to create a scenario where the distribution of the error bounds <span class="math inline">E(\sigma)</span> has a significantly larger variance (to be precise, the ratio of the maximum to minimum values for different permutations should be maximized). <em>Hint: Think about how the projectors <span class="math inline">\Pi_i</span> interact. How could you make the product <span class="math inline">\Pi_{\sigma(m)} \cdots \Pi_{\sigma(1)}</span> very different for different orders <span class="math inline">\sigma</span>? Consider cases where the null spaces have specific overlaps or orthogonality properties.</em></li>
<li>Explain your reasoning for the modification.</li>
<li>Repeat the calculation and plotting from Task 1 for your modified problem to demonstrate the increased variance in the error bounds. Report the ratio of the maximum to minimum values for different permutations before and after adversarial dataset construction.</li>
</ul></li>
<li><strong>Testing SGD Convergence</strong> [5 points]
<ul>
<li>Using the adversarial dataset from Task 2, identify two specific permutations: <span class="math inline">\sigma_{\text{low}}</span> with a low error bound <span class="math inline">E(\sigma_{\text{low}})</span> and <span class="math inline">\sigma_{\text{high}}</span> with a high error bound <span class="math inline">E(\sigma_{\text{high}})</span>.</li>
<li>Implement SGD for the linear least squares problem <span class="math inline">\min_x \frac{1}{2n} \|Xx - y\|^2</span>. Use a fixed, small learning rate (e.g., <span class="math inline">\alpha = 0.01/L</span> where <span class="math inline">L</span> is the Lipschitz constant of the full gradient).</li>
<li>Run SGD for a sufficient number of epochs (e.g., 50-100), applying the batches <em>deterministically</em> according to the order defined by <span class="math inline">\sigma_{\text{low}}</span> in each epoch. Record the squared error <span class="math inline">\|X x_k - y\|^2</span> at the end of each epoch <span class="math inline">k</span>.</li>
<li>Repeat the SGD run using the fixed batch order defined by <span class="math inline">\sigma_{\text{high}}</span>.</li>
<li>Plot the convergence curves (squared error vs.&nbsp;epoch number) for both <span class="math inline">\sigma_{\text{low}}</span> and <span class="math inline">\sigma_{\text{high}}</span> on the same graph.</li>
<li>Discuss your results. Does the observed convergence speed of SGD correlate with the theoretical asymptotic error bound <span class="math inline">E(\sigma)</span>? Does the order of batches appear to matter more in your modified problem compared to the random one?</li>
</ul></li>
</ol></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "–°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "–°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/cu25\.fmin\.xyz");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/MerkulovDaniil/cu25/edit/main/homework.md" class="toc-action"><i class="bi bi-github"></i>–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É</a></li></ul></div></div></div></footer><script>videojs(video_shortcode_videojs_video1);</script>




</body></html>