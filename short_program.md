# Программа курса "Оптимизация для всех"

| № | Тема Лекции | Тема практического занятия |
|---|-------------|---------------------------|
| 1 | Вспоминаем линейную алгебру. Скорость сходимости | Матричные вычисления, нормы, LoRA, анализ сходимости |
| 2 | Матричное дифференцирование и одномерная оптимизация | Градиенты матричных функций, line search методы, Brent method |
| 3 | Автоматическое дифференцирование | Forward/reverse mode AD, gradient checkpointing, JAX практикум |
| 4 | Выпуклость и сильная выпуклость | Доказательства выпуклости, PL-условие, логистическая регрессия, SVM |
| 5 | Условия оптимальности и KKT | Множители Лагранжа, KKT условия, adversarial attacks |
| 6 | Линейное программирование и симплекс-алгоритм | Планирование производства, max-flow min-cut, применения ЛП |
| 7 | Градиентный спуск и анализ сходимости | Теоретический анализ, выбор размера шага, PL-условие |
| 8 | Ускорение градиентного спуска | Heavy Ball, Nesterov acceleration, hobbit village challenge |
| 9 | Метод сопряженных градиентов | CG для квадратичных задач, нелинейный CG, патологические примеры |
| 10 | Метод Ньютона и квази-Ньютоновские методы | Newton method, BFGS, L-BFGS, cubic-regularized Newton |
| 11 | Проекционные методы и Frank-Wolfe | Проекция на симплекс, PGD, Frank-Wolfe algorithm |
| 12 | Субградиенты и проксимальные методы | Субдифференциалы, ISTA/FISTA, image denoising, L1-регуляризация |
| 13 | Стохастический градиентный спуск | SGD vs GD, адаптивные методы (Adam, AdaGrad, RMSProp, AdamW) |
| 14 | Методы уменьшения дисперсии | SVRG, SAG/SAGA, эксперименты с нейросетями |
| 15 | Современные методы и практики оптимизации | SAM, mode connectivity, grokking, Muon optimizer, distributed training |