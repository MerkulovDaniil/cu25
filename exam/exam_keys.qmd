---
lang: ru
format: 
    pdf:
        pdf-engine: xelatex
        include-in-header: ../files/exam_header.tex  # Custom LaTeX commands and preamble
---

# Определения и формулировки

1.  Положительно определённая матрица.

    :::{.callout-tip appearance="simple"}
    Матрица $A\in \mathbb{S}$ называется положительно (отрицательно) определённой, если для $\forall x\neq 0:\; x^TAx > (<) 0$. Обозначение: $A \succ0\; (A \prec 0)$.

    Аналогично определяется полуопределённость, только там неравенства нестрогие.
    :::
1.  Евклидова норма вектора.

    :::{.callout-tip appearance="simple"}
    $$
    ||x||_2 = \sqrt{\sum_{i=1}^{n}|x_i|^2}
    $$

    Данная норма соответствует расстоянию в реальном мире. Иначе называется 2-норма (см. p-норма вектора)
    :::
<!-- 1. Неравенство треугольника для нормы.
    :::{.callout-tip appearance="simple"}
    Норма должна удовлетворять следующим свойствам:

    1. $||\alpha x|| = |\alpha| ||x||$, $\alpha \in \mathbb{R}$
    2. $||x||=0 \; \Rightarrow \; x=0$
    3. $||x+y||\leq ||x||+||y||$ -- неравенство треугольника
    ::: -->
1.  $p$-норма вектора.

    :::{.callout-tip appearance="simple"}
    $$
    ||x||_p = \left(\sum_{i=1}^{n}|x_i|^{p}\right)^{\frac{1}{p}}
    $$

    Важные частные случаи:

    - Норма Чебышева:
    $||x||_{\infty} = \max\limits_{i} |x_i|$

    - Манхэттенское расстояние или $L1$ норма:
    $||x||_{1} = \sum_{i=1}^{n}|x_i|$
    :::
1.  Как выглядит единичный шар в $p$ - норме на плоскости для $p=1,2,\infty$?

    :::{.callout-tip appearance="simple"}
    ![Шары в разных нормах](p_balls.pdf)
    :::
1.  Норма Фробениуса для матрицы.

    :::{.callout-tip appearance="simple"}
    $$
    ||A||_{F} = \left(\sum\limits_{i = 1}^{m}\sum\limits_{j = 1}^{n} |a_{ij}|^2\right)^{\frac{1}{2}}
    $$
    :::
1.  Спектральная норма матрицы.

    :::{.callout-tip appearance="simple"}
    $$
    \|A\|_2 = \sup_{x \neq 0} \frac{\|Ax\|_2}{\|x\|_2} = \sigma_1 (A) = \sqrt{\lambda_{max}(A^\top A)}
    $$

    Где $\sigma_1 (A)$ -- старшее сингулярное значение $A$, $\lambda_{max}(A^\top A)$ -- наибольшее собственное значение $A^\top A$.
    :::
1.  Скалярное произведение двух векторов.

    :::{.callout-tip appearance="simple"}
    Пусть $x, y \in \mathbb{R}^{n}$, тогда их скалярное произведение это

    $$
    \langle x, y \rangle = x^Ty = \sum\limits_{i=1}^{n} x_iy_i = y^Tx = \langle y, x \rangle
    $$
    :::
1.  Скалярное произведение двух матриц, согласованное с нормой Фробениуса.

    :::{.callout-tip appearance="simple"}
    Пусть $X, Y \in \mathbb{R}^{m \times n}$, тогда их скалярное произведение это

    $$
    \langle X, Y \rangle = tr(X^TY) = \sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n} X_{ij}Y_{ij}= tr(Y^TX) = \langle Y, X \rangle
    $$

    Связь с нормой Фробениуса: $\langle X, X \rangle = ||X||^2_F$
    :::
1.  Собственные значения матрицы. Спектр матрицы.

    :::{.callout-tip appearance="simple"}
    Скаляр $\lambda$ является собственным значением для матрицы $A$, если существует вектор $q$, такой что $Aq = \lambda q$. В таком случае $q$ называют собственным вектором.

    Спектр матрицы -- совокупность её собственных значений.
    :::
1.  Связь спектра матрицы и её определенности.

    :::{.callout-tip appearance="simple"}
    Симметричная матрица положительно (неотрицательно) определена $\iff$ её спектр (все её собственные значения) положителен (неотрицателен).
    :::
1.  Спектральное разложение матрицы.

    :::{.callout-tip appearance="simple"}
    Спектральное разложение матрицы, или разложение матрицы на основе собственных векторов, — это представление квадратной матрицы $A$ в виде произведения трёх матриц $A=S\Lambda S^{-1}$, где $S$ — матрица, столбцы которой являются собственными векторами матрицы $A$, $\Lambda$ — диагональная матрица с соответствующими собственными значениями на главной диагонали. В таком виде могут быть представлены только матрицы, обладающие полным набором собственных векторов.

    ![Спектральное разложение матрицы](Spectral.pdf)
    :::
1.  Сингулярное разложение матрицы.

    :::{.callout-tip appearance="simple"}
    $A \in \mathbb{R}^{m \times n}$ , $rank~A = r$.
    $$
    A = U\Sigma V^T
    $$
    $U \in \mathbb{R}^{m \times r}$, $U^T U = I$, $V \in \mathbb{R}^{n \times r}$, $V^T V = I$, $\Sigma$ диагональная матрица
    $$
    \Sigma = \text{diag}(\sigma_1, \ldots, \sigma_r)
    $$
    такая что
    $$
    \sigma_1 \ge \sigma_2 \ge \ldots \ge \sigma_r > 0
    $$
    Столбцы $U$, $V$ - левые и правые сингулярные векторы $A$, $\sigma_i$ - сингулярные значения.

    $$
    A = \sum_{i=1}^r \sigma_i u_i v_i^T
    $$

    ![Сингулярное разложение матрицы](SVD.pdf)
    :::
1.  Связь определителя и собственных чисел для квадратной матрицы.

    :::{.callout-tip appearance="simple"}
    Если у матрицы $A$ собственные значения $\lambda_1, \lambda_2, \ldots, \lambda_n$, то её определитель равен:

    $$
    \det(A) = \lambda_1 \cdot \lambda_2 \cdot \ldots \cdot \lambda_n
    $$
    :::
1.  Связь следа и собственных чисел для квадратной матрицы.

    :::{.callout-tip appearance="simple"}
    Если у матрицы $A$ собственные значения $\lambda_1, \lambda_2, \ldots, \lambda_n$, то её след равен:

    $$
    \text{tr}(A) = \lambda_1 + \lambda_2 + \ldots + \lambda_n
    $$
    :::
1.  Линейная сходимость последовательности. 

    :::{.callout-tip appearance="simple"}
    Последовательность $\{r_k\}$ называется линейно сходящейся, если существуют константы $C>0$ и $q\in(0,1)$, такие что
    $$
    r_k \le C q^k,\quad k\ge m.
    $$
    Наименьшее возможное $q$ называется константой (скоростью) линейной сходимости.
    :::
1.  Сублинейная сходимость последовательности. 

    :::{.callout-tip appearance="simple"}
    Если последовательность $r_k$ сходится к нулю, но не обладает линейной сходимостью, то говорят, что она сходится сублинейно. Иногда мы можем рассматривать следующий класс сублинейной сходимости:
    $$
    r_k \leq Ck^q,
    $$

    где $q < 0$ и $0 < C < \infty$.
    :::
1.  Сверхлинейная сходимость последовательности. 

    :::{.callout-tip appearance="simple"}
    Мы определяем сверхлинейную сходимость как сходимость последовательности, которая быстрее любой линейной сходимости. Иногда рассматривают более специальный класс. Тогда говорят, что последовательность $r_k > 0$ имеет сверхлинейную сходимость порядка $p$, если существуют $C > 0$ и $p > 1$ такие, что выполнено для всех достаточно больших $k$:
    $$
    r_{k+1} \leq C r_k^p
    $$
    :::
1.  Квадратичная сходимость последовательности.

    :::{.callout-tip appearance="simple"}
    Говорят, что последовательность $r_k > 0$ сходится квадратично, если существует $C > 0$ такое, что выполнено для всех достаточно больших $k$:
    $$
    r_{k+1} \leq C r_k^2
    $$
    :::
1.  Рассмотрим итеративный процесс $x_k \in \mathbb{R}$, сходящийся к решению $x^* \in \mathbb{R}$ квадратично. Как изменяется количество верных значащих цифр в решении после одной итерации метода?

    :::{.callout-tip appearance="simple"}
    Удваивается.

    Пояснение: Пусть $x^* = 1.23456789012345$ (истинное решение), и итерационная последовательность начинается с ошибкой $r_k = 10^{-3}$, соответствующей 3 верным значащим цифрам ($1.234$).

    После первой итерации:
    $$
    r_{k+1} \approx r_k^2 = (10^{-3})^2 = 10^{-6}.
    $$
    Теперь ошибка равна $10^{-6}$, и мы имеем 6 верных значащих цифр ($1.23456$).

    После второй итерации:
    $$
    r_{k+2} \approx r_{k+1}^2 = (10^{-6})^2 = 10^{-12}.
    $$
    Теперь ошибка равна $10^{-12}$, и мы имеем 12 верных значащих цифр ($1.234567890123$).
    :::
1.  Тест корней для определения скорости сходимости последовательности.

    :::{.callout-tip appearance="simple"}
    Пусть $(r_k)_{k=m}^\infty$ - последовательность неотрицательных чисел, сходящаяся к нулю, и пусть $\alpha := \limsup_{k \to \infty} r_k^{1/k}$. (Заметим, что $\alpha \ge 0$.)

    1. Если $0 \le \alpha < 1$, то $(r_k)_{k=m}^\infty$ сходится линейно с константой $\alpha$.
    1. В частности, если $\alpha = 0$, то $(r_k)_{k=m}^\infty$ сходится сверхлинейно.
    1. Если $\alpha = 1$, то $(r_k)_{k=m}^\infty$ сходится сублинейно.
    1. Случай $\alpha > 1$ невозможен.
    :::
<!-- 1.  Тест отношений для определения скорости сходимости последовательности.

    :::{.callout-tip appearance="simple"}
    Пусть ${r_k}_{k=m}^\infty$ - последовательность строго положительных чисел, сходящаяся к нулю. Пусть
    $$
    q = \lim_{k \to \infty} \frac{r_{k+1}}{r_k}
    $$

    1. Если существует $q$ и $0 \le q < 1$, то ${r_k}_{k=m}^\infty$ имеет линейную сходимость с константой $q$.

    2. В частности, если $q = 0$, то ${r_k}_{k=m}^\infty$ имеет сверхлинейную сходимость.

    3. Если $q$ не существует, но $q = \lim_{k \to \infty} \sup_k \frac{r_{k+1}}{r_k} < 1$, то ${r_k}_{k=m}^\infty$ имеет линейную сходимость с константой, не превышающей $q$.

    4. Если $\lim_{k \to \infty} \inf_k \frac{r_{k+1}}{r_k} = 1$, то ${r_k}_{k=m}^\infty$ имеет сублинейную сходимость.

    5. Случай $\lim_{k \to \infty} \inf_k \frac{r_{k+1}}{r_k} > 1$ невозможен.
    ::: -->
1.  Унимодальная функция.

    :::{.callout-tip appearance="simple"}
    Функция $f(x)$ называется унимодальной на $[a, b]$, если существует $x^* \in [a, b]$, такое, что

    1. $f(x_1) > f(x_2)$ для всех $a \le x_1 < x_2 < x^*$

    2. $f(x_1) < f(x_2)$ для всех $x^* < x_1 < x_2 \le b$
    :::
1.  Метод дихотомии.

    :::{.callout-tip appearance="simple"}
    Решаемая задача: $\min_{x \in [a, b]} f(x)$.
    В начале берётся середина отрезка и одна из четвертей (например, левая). Тогда по результатам измерения значений функции в этих двух точках решение может оказаться либо в левой половине отрезка, либо в правых трёх четвертях. Во втором случае нам необходимо провести дополнительное измерение значения функции в правой четверти исходного отрезка, тогда мы гарантируем уменьшение области поиска вдвое. Таким образом, в методе дихотомии мы гарантируем уменьшение области поиска вдвое на каждой итерации, однако, на каждой итерации (кроме самой первой) может потребоваться не более двух измерений функции. 
    :::
1.  Метод золотого сечения.

    :::{.callout-tip appearance="simple"}
    Решаемая задача: $\min_{x \in [a, b]} f(x)$. На отрезке выбираются правая и левая точки золотого сечения. По результатам измерения значений функции в этих двух точках решение может оказаться либо от начала до правой точки золотого сечения, либо от левой точки золотого сечения до конца отрезка. В любом из этих двух случаев на следующей итерации метода, одна из предыдущих измеренных точек золотого сечения станет точкой (левая на предыдущей итерации станет правой на новой или правая на предыдущей итерации станет левой на новой) золотого сечения отрезка на следующей итерации. Таким образом, нам останется доизмерить значение функции не более, чем один раз за итерацию.   
    :::
<!-- 1.  Метод параболической интерполяции.

    :::{.callout-tip appearance="simple"}
    Идея метода: берем 3 точки, по этим 3 точкам однозначно строим параболу, находим ее минимум, и из этих 4 точек оставляем 3 так, чтобы между первой и третьей находился минимум.

    Сходится сверхлинейно, но метод довольно неустойчивый. Если $f(x)$ не похожа на параболу, гарантий сходимости нет.
    ::: -->
1.  Условие достаточного убывания для неточного линейного поиска.

    :::{.callout-tip appearance="simple"}
    Неточный линейный поиск:

    $$
    x_{k+1} = x_k - \alpha \nabla f(x_k), \\
    \alpha = \arg\min_{\alpha\ge 0} f(x_k - \alpha \nabla f(x_k)).
    $$

    Хотим приближенно найти $\alpha$. Сведем задачу к поиску минимума следующей функции:

    $$
    \phi(\alpha) = f(x_k - \alpha\nabla f(x_k)), \alpha \geq 0
    $$

    Приблизим ее через первые 2 члена ряда Тейлора:

    $$
    \phi(\alpha) \approx f(x_k) - \alpha\nabla f(x_k)^\top \nabla f(x_k)
    $$

    Тогда условием достаточного убывания (Armijo condition) является:

    $$
    f(x_k - \alpha \nabla f (x_k)) \leq f(x_k) - c_1 \cdot \alpha\nabla f(x_k)^\top \nabla f(x_k), c_1 \in (0, 1)
    $$

    Иллюстрация для понимания:

    ![Иллюстрация условия достаточного убывания](sufficient%20decrease.pdf){width=40%}
    :::
1.  Условия Гольдштейна для неточного линейного поиска.

    :::{.callout-tip appearance="simple"}
    Определим $\phi_1$ и $\phi_2$ следующим образом ($0 < c_1 < c_2 < 1$)
    $$
    \phi_1(\alpha) = f(x_k) - c_1 \alpha \|\nabla f(x_k)\|^2
    $$

    $$
    \phi_2(\alpha) = f(x_k) - c_2 \alpha \|\nabla f(x_k)\|^2
    $$

    Тогда условие Гольдштейна заключается в том, что $\phi_1(\alpha) \leq \phi(\alpha) \leq \phi_2(\alpha)$.

    Иллюстрация для понимания:

    ![Иллюстрация условий Гольдштейна](Goldstein.pdf){width=40%}
    :::
1.  Условие ограничения на кривизну для неточного линейного поиска.

    :::{.callout-tip appearance="simple"}
    $$
    -\nabla f (x_k - \alpha \nabla f(x_k))^\top \nabla f(x_k) \geq c_2 \nabla f(x_k)^\top(- \nabla f(x_k)),
    $$
    где $c_2\in (c_1, 1)$, и $c_1$ взято из условия достаточного убывания.

    Иллюстрация для понимания:

    ![Иллюстрация условия ограничения на кривизну](Curvature.pdf){width=40%}
    :::
1.  Градиент функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    $\nabla f(x)$, вектор частных производных функции $f$.
    :::
1.  Гессиан функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    $$
    f''(x) = \nabla^2 f(x) = \dfrac{\partial^2 f}{\partial x_i \partial x_j} = \begin{pmatrix}
        \frac{\partial^2 f}{\partial x_1 \partial x_1} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
        \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2 \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_n \partial x_n}
    \end{pmatrix}
    $$
    :::
1.  Якобиан функции $f(x): \mathbb{R}^n \to \mathbb{R}^m$.

    :::{.callout-tip appearance="simple"}
    $$
    J_f(x) = \begin{pmatrix}
    \dfrac{\partial f_1}{\partial x_1} & \dots & \dfrac{\partial f_1}{\partial x_n} \\
    \vdots & \ddots & \vdots \\
    \dfrac{\partial f_m}{\partial x_1} & \dots & \dfrac{\partial f_m}{\partial x_n}
    \end{pmatrix}.
    $$
    :::
1.  Формула для аппроксимации Тейлора первого порядка $f^I_{x_0}(x)$ функции $f(x): \mathbb{R}^n \to \mathbb{R}$ в точке $x_0$.

    :::{.callout-tip appearance="simple"}
    Для дифференцируемой f: 
    $$f_{x_0}^I(x) = f(x_0) + \nabla f(x_0)^T (x - x_0)$$
    :::
1.  Формула для аппроксимации Тейлора второго порядка $f^{II}_{x_0}(x)$ функции $f(x): \mathbb{R}^n \to \mathbb{R}$ в точке $x_0$.

    :::{.callout-tip appearance="simple"}
    Для дважды дифференцируемой f: 
    $$
    f_{x_0}^{II}(x) = f(x_0) + \nabla f(x_0)^T (x - x_0) + \frac{1}{2} (x - x_0)^T \nabla^2 f(x_0) (x - x_0)
    $$
    :::
1.  Связь дифференциала функции $df$ и градиента $\nabla f$ для функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    $$
    df(x) = \langle \nabla f(x), dx\rangle
    $$
    :::
1.  Связь второго дифференциала функции $d^2f$ и гессиана $\nabla^2 f$ для функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    $$ d(df) = d^2f(x) = \langle \nabla^2 f(x) dx_1, dx\rangle = \langle H_f(x) dx_1, dx\rangle
    $$
    :::
1.  Формула для приближенного вычисления производной функции $f(x): \mathbb{R}^n \to \mathbb{R}$ по $k$-ой координате с помощью метода конечных разностей.

    :::{.callout-tip appearance="simple"}
    $$
    \dfrac{\partial f}{\partial x_k} (x) \approx \dfrac{f(x+\varepsilon e_k) - f(x)}{\varepsilon}, \quad e_k = (0, \ldots, \underset{{\tiny k}}{1}, \ldots, 0)
    $$

    Время работы: $(d+1)T$, где вызов $f(x)$ занимает $T$, $x \in \mathbb{R}^d$
    :::
<!-- 1.  Пусть $f = f(x_1(t), \ldots, x_n(t))$. Формула для вычисления $\frac{\partial f}{\partial t}$ через $\frac{\partial x_i}{\partial t}$ (Forward chain rule).

    :::{.callout-tip appearance="simple"}
    $$
    \dfrac{\partial f}{\partial t} = \sum_{i = 1}^{n}\dfrac{\partial f}{\partial x_i}\dfrac{\partial x_i}{\partial t}
    $$
    ::: -->
<!-- 1.  Пусть $L$ - функция, возвращающая скаляр, а $v_k$ - функция, возвращающая вектор $x \in \mathbb{R}^t$. Формула для вычисления $\frac{\partial L}{\partial v_k}$ через $\frac{\partial L}{\partial x_i}$ (Backward chain rule).

    :::{.callout-tip appearance="simple"}
    $$
    \dfrac{\partial L}{\partial v_k} = \sum_{i = 1}^{t} \dfrac{\partial L}{\partial x_i} \dfrac{\partial x_i}{\partial v_k}
    $$
    ::: -->
1.  Аффинное множество. Аффинная комбинация. Аффинная оболочка.

    :::{.callout-tip appearance="simple"}
    Множество $A$ называется аффинным если для любых $x_1$, $x_2$ из $A$ прямая, проходящая через $x_1$, $x_2$, тоже лежит в $A$. То есть:
    $$
    \forall \theta \in \R, \forall x_1, x_2 \in A : \theta x_1 + \left(1 - \theta \right)x_2 \in A
    $$
    Пример аффинного множества: $\mathbb{R}^n$

    Пусть $x_1, x_2, \ldots, x_k \in S$. Тогда точка $\theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_k x_k$ называется аффинной комбинацией, если
    $$
    \forall i \in \{1, \ldots, k\} \colon \theta_i \in \R, \quad \sum_{i=1}^k \theta_i = 1
    $$

    Аффинная оболочка -- множество всех возможных аффинных комбинаций элементов множества.
    $$
    \text{aff}(S) = \left\{ \sum_{i=1}^{k}\theta_i x_i \bigg| k > 0, x_i \in S, \theta_i \in \R, \sum_{i=1}^{k} \theta_i = 1 \right\}
    $$

    :::
1.  Выпуклое множество. Выпуклая комбинация. Выпуклая оболочка.

    :::{.callout-tip appearance="simple"}
    Множество $S$ называется выпуклым если для любых $x_1$, $x_2$ из $S$ отрезок между $x_1$, $x_2$ тоже лежит в $S$. То есть:
    $$
    \forall \theta \in [0, 1], \, \forall x_1, x_2 \in S : \; \theta x_1 + \left(1 - \theta \right)x_2 \in S
    $$

    Пусть $x_1, x_2, \ldots, x_k \in S$. Тогда точка $\theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_k x_k$ называется выпуклой комбинацией, если
    $$
    \forall i \in \{1, \ldots, k\} \colon \; \theta_i \geq 0, \quad \sum_{i=1}^k \theta_i = 1
    $$

    Выпуклая оболочка -- множество всех возможных выпуклых комбинаций элементов множества.
    $$
    \text{conv}(S) = \left\{ \sum_{i=1}^{k}\theta_i x_i \, \bigg| \, k > 0, \, x_i \in S, \, \theta_i \geq 0, \, \sum_{i=1}^{k} \theta_i = 1 \right\}
    $$
    :::
1.  Конус. Выпуклый конус. Коническая комбинация. Коническая оболочка.

    :::{.callout-tip appearance="simple"}
    Множество $S$ называется конусом если для любого $x$ из $S$ луч, проходящий из 0 через $x$, тоже лежит в $S$. То есть:
    $$
    \forall \theta \geq 0, \, \forall x \in S : \; \theta x \in S
    $$

    Множество $S$ называется выпуклым конусом если для любых $x_1, x_2 \in S$ их коническая комбинация тоже лежит в $S$. То есть:

    $$
    \forall x_1, x_2 \in S, \, \theta_1, \theta_2 \geq 0 : \theta_1 x_1 + \theta_2 x_2 \in S
    $$

    Пусть $x_1, x_2, \ldots, x_k \in S$. Тогда точка $\theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_k x_k$ называется конической комбинацией, если
    $$
    \forall i \in \{1, \ldots, k\} \colon \; \theta_i \geq 0
    $$

    Коническая оболочка - множество всех возможных конических комбинаций элементов множества.
    $$
    \text{coni}(S) = \left\{ \sum_{i=1}^{k}\theta_i x_i \, \bigg| \, k > 0, \, x_i \in S, \, \theta_i \geq 0 \right\}
    $$

    :::
1.  Сумма Минковского.

    :::{.callout-tip appearance="simple"}
    Сумма Минковского двух множеств $S_1, S_2$ — это множество
    $$
    S_1 + S_2 = \{ s_1 + s_2 \mid s_1 \in S_1,\; s_2 \in S_2 \}.
    $$

    ![](minkowski.pdf)

    :::
1.  Любые 2 операции с множествами, сохраняющие выпуклость.

    :::{.callout-tip appearance="simple"}
    1. Линейная комбинация:
    $$
    S = \left\{s \, | \, s = c_1 x + c_2 y, \; x \in S_x, \; y \in S_y, \; c_1, c_2 \in \R \right\}
    $$

    2. Пересечение любого числа выпуклых множеств

    3. Образ множества в аффинном преобразовании:
    $$
    S \subseteq \mathbb{R}^n \; \text{convex} \rightarrow f(S) = \left\{ f(x) | x \in S \right\} \; \text{convex} \quad \left( f(x) = Ax + b \right)
    $$
    :::
1.  Выпуклая функция.

    :::{.callout-tip appearance="simple"}
    Функция $f(x)$, определённая на выпуклом множестве $S \subseteq \mathbb{R}^n$ называется выпуклой на $S$ если:
    $$
    \forall x_1, x_2 \in S, \quad \forall \lambda \in [0, 1]
    $$
    $$
    f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
    $$

    ![Иллюстрация выпуклой функции](convex_function_ru.pdf)
    
    :::
<!-- 1.  Строго выпуклая функция.

    :::{.callout-tip appearance="simple"}
    Функция $f(x)$, определённая на выпуклом множестве $S \subseteq \mathbb{R}^n$ называется строго выпуклой на $S$ если:
    $$
    \forall x_1, x_2 \in S: x_1 \neq x_2, \quad \forall \lambda \in (0, 1)
    $$
    $$
    f(\lambda x_1 + (1 - \lambda) x_2) < \lambda f(x_1) + (1 - \lambda) f(x_2)
    $$
    ::: -->
1.  Надграфик функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    Для функции, определённой на $S \subseteq \mathbb{R}^n$, множество:
    $$
    \text{epi}\; f = \left\{[x, \mu] \in S \times \mathbb{R} : f(x) \leq \mu \right\}
    $$
    называется надграфиком функции $f(x)$.

    ![Иллюстрация надграфика функции](epigraph.pdf)

    :::
<!-- 1.  Множество подуровней функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    Для функции, определённой на $S \subseteq \mathbb{R}^n$, множество:
    $$
    \mathcal{L}_{\beta} = \left\{x \in S : f(x) \leq \beta \right\}
    $$
    называется множеством подуровней или множеством Лебега функции $f(x)$

    Если функция $f(x)$ выпукла, то её множество подуровней выпукло для любого $\beta \in \mathbb{R}$. 

    **Обратное неверно**. (Например, рассмотрим функцию $f(x) = \sqrt{|x|}$)

    ![Иллюстрация множества подуровней функции](sublevel_set.pdf)

    ::: -->
1.  Дифференциальный критерий выпуклости первого порядка.

    :::{.callout-tip appearance="simple"}
    Дифференцируемая функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ выпукла тогда и только тогда когда $\forall x, y \in S$:
    $$f(y) \geq f(x) + \nabla f(x)^T (y - x)$$
    :::
1.  Дифференциальный критерий выпуклости второго порядка.

    :::{.callout-tip appearance="simple"}
    Дважды дифференцируемая функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ выпукла тогда и только тогда когда для любой внутренней точки x $\forall x \in \textbf{int}(S) \neq \emptyset$:
    $$\nabla^2f(x) \succeq 0$$
    :::
1.  Связь выпуклости функции и её надграфика.

    :::{.callout-tip appearance="simple"}
    Функция выпукла тогда и только тогда, когда её надграфик - выпуклое множество.
    :::
1.  $\mu$-сильно выпуклая функция.

    :::{.callout-tip appearance="simple"}
    Функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ называется сильно выпуклой если $\forall x_1, x_2 \in S$, $ 0 \leq \lambda \leq 1$ и $\mu > 0$:

    $$f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2) - \frac{\mu}{2} \lambda(1 - \lambda) \|x_1 - x_2\|^2$$
    :::
1.  Дифференциальный критерий сильной выпуклости первого порядка.

    :::{.callout-tip appearance="simple"}
    Дифференцируемая функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ является сильно выпуклой тогда и только тогда, когда $\forall x, y  \in S$:

    $$f(y) \geq f(x) + \nabla f(x)^T (y - x) + \frac{\mu}{2} \| y - x\|^2$$
    :::
1.  Дифференциальный критерий сильной выпуклости второго порядка.

    :::{.callout-tip appearance="simple"}
    Дважды дифференцируемая функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ является сильно выпуклой тогда и только тогда, когда существует $\mu > 0$

    $$\nabla^2f(x) \succeq \mu I$$
    :::
1.  Любые 2 операции с функциями, сохраняющие выпуклость.

    :::{.callout-tip appearance="simple"}

    1. Сумма выпуклых функций с не отрицательными коэффициентами является выпуклой функцией.
    1. Композиция выпуклой функции с аффинной выпукла: $g(x) = f(Ax + b)$
    1. Поточечный максимум любого числа выпуклых функций есть выпуклая функция.
    :::
<!-- 1.  Теорема Тейлора.

    :::{.callout-tip appearance="simple"}
    $f: \mathbb{R}^n \to \mathbb{R}$ - непрерывная, дифференцируемая функция и $p \in \mathbb{R}^n$, тогда теорема Тейлора гласит:
    $$f(x + p) = f(x) + \nabla f(x + tp)^T p$$
    Для некоторого $t\in (0, 1)$\\

    Более того, если f - дважды дифференцируема, то:

    $$f(x + p) = f(x) + \nabla f(x)^T p + \frac{1}{2} p^T \nabla^2f(x + tp) p$$
    Для некоторого $t\in (0, 1)$
    ::: -->
1.  Необходимые условия локального экстремума.

    :::{.callout-tip appearance="simple"}
    Если $x^*$ - локальный экстремум и f непрерывно дифференцируема в открытой окрестности $x^*$, то:
    $$ \nabla f(x^*) = 0 $$
    :::
1.  Достаточные условия локального экстремума.

    :::{.callout-tip appearance="simple"}
    Если $\nabla^2 f$ непрерывна в открытой окрестности $x^*$ и 
    $$\nabla f(x^*) = 0 $$
    $$ \nabla^2 f(x^*) \succ 0$$

    То $x^*$ - локальный минимум $f(x)$.\\
    Для локального максимума аналогично, только
    $$0\succ  \nabla^2 f(x^*) $$
    :::
1.  Общая задача математического программирования. Функция Лагранжа.

    :::{.callout-tip appearance="simple"}
    $$
    \left\{
            \begin{aligned}
                & f_0(x) \to \min_{x \in \mathbb{R}^d} \\
                & f_i(x) \leq 0, \quad i = 1, \dots, m, \\
                & h_i(x) = 0, \quad i = 1, \dots, p.
            \end{aligned}
        \right.
    $$
    Функция Лагранжа:
    $$
    L(x, \lambda, \nu) = f_0(x) + \sum_{i = 1}^{m} \lambda_i f_i(x) + \sum_{i = 1}^{p} \nu_i h_i(x),
    $$
    :::
1.  Теорема Каруша - Куна - Таккера в форме необходимых условий решения задачи математического программирования.

    :::{.callout-tip appearance="simple"}
    Пусть $x_*$ - решение задачи с нулевым зазором двойственности 
    $$
    \left\{
            \begin{aligned}
                & f_0(x) \to \min_{x \in \mathbb{R}^d} \\
                & f_i(x) \leq 0, \quad i = 1, \dots, m, \\
                & h_i(x) = 0, \quad i = 1, \dots, p.
            \end{aligned}
        \right.
    $$
    Функция Лагранжа:
    $$
    L(x, \lambda, \nu) = f_0(x) + \sum_{i = 1}^{m} \lambda_i f_i(x) + \sum_{i = 1}^{p} \nu_i h_i(x),
    $$
    Тогда найдутся такие векторы~$\lambda^*$ и~$\nu^*$, что выполнены условия
    $$
    \left\{
            \begin{aligned}
                & \nabla f_0(x_*)
                    +
                    \sum_{i = 1}^{m} \lambda_i^* \nabla f_i(x_*)
                    +
                    \sum_{i = 1}^{p} \nu_i^* \nabla h_i(x_*) = 0 \\
                & f_i(x_*) \leq 0, \quad i = 1, \dots m \\
                & h_i(x_*) = 0, \quad i = 1, \dots p \\
                & \lambda_i^* \geq 0, \quad i = 1, \dots m \\
                & \lambda_i^* f_i(x_*) = 0, \quad i = 1, \dots m\\
                %& (?) \, \forall y \in C(x^{*}) : \nabla^2_{xx}L(x^{*}, \lambda^{*} )y\rangle > 0
            \end{aligned}
        \right.
    $$
    :::
1.  Условие Слейтера.

    :::{.callout-tip appearance="simple"}
    1. Если задача выпуклая (т.е., говоря о задаче минимизации, оптимизируемая функция $f_0$ и ограничения вида неравенство $f_i$ -- выпуклые, ограничения вида равенства $h_i$ -- аффинные)
    1. И существует точка $x$ такая, что $h(x) = 0$ и $f_i(x) < 0$ (ограничения вида равенства активные, а ограничения вида неравенства выполняются строго)

    То тогда задача имеет нулевой зазор двойственности и условия ККТ становятся необходимыми и достаточными.
    :::
1.  Задача выпуклого программирования.

    :::{.callout-tip appearance="simple"}
    Задача выпуклого программирования — это задача оптимизации, в которой целевая функция является выпуклой функцией и область допустимых решений выпукла. В форме ниже функции $f_0, \ldots, f_m$ - выпуклые, а функции $h_i$ - аффинные.
    $$
    \left\{
            \begin{aligned}
                & f_0(x) \to \min_{x \in \mathbb{R}^d} \\
                & f_i(x) \leq 0, \quad i = 1, \dots, m, \\
                & h_i(x) = 0, \quad i = 1, \dots, p.
            \end{aligned}
        \right.
    $$

    :::
1.  Задача линейного программирования. Задача линейного программирования в стандартной форме.

    :::{.callout-tip appearance="simple"}
    Все задачи с линейным функционалом и линейными ограничениями считаются задачами линейного программирования. Стандартная форма:
    $$\min_{x \in \mathbb{R}^n} c^Tx$$
    $$s.t. Ax=b$$
    $$x_i \geq 0, i=1, \dots , n$$
    :::
1.  Идея симплекс метода.

    :::{.callout-tip appearance="simple"}
    Симплекс метод решает следующую задачу:
    $$\min_{x \in \mathbb{R}^n } c^\top x$$
    $$s.t. Ax \leq b$$

    Шаги выполнения симплекс метода:

    1. Поиск начальной базисной допустимой точки
    1. Проверка оптимальности. Если решение оптимально, то алгоритм завершается, иначе переходим к следующему шагу
    1. Замена базиса
    1. Повторяем предыдущие два шага до достижения оптимального решения или установления, что задача не имеет допустимого решения
    :::
1.  Сходимость симплекс метода.

    :::{.callout-tip appearance="simple"}
    В худшем случае симплекс метод сходится экспоненциально от размерности задачи, но на практике в среднем алгоритм работает сильно лучше. Задача, на которой симплекс метод работает экспоненциальное время, называется примером Klee Minty.
    :::
<!-- 1. Формулировка задачи о максимальном потоке в графе. -->
<!-- 1. Формулировка задачи о минимальном разрезе в графе. -->
<!-- 1. Теорема о максимальном потоке и минимальном разрезе. -->
<!-- 1.  Показать, что направление антиградиента - направление наискорейшего локального убывания функции.

    :::{.callout-tip appearance="simple"}
    Пусть $f$ дифференцируема, зададим искомое направление локального убывания - $h$ - $\| h \| = 1$.
    Тогда её апроксимация: $f(x + \alpha h) = f(x) + \alpha \langle \nabla f(x), h \rangle + o(\alpha)$
    $$
    f(x + \alpha h) < f(x) \Rightarrow \alpha  \langle \nabla f(x), h \rangle + o(\alpha) < 0.
    $$
    При $\alpha \rightarrow +0$ получаем: $\alpha \langle \nabla f(x), h \rangle \leqslant 0$
    $$
    \| \langle \nabla f(x), h \rangle \| \leqslant \| \nabla f(x) \| \| h \| \leqslant \| \nabla f(x) \|
    $$
    $$
    \langle \nabla f(x), h \rangle \geqslant -\| \nabla f(x) \| \Rightarrow h = \frac{-\nabla f(x)}{\| \nabla f(x) \|}, \text{ ч.т.д.}
    $$
    ::: -->
1.  Метод градиентного спуска.

    :::{.callout-tip appearance="simple"}
    Решаем задачу минимизации 
    $$
    f(x) \to \min_{x \in \mathbb{R}^d}
    $$
    Если $f$ дифференцируема, то тогда для решения этой задачи можно использовать метод градиентного спуска:
    $$
    x_{k + 1} = x_k - \alpha \nabla f(x_k)
    $$
    :::

1.  Наискорейший спуск.

    :::{.callout-tip appearance="simple"}
    Решаем задачу минимизации 
    $$
    f(x) \to \min_{x \in \mathbb{R}^d}
    $$
    Если $f$ дифференцируема, то тогда для решения этой задачи можно использовать метод наискорейшего спуска:
    $$
    x_{k + 1} = x_k - \alpha_k \nabla f(x_k)
    $$
    $$
    \alpha_k = \arg\min\limits_{\alpha \in \mathbb{R}^+} f(x_k - \alpha \nabla f(x_k)),
    $$
    т.е. выбираем наилучший шаг спуска на каждой итерации метода.
    :::
1.  Как направлены две соседние итерации метода наискорейшего спуска по отношению друг к другу?

    :::{.callout-tip appearance="simple"}
    Они ортогональны друг другу.

    ![Несколько итераций наискорейшего спуска](Steepest_Descent.pdf)
    :::
1.  Липшицева парабола для гладкой функции.

    :::{.callout-tip appearance="simple"}
    Если $f: \mathbb{R}^n \rightarrow \mathbb{R}$ - непрерывно дифференцируема и градиент Липшицев с константой $L$, то $\forall x, y \in \mathbb{R}^n$:
    $$
    | f(y) - f(x) - \langle \nabla f(x), y - x \rangle | \leqslant \frac{L}{2}\| y - x \|^2
    $$
    Если зафиксируем $x_0 \in \mathbb{R}^n$, то:
    $$
    \varphi_1(x) = f(x_0) + \langle \nabla f(x_0), x - x_0 \rangle - \frac{L}{2}\| x - x_0 \|^2
    $$
    $$
    \varphi_2(x) = f(x_0) + \langle \nabla f(x_0), x - x_0 \rangle + \frac{L}{2}\| x - x_0 \|^2
    $$
    Это две параболы, и для них верно, что $\varphi_1(x) \leqslant f(x) \leqslant \varphi_2(x)$ $\forall x$

    ![Иллюстрация Липшицевых парабол, между которыми зажата гладкая функция. Чаще нас интересует мажорирующая из них.](lipschitz_parabola.pdf)
    :::
1.  Размер шага наискорейшего спуска для квадратичной функции.

    :::{.callout-tip appearance="simple"}
    Решаем задачу минимизации методом наискорейшего спуска
    $$
    f(x) = \frac{1}{2}x^TAx - b^Tx + c \to \min_{x \in \mathbb{R}^d}
    $$
    $$
    \nabla f = \frac{1}{2}(A + A^T)x - b
    $$

    Из условия $\nabla f(x_{k + 1})^T \nabla f(x_k) = 0$ получаем:
    $$
    \alpha_k = \frac{2 \nabla f(x_k)^T \nabla f(x_k)}{\nabla f(x_k)^T (A + A^T) \nabla f(x_k)} = \frac{\nabla f(x_k)^T \nabla f(x_k)}{\nabla f(x_k)^T \nabla^2 f(x_k) \nabla f(x_k)}. 
    $$
    :::
1.  Характер сходимости градиентного спуска для гладких выпуклых функций в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}
    $f(x_k) - f^* \sim  \mathcal{O} \left( \frac{1}{k} \right).$
    :::
1.  Характер сходимости градиентного спуска для гладких и сильно выпуклых функций в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}
    $\|x_k - x^*\|^2 \sim \mathcal{O} \left( \left(1 - \frac{\mu}{L}\right)^k \right).$
    :::
1.  Связь спектра гессиана с константами сильной выпуклости и гладкости функции.

    :::{.callout-tip appearance="simple"}
    $\quad \mu = \min\limits_{x \in \text{dom} f}\lambda_{\min}(\nabla^2 f(x)), \quad L = \max\limits_{x \in \text{dom} f}\lambda_{\max}(\nabla^2 f(x)).$ 
    :::
1.  Условие Поляка-Лоясиевича (градиентного доминирования) для функций.

    :::{.callout-tip appearance="simple"}
    $\exists \mu > 0: \quad \| \nabla f(x) \|^2 \geqslant 2\mu(f(x) - f^*) \quad \forall x$,  где $f^*$ - минимум функции $f(x)$.
    :::
1.  Сходимость градиентного спуска для сильно выпуклых квадратичных функций. Оптимальные гиперпараметры.

    :::{.callout-tip appearance="simple"}
    Решаем задачу минимизации методом градиентного спуска. Пусть $A \in \mathbb{S}_{++}^n \Rightarrow \nabla f = Ax - b$.
    $$
    \begin{aligned}
    f(x) &= \frac{1}{2}x^TAx - b^Tx + c \to \min_{x \in \mathbb{R}^d} \\
    x_{k + 1} &= x_k - \alpha (Ax_k - b) \\
    \alpha_{opt} &= \frac{2}{\mu + L}, \text{ где } \mu = \lambda_{\min}(A), L = \lambda_{\max}(A) \\
    \kappa &= \frac{L}{\mu} \geqslant 1 \\
    \rho &= \frac{\kappa - 1}{\kappa + 1} \\
    \| x_k - x^* \| &\leqslant \rho^k \| x_0 - x^* \| \\
    \end{aligned}
    $$
    :::
1.  Связь PL-функций и сильно выпуклых функций. 

    :::{.callout-tip appearance="simple"}
    Пусть $f$ $\mu$-сильно выпуклая и дифференцируемая $\Rightarrow f \in$ PL.
    
    Обратное неверно - $f(x) = x^2 + 3\sin^2x \in$ PL, но не сильно выпуклая (она вообще не выпуклая).

    ![Пример невыпуклой PL функции](pl_2d.pdf)
    :::
1.  Привести пример выпуклой, но не сильно выпуклой задачи линейных наименьших квадратов (возможно, с регуляризацией).

    :::{.callout-tip appearance="simple"}
    Рассмотрим задачу минимизации функции:
    $$
    \| A x - b \|^2 \to \min_{x \in \mathbb{R}^d},
    $$
    где матрица $A \in \mathbb{R}^{m\times n}, \ b\in \mathbb{R}^{m}$, $m < n$ (лежачая).
    :::
1.  Привести пример сильно выпуклой задачи линейных наименьших квадратов (возможно, с регуляризацией).

    :::{.callout-tip appearance="simple"}
    Рассмотрим задачу минимизации функции:
    $$
    f(x) = \|Ax - b\|_2^2,
    $$
    где $A \in \mathbb{R}^{n \times n}$ (ранг $A = n$). Эта функция сильно выпукла, так как гессиан положительно определен.
    :::
1.  Нижние оценки для гладкой выпуклой оптимизации с помощью методов первого порядка в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}

    $f(x_k) - f^* \sim  \mathcal{\Omega}\left(\frac{1}{k^2}\right)$  
    :::
1.  Нижние оценки для гладкой сильно выпуклой оптимизации с помощью методов первого порядка в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}

    $\|x^k - x^*\|^2 \sim \mathcal{\Omega} \left( \left(1 - \sqrt{\dfrac{\mu}{L}}\right)^k \right)$  
    :::
1.  Отличие ускоренной и неускоренной линейной сходимости для методов первого порядка.

    :::{.callout-tip appearance="simple"}
    | Функция | Неускоренная | Ускоренная |
    |:--------:|:-------:|:-------:|
    Гладкая и сильно-выпуклая (или PL) |$\mathcal{O}\left(\varkappa \log\frac{1}{\varepsilon}\right)$| $\mathcal{O}\left(\sqrt{\varkappa} \log\frac{1}{\varepsilon}\right)$  |
    Гладкая и выпуклая (в вопросе не требуется) |$\mathcal{O}\left(\frac{1}{\varepsilon}\right)$| $\mathcal{O}\left(\frac{1}{\sqrt{\varepsilon}}\right)$  |
    :::
1.  Метод тяжелого шарика (Поляка).

    :::{.callout-tip appearance="simple"}
    Задача: $f(x) \rightarrow \min\limits_{x \in \mathbb{R}^d}, \ f(x)$ - непрерывно дифференцируемая функция
    $$
    x_{k+1} = x_{k} −\alpha\nabla f(x_{k}) + \beta(x_k−x_{k−1}), \qquad 0 < \beta < 1.
    $$
    :::
1.  Понятие локальной и глобальной сходимости численного метода оптимизации.

    :::{.callout-tip appearance="simple"}
    Локальная сходимость означает, что последовательность итераций сходится к решению $x^*$ внутри некоторого множества. За пределами этого множества сходимости нет.
    Глобальная сходимость означает, что последовательность итераций сходится к решению $x^*$ вне зависимости от начального приближения.
    :::
1.  Ускоренный градиентный метод Нестерова для выпуклых гладких функций.

    :::{.callout-tip appearance="simple"}
    Предположим, что $f : \mathbb{R}^n \rightarrow \mathbb{R}$ является выпуклой и $L$-гладкой. Ускоренный градиентный метод Нестерова (NAG) предназначен для решения задачи минимизации, начиная с начальной точки $x_0 = y_0 \in \mathbb{R}^n$ и $\lambda_0 = 0$. Алгоритм выполняет следующие шаги:
    $$
    \begin{aligned}
    &\textbf{Обновление градиента: } &x_{k+1} &= y_k - \frac{1}{L} \nabla f(y_k) \\
    & \textbf{Вес экстраполяции: } &\lambda_{k+1} &= \frac{1 + \sqrt{1 + 4\lambda_k^2}}{2} \\
    & \quad &\gamma_k &= \frac{\lambda_k - 1}{\lambda_{k+1}} \\
    &\textbf{Экстраполяция: } &y_{k+1} &= x_{k+1} + \gamma_k\left(x_{k+1} - x_k\right)
    \end{aligned}
    $$
    Последовательность $\{f(x_k)\}_{k\in\mathbb{N}}$, генерируемая алгоритмом, сходится к оптимальному значению $f^*$ со скоростью $\mathcal{O}\left(\frac{1}{k^2}\right)$, в частности:
    $$
    f(x_k) - f^* \leq \frac{2L \|x_0 - x^*\|^2}{k^2}
    $$

    ![](AGD_horizontal.pdf)

    :::
1.  Ускоренный градиентный метод Нестерова для сильно выпуклых гладких функций.

    :::{.callout-tip appearance="simple"}
    Предположим, что $f : \mathbb{R}^n \rightarrow \mathbb{R}$ является $\mu$-сильно выпуклой и $L$-гладкой. Ускоренный градиентный метод Нестерова (NAG) предназначен для решения задачи минимизации, начиная с начальной точки $x_0 = y_0 \in \mathbb{R}^n$ и $\lambda_0 = 0$. Алгоритм выполняет следующие шаги:
    $$
    \begin{aligned}
    &\textbf{Обновление градиента: } &x_{k+1} &= y_k - \frac{1}{L} \nabla f(y_k) \\
    &\textbf{Экстраполяция: } &y_{k+1} &= x_{k+1} -  \gamma \left(x_{k+1} - x_k\right) \\
    &\textbf{Вес экстраполяции: } &\gamma &= \frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}
    \end{aligned}
    $$
    Последовательность $\{f(x_k)\}_{k\in\mathbb{N}}$, генерируемая алгоритмом, сходится к оптимальному значению $f^*$ линейно:
    $$
    f(x_k) - f^* \leq \frac{\mu + L}{2}\|x_0 - x^*\|^2_2 \exp \left(-\frac{k}{\sqrt{\varkappa}}\right)
    $$
    :::
1.  $A$-сопряженность двух векторов. $A$-ортогональность. Скалярное произведение $\langle \cdot, \cdot \rangle_A$.

    :::{.callout-tip appearance="simple"}
    $A$-ортогональность (сопряженность):
    $$
    x \perp_A y \iff x^T A y = 0.
    $$
    :::
1.  Зависимость сходимости метода сопряженных градиентов от спектра матрицы.

    :::{.callout-tip appearance="simple"}
    Если матрица $A$ имеет только $r$ различных собственных чисел, тогда метод сопряжённых градиентов сходится за $r$ итераций.

    ![](cg_random_10_100_60.pdf)
    ![](cg_clustered_10_1000_60.pdf)

    :::
1.  Метод Ньютона.

    :::{.callout-tip appearance="simple"}
    Рассматривается задача минимизации функции с невырожденным гессианом.
    $$
    x_{k+1} = x_k - [\nabla^2f(x_k)]^{-1} \nabla f(x_k)
    $$
    :::
1.  Сходимость метода Ньютона для квадратичной функции.

    :::{.callout-tip appearance="simple"}
    Метод Ньютона сходится для квадратичной функции за одну итерацию, при условии, что гессиан невырожден. Следует из того, что метода Ньютона минимизирует квадратичную тейлоровскую аппроксимацию функции, которая в случае квадратичной функции совпадает с самой функцией:
    $$
    f(x) = f(x_k) + \nabla f(x_k)^T(x - x_k) + \frac{1}{2} (x-x_k)^T \nabla^2 f(x_k) (x-x_k), \quad \nabla f(x_{k+1}) = 0
    $$
    :::
1.  Характер сходимости метода Ньютона для сильно выпуклых гладких функций - куда и как сходится.

    :::{.callout-tip appearance="simple"}
    Пусть $f(x)$ сильно выпукла дважды непрерывно дифференцируемая на $\mathbb{R}^n$ и выполняются неравенства: $\mu I_n\preceq \nabla ^2f(x) \preceq L I_n$. Тогда метод Ньютона с постоянным шагом локально сходится к решению со сверхлинейной скоростью. Если вдобавок, Гессиан $M$-Липшицев, тогда метод сходится  локально к  $x^*$ с квадратичной скоростью.
    :::
1.  Демпфированный метод Ньютона.

    :::{.callout-tip appearance="simple"}
    $$
    x_{k+1} = x_k - \alpha_k \left[\nabla^2f(x_k)\right]^{-1} \nabla f(x_k),\;\;\; \alpha_k \in [0,1]
    $$
    где $\alpha_k$ находят с помощью линейного поиска. Сходимость глобальная.
    :::
1.  Идея квазиньютоновских методов. Метод SR-1.

    :::{.callout-tip appearance="simple"}
    $$
    \min_{x\in \mathbb{R}^{d}} f(x)
    $$
    Пусть $x_{0} \in \mathbb{R}^n$, $B_{0} \succ 0$. Для $k = 1, 2, 3, \dots$, повторим: 

      1.  Решить $B_{k} d_{k} = -\nabla f(x_{k})$ относительно $d_k$.
      2.  Обновить $x_{k+1} = x_{k} + \alpha_k d_{k}$.
      3.  Вычислить $B_{k+1}$ из $B_{k}$.
          $$
          B_{k+1} = B_k + \frac{(\Delta y_k - B_k d_k)(\Delta y_k - B_k d_k)^T}{(\Delta y_k - B_k d_k)^T d_k}, \quad \Delta y_k = \nabla f(x_{k+1}) - \nabla f(x_{k}).
          $$
    :::
1.  Проекция.

    :::{.callout-tip appearance="simple"}
    Проекция точки $y \in \mathbb{R}^n \text{ на множество } S \subseteq \mathbb{R}^n \text{ это точка }\text{proj}_S(y) \in S$:
    $$
    \text{proj}_S(y) = \arg \min\limits_{x \in S}\frac{1}{2}||x - y||^2_2
    $$
    :::
1.  Достаточное условие существования проекции точки на множество.

    :::{.callout-tip appearance="simple"}
    Если $S \subseteq \mathbb{R}^n -$ замкнутое множество, тогда проекция на множество $S$ существует для любой точки.
    :::
1.  Достаточное условие единственности проекции точки на множество.

    :::{.callout-tip appearance="simple"}
    Если $S \subseteq \mathbb{R}^n -$ замкнутое выпуклое множество, тогда проекция на множество $S$ единственна для каждой точки.
    :::
1.  Метод проекции градиента.

    :::{.callout-tip appearance="simple"}
    Рассматривается задача $f(x) \rightarrow \min\limits_{x \in S}$, где $S \subseteq \mathbb{R}^n$. Метод проекции градиента — это метод оптимизации с проекцией на бюджетное множество $S$:
    $$
    x_{k+1} = \text{proj}_S(x_{k} - \alpha_k \nabla f(x_{k})),
    $$
    где $\alpha_k$ — learning rate.
    :::
<!-- 1.  Критерий проекции точки на выпуклое множество (Неравенство Бурбаки-Чейни-Гольдштейна).

    :::{.callout-tip appearance="simple"}
    Проекция $\text{proj}_S(x)$ точки $x$ на выпуклое множество $S$ удовлетворяет:
    $$
    \langle x - \text{proj}_S(x), y - \text{proj}_S(x) \rangle \leqslant0 \quad \forall y \in S.
    $$
    ::: -->
1.  Проекция как нерастягивающий оператор.

    :::{.callout-tip appearance="simple"}
    Проекция на выпуклое множество $S$ является нерастягивающим оператором:
    $$
    \|\text{proj}_S(x) - \text{proj}_S(y)\| \leqslant \|x - y\| \quad \forall x, y.
    $$
    :::
1.  Метод Франк-Вульфа.

    :::{.callout-tip appearance="simple"}
    Рассматриваем задачу $f(x) \rightarrow \min\limits_{x \in S}$. Метод Франк-Вульфа имеет вид:
    $$
    \begin{aligned}
    y_k &= \text{arg}\min_{x \in S} f^I_{x_k}(x) = \text{arg}\min_{x \in S} \langle\nabla f(x_k), x \rangle \\
    x_{k+1} &= \gamma_k x_k + (1-\gamma_k)y_k
    \end{aligned}
    $$
    где $\gamma_k$ - гиперпараметр.
    ![Иллюстрация метода Франк-Вульфа](FW.pdf)
    :::
1.  Характер сходимости метода проекции градиента для гладких выпуклых функций в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}
    Для гладких выпуклых функций метод проекции градиента имеет сходимость порядка $\mathcal{O}\left(\frac{1}{k}\right)$, где $k$ — число итераций. То есть сходимость такая же, как и для безусловной задачи, но стоимость итерации может быть выше из-за проекции.
    :::
1.  Характер сходимости метода проекции градиента для гладких сильно выпуклых функций в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}
    Для гладких сильно выпуклых функций метод проекции градиента имеет линейную сходимость порядка $\mathcal{O}\left(\left(1 - \frac{\mu}{L}\right)^k\right)$, где $k$ — число итераций. То есть сходимость такая же, как и для безусловной задачи, но стоимость итерации может быть выше из-за проекции.
    :::
1.  Характер сходимости метода Франк-Вульфа для гладких выпуклых функций в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}
    Метод Франк-Вульфа для гладких выпуклых функций имеет сходимость порядка $\mathcal{O}\left(\frac{1}{k}\right)$, где $k$ — число итераций.
    :::
1.  Характер сходимости метода Франк-Вульфа для гладких сильно выпуклых функций в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}
    Для гладких сильно выпуклых функций метод Франк-Вульфа имеет сходимость порядка $\mathcal{O}\left(\frac{1}{k}\right)$, где $k$ — число итераций.
    :::
1.  Привести пример выпуклой негладкой задачи линейных наименьших квадратов (возможно, с регуляризацией).

    :::{.callout-tip appearance="simple"}
    Рассмотрим задачу минимизации функции:
    $$
    f(x) = \|Ax - b\|_2^2 + \lambda \|x\|_1,
    $$
    где $A \in \mathbb{R}^{n \times n}$, $\lambda > 0$. Эта функция выпукла, но негладка из-за наличия $\ell_1$-регуляризации.
    :::
1.  Субградиент. Субдифференциал.

    :::{.callout-tip appearance="simple"}
    Субградиент функции $f$ в точке $x$ — это вектор $g$, удовлетворяющий условию:
    $$
    f(y) \geqslant f(x) + g^T (y - x), \quad \forall y.
    $$
    Множество всех субградиентов в точке $x$ называется субдифференциалом и обозначается как $\partial f(x)$.

    ![Субдифференциал функции ReLU.](subgrad.pdf)
    :::
1.  Как считать субдифференциал поточечного максимума выпуклых функций.

    :::{.callout-tip appearance="simple"}
    Пусть $f_i(x)$ - выпуклые функции на открытом выпуклом множестве $S \subseteq \mathbb{R}^n, \; x_0 \in S$, и поточечный максимум определяется как $f(x) = \underset{i}{\operatorname{max}} f_i(x)$. Тогда:
    $$
    \partial_S f(x_0) = \mathbf{conv}\left\{  \bigcup\limits_{i \in I(x_0)} \partial_S f_i(x_0) \right\}, \quad I(x) = \{ i \in [1:m]: f_i(x) = f(x)\}
    $$
    :::
1.  Субградиентный метод.

    :::{.callout-tip appearance="simple"}
    Субградиентный метод используется для минимизации выпуклых функций, которые могут быть негладкими. Итерационная формула метода:
    $$
    x_{k+1} = x_k - \alpha_k g_k,
    $$
    где $g_k \in \partial f(x_k)$ — субградиент функции $f$ в точке $x_k$, $\alpha_k$ — шаг метода на $k$-й итерации.
    :::
1.  Характер сходимости субградиентного метода для негладких выпуклых Липшицевых функций в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}
    $f^{\text{best}}_k - f^* \sim \mathcal{O} \left( \frac{1}{\sqrt{k}} \right)$
    :::
1.  Какому условию должна удовлетворять стратегия выбора шага, чтобы субградиентный метод сходился для выпуклых Липшицевых функций?

    :::{.callout-tip appearance="simple"}
    $\sum_{i=1}^k \alpha_i = \infty, \quad \sum_{i=1}^k \alpha_i^2 < \infty$
    :::
1.  Характер сходимости субградиентного метода для негладких сильно выпуклых Липшицевых функций в терминах $\mathcal{O}$ от числа итераций метода. Стратегия выбора шага.

    :::{.callout-tip appearance="simple"}
    $f^{\text{best}}_k - f^* \sim \mathcal{O} \left( \frac{1}{k} \right), \alpha_k = \frac{2}{\mu (k+1)}$
    :::
1.  Метод стохастического градиентного спуска.

    :::{.callout-tip appearance="simple"}
    Решаемая задача: $f(x) \rightarrow \min\limits_{x \in \mathbb{R}^p}$, где $f(x) =\frac{1}{n} \sum\limits_{i=1}^n f_i(x)$
    $$
    \text{SGD:}\quad x_{k+1} = x_k - \alpha_k \nabla f_{i_k}(x_k),
    $$
    где $i_k$ - случайно выбранный индекс. Если $\mathbb{P}(i_k = i) = \frac{1}{n}$, то $\mathbb{E}[\nabla f_{i_k}(x)] = \nabla f(x)$

    :::
1.  Идея мини-батча для метода стохастического градиентного спуска. Эпоха.

    :::{.callout-tip appearance="simple"}
    Разделим данные размера $N$ на $k$ мини-батчей (выборок) размера $B_k$, на каждой итерации посчитаем градиент мини-батча с использованием параллелизма. За $k$ итераций пройдёмся по всей выборке. **Эпоха** - набор $k$ итераций с батчем размера $B_k = \frac{N}{k}$.
    $$
    x_{k + 1} = x_k -  \frac{1}{|B_k|} \sum_{i \in B_k} \nabla f_i(x_k).
    $$
    C увеличением размера мини-батча время на эпоху уменьшается до тех пор, пока нам хватает памяти (в случае наличия параллелизма).

    ![](batches_5.pdf)
    :::
1.  Характер сходимости стохастического градиентного спуска для гладких выпуклых функций в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}
    $f$ - гладкая и выпуклая $\Rightarrow \mathcal{O}\left(\frac{1}{\varepsilon^2}\right), \mathcal{O}\left(\frac{1}{\sqrt{k}}\right)$
    :::
1.  Характер сходимости стохастического градиентного спуска для гладких PL-функций в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}
    $f \in$ PL $\Rightarrow \mathcal{O}\left(\frac{1}{k}\right), \mathcal{O}\left(\frac{1}{\varepsilon}\right)$
    :::
1.  Характер работы стохастического градиентного спуска с постоянным шагом для гладких PL-функций.

    :::{.callout-tip appearance="simple"}
    Пусть $\min_{x \in \mathbb{R}^p} f(x) = \min_{x \in \mathbb{R}^p}\frac{1}{n} \sum_{i=1}^n f_i(x)$ при использовании стохастического градиентного спуска с постоянным шагом $\alpha$
    $$
    x_{k+1} = x_k - \alpha  \nabla f_{i_k}(x_k)
    $$
    имеем следующую оценку $\mathbb{E}[f(x_{k+1}) - f^*] \leq (1 - 2\alpha \mu)^k[f(x_{0}) - f^*]  + \frac{L \sigma^2 \alpha }{ 4 \mu}$. Характер сходимости - линейный до некоторого шара несходимости, в котором будут происходить осцилляции и сходимости не будет.
    :::
1.  Grokking.

    :::{.callout-tip appearance="simple"}
    Grokking при обучении нейронных сетей — это явление, когда модель после продолжительного обучения сначала демонстрирует плохую обобщающую способность на новых данных, несмотря на хорошее качество на обучающем наборе. Затем, после дальнейшего обучения, модель неожиданно начинает показывать значительно лучшую производительность и на тестовых данных. Это подразумевает, что модель в конечном итоге находит более глубокие и универсальные закономерности, которые позволяют ей лучше обобщать на неизвестные данные.

    ![](grokking.png){fig-align="center" width=60%}

    :::
1.  Double Descent.

    :::{.callout-tip appearance="simple"}
    Double descent — это явление, наблюдаемое при обучении нейронных сетей, когда увеличение количества параметров модели сначала приводит к снижению ошибки на обучающем и тестовом наборах (классическое поведение bias-variance tradeoff), затем происходит резкое увеличение ошибки (первая точка перегиба, связанная с переобучением), после чего, с дальнейшим увеличением количества параметров, ошибка снова начинает уменьшаться, формируя вторую "волну" улучшения. Это поведение отличается от традиционной U-образной кривой, и его понимание важно для эффективной настройки гиперпараметров и выбора архитектуры модели.

    ![Иллюстрация зависимости обобщающей способности модели от размера.](doubledescent.pdf)
    :::
1.  Взрыв/Затухание градиентов при обучении глубоких нейронных сетей.

    :::{.callout-tip appearance="simple"}
    При обучении глубоких нейронных сетей часто возникают проблемы взрыва и затухания градиентов, что приводит к медленной или нестабильной сходимости модели. Эти явления можно описать с помощью производной функции ошибки $L$ по весам сети $W$. Пусть $L$ - функция потерь, а $\frac{\partial L}{\partial W}$ - градиенты, используемые для обновления весов. Когда сеть имеет много слоев, градиенты вычисляются как произведение матриц Якоби каждого слоя: $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial z^{(n)}} \cdot \frac{\partial z^{(n)}}{\partial z^{(n-1)}} \cdots \frac{\partial z^{(2)}}{\partial z^{(1)}} \cdot \frac{\partial z^{(1)}}{\partial W}$, где $z^{(i)}$ - активации i-го слоя. Если значения производных $\frac{\partial z^{(i+1)}}{\partial z^{(i)}}$ в среднем больше единицы, градиенты начинают экспоненциально увеличиваться при обратном распространении, вызывая взрыв градиентов. Напротив, если значения производных меньше единицы, градиенты экспоненциально уменьшаются, что приводит к их затуханию.
    :::
1.  Идея gradient checkpointing.

    :::{.callout-tip appearance="simple"}
    Gradient checkpointing — это техника, которая позволяет значительно снизить потребление памяти при обучении глубоких нейронных сетей за счет стратегического пересчета промежуточных активаций во время обратного распространения ошибки. В стандартном процессе обучения с использованием обратного распространения ошибка вычисляется для каждого слоя и промежуточные активации сохраняются в памяти, что требует $O(N)$ памяти, где $N$ — количество слоев в сети.

    При gradient checkpointing вместо сохранения активаций для всех слоев, мы сохраняем их только для некоторых стратегически выбранных слоев, называемых чекпоинтами. Активации для остальных слоев пересчитываются на этапе обратного распространения, что снижает общее потребление памяти. Если мы сохраняем активации через каждые $k$ слоев, то потребление памяти уменьшается до $O(\frac{N}{k})$. Однако, это приводит к дополнительным вычислительным затратам, так как активации некоторых слоев пересчитываются несколько раз.

    ![](checkpoint_backprop.pdf)
    :::

\pagebreak

# Теоремы с доказательствами

1.  Тест корней для определения скорости сходимости последовательности.

    :::{.callout-note appearance="simple"}
    Пусть $(r_k)_{k=m}^\infty$ — последовательность неотрицательных чисел, сходящаяся к нулю, и пусть $\alpha := \limsup_{k \to \infty} r_k^{1/k}$. (Заметим, что $\alpha \geq 0$.)

    (a) Если $0 \leq \alpha < 1$, то $(r_k)_{k=m}^\infty$ сходится линейно с константой $\alpha$.

    (b) В частности, если $\alpha = 0$, то $(r_k)_{k=m}^\infty$ сходится сверхлинейно.

    (c) Если $\alpha = 1$, то $(r_k)_{k=m}^\infty$ сходится сублинейно.

    (d) Случай $\alpha > 1$ невозможен.
    :::

    **Доказательство**. 

    1. Покажем, что если $(r_k)_{k=m}^\infty$ сходится линейно с константой $0 \leq \beta < 1$, то $\alpha \leq \beta$. Действительно, по определению константы линейной сходимости, для любого $\varepsilon > 0$ такого, что $\beta + \varepsilon < 1$, существует $C > 0$ такое, что $r_k \leq C(\beta + \varepsilon)^k$ для всех $k \geq m$. Из этого следует, что $r_k^{1/k} \leq C^{1/k}(\beta + \varepsilon)$ для всех $k \geq m$. Переходя к пределу при $k \to \infty$ и используя $C^{1/k} \to 1$, получаем $\alpha \leq \beta + \varepsilon$. Учитывая произвольность $\varepsilon$, получаем $\alpha \leq \beta$.

    1. Таким образом, в случае $\alpha = 1$ последовательность $(r_k)_{k=m}^\infty$ не может иметь линейной сходимости в соответствии с приведенным выше результатом (доказано от противного). Тем не менее, $(r_k)_{k=m}^\infty$ сходится к нулю, поэтому она должна сходиться сублинейно.
    
    1. Теперь рассмотрим случай $0 \leq \alpha < 1$. Пусть $\varepsilon > 0$ — произвольное число такое, что $\alpha + \varepsilon < 1$. Согласно свойствам limsup, существует $N \geq m$ такое, что $r_k^{1/k} \leq \alpha + \varepsilon$ для всех $k \geq N$. Следовательно, $r_k \leq (\alpha + \varepsilon)^k$ для всех $k \geq N$. Таким образом, $(r_k)_{k=m}^\infty$ сходится линейно с параметром $\alpha + \varepsilon$ (не важно, что неравенство выполняется только начиная с числа $N$). Учитывая произвольность $\varepsilon$, это означает, что константа линейной сходимости $(r_k)_{k=m}^\infty$ не превышает $\alpha$. Поскольку, как показано выше, константа линейной сходимости не может быть меньше $\alpha$, это означает, что константа линейной сходимости $(r_k)_{k=m}^\infty$ точно равна $\alpha$.
    
    1. Наконец, покажем, что случай $\alpha > 1$ невозможен. Действительно, предположим, что $\alpha > 1$. Тогда из определения limsup следует, что для любого $N \geq m$ существует $k \geq N$ такое, что $r_k^{1/k} \geq 1$, и, в частности, $r_k \geq 1$. Но это означает, что $r_k$ имеет подпоследовательность, ограниченную от нуля. Следовательно, $(r_k)_{k=m}^\infty$ не может сходиться к нулю, что противоречит условию.

1.  Метод дихотомии и золотого сечения для унимодальных функций. Скорость сходимости.

    :::{.callout-note appearance="simple"}
    Методы локализации решения для скалярной минимизации. Сходятся линейно.
    :::

    **Метод дихотомии**

    Решаем следующую задачу: 
    $$
    f(x) \rightarrow \min_{x \in [a, b]}
    $$
    где $f(x)$ — унимодальная функция.

    Мы хотим на каждом шаге вдвое сокращать область, в которой ищем минимум. Для этого будем пользоваться основным свойством унимодальных функций:
    $$
    \forall a \leq x_1 < x_2 \leq b:
    $$
    $$
    f(x_1) \leq f(x_2) \Rightarrow x_* \in [a, x_2]
    $$
    $$
    f(x_1) \geq f(x_2) \Rightarrow x_* \in [x_1, b]
    $$
    где $x_*$ — точка, в которой достигается минимум

    Алгоритм:

    ![Алгоритм дихотомии](Dichotomy4.pdf)

    Можно заметить, что на каждой итерации требуется не более 2-х вычислений значения функции.

    **Сходимость метода дихотомии**

    Длина отрезка на $k$-ой итерации:
    $$
    \Delta_{k} = b_{k} - a_{k} = \frac{1}{2^k}(b - a)
    $$

    Если будем выбирать середину отрезка как выход $k$-ой итерации:
    $$
    |x_{k} - x_*| \leq \frac{\Delta_{k}}{2}
    $$

    Подставим полученное ранее выражение для длины отрезка:
    $$
    |x_{k} - x_*| \leq \frac{1}{2^{k+1}}(b - a)
    $$
    $$
    |x_{k} - x_*| \leq (0.5)^{k+1}(b - a)
    $$

    Получили выражение для сходимости по итерациям. Отсюда также можно выразить необходимое количество итераций для достижения точности $\varepsilon$:

    $$
    K = \left\lceil \log_2 \frac{b-a}{\varepsilon} - 1 \right\rceil
    $$

    Теперь получим выражение для сходимости по количеству вычислений значения функции. Знаем, что на каждой итерации вычисляем значение не более 2-х раз, значит количество вычислений значения функции возьмём $N = 2k$:

    $$
    |x_{k} - x_*| \leq (0.5)^{\frac{N}{2}+1}(b - a)
    $$
    $$
    |x_{k} - x_*| \leq (0.707)^{N}\frac{b - a}{2}
    $$

    **Метод золотого сечения**

    Идея такая же, как и в методе дихотомии, но хотим уменьшить количество вычислений значения функции. Для этого будем вычислять значения в точках золотого сечения. Так на каждой итерации нам нужно будет вычислять значение только в одной точке, так как для нового отрезка в одной из точек золотого сечения значение будет уже посчитано:

    ![Золотое сечение](golden_search.pdf)

    Алгоритм:

    ```python
    def golden_search(f, a, b, epsilon):
        tau = (sqrt(5) + 1) / 2
        y = a + (b - a) / tau**2
        z = a + (b - a) / tau
        while b - a > epsilon:
            if f(y) <= f(z):
                b = z
                z = y
                y = a + (b - a) / tau**2
            else:
                a = y
                y = z
                z = a + (b - a) / tau
        return (a + b) / 2
    ```

    **Сходимость метода золотого сечения**

    На каждой итерации длина отрезка будет уменьшаться в $\tau = \frac{\sqrt{5} + 1}{2}$ раз. Тогда оценка сходимости (и по итерациям, и по вычислениям значений функции):
    $$
    |x_{k} - x_*| \leq \frac{b_{k} - a_{k}}{2} = \left( \frac{1}{\tau} \right)^{N} \frac{b - a}{2} \approx 0.618^k\frac{b - a}{2}
    $$

    Получили сходимость по итерациям хуже, чем у дихотомии, так как отрезки уменьшаются слабее на каждой итерации. Но по количеству вычислений значения функции сходимость у метода золотого сечения быстрее.
1.  Автоматическое дифференцирование. Вычислительный граф. Forward/ Backward mode (в этом вопросе нет доказательств, но необходимо подробно описать алгоритмы).

    :::{.callout-note appearance="simple"}

    Для функции $L(w_1, w_2, \dotsc, w_d)$ вычислительный граф представляет собой ориентированный ациклический граф. Здесь $w_1, w_2, \dotsc, w_d$ обозначают входные переменные, а $v_1, v_2, \dotsc$ обозначают промежуточные функции. В $w_1, w_2, \dotsc, w_d$ нет входящих ребер, а входящие ребра в $v_1, v_2, \dotsc$ обозначают значения, подающиеся на вход функциям. Также есть одна единственная вершина, соответствующая итоговому значению $L$, из которой не выходит ребер.

    Это описание характерно для функции, используемой в обучении нейросети, однако, вершин-стоков $L_1, L_2, \dotsc$ может быть несколько.

    ![Вычислительный граф](comp_graph.pdf)
    :::

    Для вычисления $\frac{\partial L}{\partial w_k}$ для всех $k$ рассмотрим две процедуры.

    **Forward mode**

    Для каждой $w_1, w_2, \dotsc, w_d$ выполняется следующее: для всех $i$ вычисляются $\dot{v}_i = \frac{\partial v_i}{\partial w_k}$. Для этого выполняется:

    - Вычисляется $v_i$ как функция от её родителей (входов) $x_1, \ldots, x_{t_i}$:
    $$
    v_i = v_i(x_1, \ldots, x_{t_i})
    $$

    - Вычисляется производная $\dot{v}_i$ с использованием правила производной сложной функции:
    $$
    \dot{v}_i = \sum_{j=1}^{t_i} \frac{\partial v_i}{\partial x_j} \frac{\partial x_j}{\partial w_k}
    $$

    ![Forward mode](auto_diff_forward.pdf)

    Заметим, что для вычисления $\frac{\partial L}{\partial w_k}$ требуется $O(T)$ операций умножения и сложения градиентов, где $T$ - число элементов в вычислительном графе. Итого $O(dT)$.

    **Backward mode**

    **Forward pass**

    - Вычисляется $v_i$ как функция от её родителей (входов) $x_1, \ldots, x_{t_i}$:
    $$
    v_i = v_i(x_1, \ldots, x_{t_i})
    $$

    **Backward pass**

    Обрабатывая вершины в обратном порядке топологического упорядочения:

    - Вычисляется производная $\dot{v_i}$, используя правило дифференцирования сложной функции в обратном порядке и информацию от всех его потомков (выходов) $(x_1, \dots, x_{t_i})$, используя предподсчитанные значения на forward pass:
    $$
    \dot{v_i} = \frac{\partial L}{\partial \mathbf{v}_i} = \sum_{j=1}^{t_i} \frac{\partial L}{\partial x_j} \frac{\partial x_j}{\partial \mathbf{v}_i}
    $$

    ![Backward mode](auto_diff_reverse.pdf)

    Backward mode быстрее для единственного выхода $L$. Работает аналогично за линейное время.

    ![ [$\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Autograd_and_Jax.ipynb) График наглядно демонстрирует разницу между режимами работы автоматического дифференцирования. Функция $f(x): \mathbb{R}^n \to \mathbb{R}^m, f(x) = Ax$. Размерность $n = 100$ фиксирована, меняется количество строк $m$ случайной матрицы A. График показывает время, необходимое для вычисления якобиана $J = \left\{ \dfrac{\partial L_i}{\partial w_j} \right\}_{i,j}$](forward_vs_reverse_ad.pdf)
<!-- 1. Неравенство Йенсена для выпуклой функции и выпуклой комбинации точек. -->
1.  Выпуклость надграфика как критерий выпуклости функции.

    :::{.callout-note appearance="simple"}
    Чтобы функция $f(x)$, определенная на выпуклом множестве $X$, была выпуклой на $X$, необходимо и достаточно чтобы надграфик $f$ был выпуклым множеством.
    :::

    Для функции $f(x)$, определенной на $X \subseteq \mathbb{R}^n$, множество:
    $$
    \text{epi}\ f = \left\{[x, \mu] \in X \times \mathbb{R}: f(x) \le \mu \right\}
    $$
    называется **надграфиком** функции $f(x)$ (здесь $\mu \in \mathbb{R}, x \in S$).


    **Необходимость**

    Предположим, что $f(x)$ выпукла на $X$. Возьмем две произвольные точки $[x_1, \mu_1] \in \text{epi}f$ и $[x_2, \mu_2] \in \text{epi}f$. Также возьмем $0 \leq \lambda \leq 1$ и обозначим $x_{\lambda} = \lambda x_1 + (1 - \lambda) x_2, \mu_{\lambda} = \lambda \mu_1 + (1 - \lambda) \mu_2$. Тогда,
    $$
    \lambda\begin{bmatrix} x_1 \\ \mu_1 \end{bmatrix} + (1 - \lambda)\begin{bmatrix} x_2 \\ \mu_2 \end{bmatrix} = \begin{bmatrix} x_{\lambda} \\ \mu_{\lambda} \end{bmatrix}.
    $$
    Из выпуклости $X$ следует, что $x_{\lambda} \in X$. Более того, так как $f(x)$ -- выпуклая функция, то
    $$
    f(x_{\lambda}) \leq \lambda f(x_1) + (1 - \lambda) f(x_2) \leq \lambda \mu_1 + (1 - \lambda) \mu_2 = \mu_{\lambda}
    $$
    Из неравенства выше по определению надграфика следует, что $\begin{bmatrix} x_{\lambda} \\ \mu_{\lambda} \end{bmatrix} \in \text{epi}f$. Следовательно, надграфик $f$ -- выпуклое множество.

    **Достаточность**

    Предположим, что надграфик $f$, $\text{epi}f$, выпуклое множество. Тогда, исходя из того что $[x_1, \mu_1] \in \text{epi}f$ и $[x_2, \mu_2] \in \text{epi}f$, получаем
    $$
    \begin{bmatrix} x_{\lambda} \\ \mu_{\lambda} \end{bmatrix} = \lambda\begin{bmatrix} x_1 \\ \mu_1 \end{bmatrix} + (1 - \lambda)\begin{bmatrix} x_2 \\ \mu_2 \end{bmatrix} \in \text{epi}f
    $$
    для любого $0 \leq \lambda \leq 1$.

    Следовательно, из определения надграфика, подставив значение $\mu_{\lambda}$, получаем, что $f(x_{\lambda}) \leq \mu_{\lambda} = \lambda \mu_1 + (1 - \lambda) \mu_2$. 
    $$
    f(x_{\lambda}) = f(\lambda x_1 + (1 - \lambda) x_2) \leq \mu_\lambda = \lambda \mu_1 + (1 - \lambda) \mu_2
    $$

    Но это верно для всех $\mu_1 \geq f(x_1)$ и $\mu_2 \geq f(x_2)$, в том числе и при $\mu_1 = f(x_1)$ и $\mu_2 = f(x_2)$. Тогда мы получаем неравенство:
    $$
    f(x_{\lambda}) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
    $$

    Так как $x_1 \in X$ и $x_2 \in X$ выбирались произвольно, $f(x)$ - выпуклая функция на $X$.

1.  Дифференциальный критерий сильной выпуклости первого порядка.

    :::{.callout-note appearance="simple"}
    Пусть $f(x)$ — дифференцируемая функция на выпуклом множестве $X \subseteq \mathbb{R}^n$. Тогда $f(x)$ сильно выпукла на $X$ с константой $\mu > 0$ тогда и только тогда, когда

    $$
    f(x) - f(x_0) \geq \langle \nabla f(x_0), x - x_0 \rangle + \frac{\mu}{2} \|x - x_0\|^2
    $$

    для всех $x, x_0 \in X$.
    :::

    **Необходимость**

    Пусть $0 < \lambda \leq 1$. Согласно определению сильно выпуклой функции,

    $$
    f(\lambda x + (1 - \lambda)x_0) \leq \lambda f(x) + (1 - \lambda) f(x_0) - \frac{\mu}{2} \lambda (1 - \lambda) \|x - x_0\|^2
    $$

    или эквивалентно,

    $$
    f(x) - f(x_0) - \frac{\mu}{2} (1 - \lambda) \|x - x_0\|^2 \geq \frac{1}{\lambda} \left[ f(\lambda x + (1 - \lambda)x_0) - f(x_0) \right] =
    $$

    $$
    = \frac{1}{\lambda} \left[ f(x_0 + \lambda(x - x_0)) - f(x_0) \right] = \frac{1}{\lambda} \left[ \lambda \langle \nabla f(x_0), x - x_0 \rangle + o(\lambda) \right] =
    $$

    $$
    = \langle \nabla f(x_0), x - x_0 \rangle + \frac{o(\lambda)}{\lambda}.
    $$

    Таким образом, переходя к пределу при $\lambda \to 0$, мы приходим к первоначальному утверждению.

    **Достаточность**

    Предположим, что неравенство в теореме выполнено для всех $x, x_0 \in X$. Возьмем $x_0 = \lambda x_1 + (1 - \lambda)x_2$, где $x_1, x_2 \in X$, $0 \leq \lambda \leq 1$. Согласно неравенству из условия теоремы, выполняются следующие неравенства:

    $$
    f(x_1) - f(x_0) \geq \langle \nabla f(x_0), x_1 - x_0 \rangle + \frac{\mu}{2} \|x_1 - x_0\|^2,
    $$

    $$
    f(x_2) - f(x_0) \geq \langle \nabla f(x_0), x_2 - x_0 \rangle + \frac{\mu}{2} \|x_2 - x_0\|^2.
    $$

    Умножая первое неравенство на $\lambda$ и второе на $1 - \lambda$ и складывая их, учитывая, что

    $$
    x_1 - x_0 = (1 - \lambda)(x_1 - x_2), \quad x_2 - x_0 = \lambda(x_2 - x_1),
    $$

    и что $\lambda(1 - \lambda)^2 + \lambda^2(1 - \lambda) = \lambda(1 - \lambda)$, получаем:

    $$
    \lambda f(x_1) + (1 - \lambda)f(x_2) - f(x_0) - \frac{\mu}{2} \lambda(1 - \lambda) \|x_1 - x_2\|^2 \geq
    $$

    $$
    \geq \langle \nabla f(x_0), \lambda x_1 + (1 - \lambda)x_2 - x_0 \rangle = 0.
    $$

    Таким образом, неравенство из определения сильно выпуклой функции выполнено. Важно отметить, что при $\mu = 0$ получаем случай выпуклости и соответствующий дифференциальный критерий.

1.  Дифференциальный критерий сильной выпуклости второго порядка.

    :::{.callout-note appearance="simple"}

    Пусть $X \subseteq \mathbb{R}^n$ — выпуклое множество с непустой внутренностью. Пусть также $f(x)$ — дважды непрерывно дифференцируемая функция на $X$. Тогда $f(x)$ сильно выпукла на $X$ с константой $\mu > 0$ тогда и только тогда, когда

    $$
    \langle y, \nabla^2 f(x) y \rangle \geq \mu \|y\|^2
    $$

    для всех $x \in X$ и $y \in \mathbb{R}^n$.

    Другая форма записи:
    $$ \nabla^2 f(x) \succcurlyeq \mu I $$
    :::

    Целевое неравенство тривиально, когда $y = 0_n$, поэтому предположим, что $y \neq 0_n$.

    **Необходимость**

    Пусть $x$ является внутренней точкой $X$. Тогда $x + \alpha y \in X$ для всех $y \in \mathbb{R}^n$ и достаточно малых $\alpha$. Поскольку $f(x)$ дважды дифференцируема,

    $$
    f(x + \alpha y) = f(x) + \alpha \langle \nabla f(x), y \rangle + \frac{\alpha^2}{2} \langle y, \nabla^2 f(x) y \rangle + o(\alpha^2).
    $$

    Основываясь на критерии первого порядка сильной выпуклости, имеем

    $$
    \frac{\alpha^2}{2} \langle y, \nabla^2 f(x) y \rangle + o(\alpha^2) = f(x + \alpha y) - f(x) - \alpha \langle \nabla f(x), y \rangle \geq \frac{\mu}{2} \alpha^2 \|y\|^2.
    $$

    Это неравенство сводится к целевому неравенству после деления обеих частей на $\alpha^2$ и перехода к пределу при $\alpha \to 0$.

    Если $x \in X$, но $x \notin \text{int}X$, рассмотрим последовательность $\{x_k\}$ такую, что $x_k \in \text{int}X$ и $x_k \to x$ при $k \to \infty$. Тогда мы приходим к целевому неравенству после перехода к пределу.

    **Достаточность**

    Формула Тейлора с остаточным членом Лагранжа второго порядка $\forall x, y: x, x + y \in X$ найдется $\alpha$ такая, что:

    $$
    f(x + y) = f(x) + \langle \nabla f(x), y \rangle + \frac{1}{2} \langle y, \nabla^2 f(x + \alpha y) y \rangle
    $$

    где $0 < \alpha < 1$.

    Используя формулу Тейлора с остаточным членом Лагранжа и неравенство из условия, получаем для $x + y \in X$:

    $$
    f(x + y) - f(x) - \langle \nabla f(x), y \rangle = \frac{1}{2} \langle y, \nabla^2 f(x + \alpha y) y \rangle \geq \frac{\mu}{2} \|y\|^2,
    $$

    где $0 \leq \alpha \leq 1$. Следовательно,

    $$
    f(x + y) - f(x) \geq \langle \nabla f(x), y \rangle + \frac{\mu}{2} \|y\|^2.
    $$

    Таким образом, по критерию первого порядка сильной выпуклости, функция $f(x)$ является сильно выпуклой с константой $\mu$. Важно отметить, что $\mu = 0$ соответствует случаю выпуклости и соответствующему дифференциальному критерию.

1.  Необходимые условия безусловного экстремума.

    :::{.callout-note appearance="simple"}
    Если в $x^*$ достигается локальный минимум и $f$ непрерывно дифференцируема в открытой окрестности $x^*$, то
    $$
    \nabla f(x^*) = 0
    $$
    :::

    Предположим обратное. Пусть $\nabla f(x^*) \neq 0$. Рассмотрим вектор $p = -\nabla f(x^*)$ и заметим, что
    $$
    p^T \nabla f(x^*) = -\| \nabla f(x^*) \|^2 < 0
    $$
    Так как $\nabla f$ непрерывна в окрестности $x^*$, то существует скаляр $T > 0$ такой, что
    $$
    p^T \nabla f(x^* + tp) < 0, \text{ для любого } t \in [0,T]
    $$
    Для любого $\bar{t} \in (0, T]$, мы можем воспользоваться теоремой Тейлора:
    $$
    f(x^* + \bar{t}p) = f(x^*) + \bar{t} p^T \nabla f(x^* + tp), \text{ для некоторого } t \in (0,\bar{t})
    $$
    Следовательно, $f(x^* + \bar{t}p) < f(x^*)$ для любого $\bar{t} \in (0, T]$. Мы нашли направление, идя вдоль которого из $x^*$ функция $f$ убывает. Тогда $x^*$ -- не точка локального минимума. Получили противоречие.

1.  Достаточные условия безусловного экстремума.

    :::{.callout-note appearance="simple"}
    Пусть $\nabla^2 f$ непрерывна в открытой окрестности $x^*$ и
    $$
    \nabla f(x^*) = 0 \quad \nabla^2 f(x^*) \succ 0.
    $$
    Тогда $x^*$ -- точка локального минимума $f$.
    :::

    Так как гессиан непрерывен и положительно определен в $x^*$, то мы можем выбрать радиус $r > 0$ такой, что $\nabla^2 f(x)$ остается положительно определенной для всех $x$ в открытом шаре $B = \{ z \mid \|z - x^*\| < r \}$. Взяв любой ненулевой вектор $p$, для которого выполняется $\|p\| < r$, мы получаем $x^* + p \in B$, а также по формуле Тейлора:
    $$ 
    f(x^* + p) = f(x^*) + p^T \nabla f(x^*) + \frac{1}{2} p^T \nabla^2 f(z) p
    $$
    $$ 
    = f(x^*) + \frac{1}{2} p^T \nabla^2 f(z) p
    $$
    где $z = x^* + tp$ для некоторого $t \in (0,1)$. Так как $z \in B$, мы получаем $p^T \nabla^2 f(z) p > 0$, и следовательно $f(x^* + p) > f(x^*)$. Таким образом $x^*$ -- точка локального минимума.
1.  Симплекс метод для задачи линейного программирования с ограничениями в форме неравенств. Двухфазный симплекс метод.

    :::{.callout-note appearance="simple"}
    Решаемая задача:
    $$
    \begin{aligned}
    & \min_{x \in \mathbb{R}^n} && c^T x \\
    & \text{s.t.} && Ax \leq b
    \end{aligned}
    $$
    Идея метода:
    1. Убедиться, что точка, в которой мы находимся, является угловой
    2. Проверить оптимальность точки
    3. Если необходимо, сменить угол (то есть сменить базис)
    4. Повторять до сходимости

    Идея двухфазного симплекс метода:
    1. Сформулировать вспомогательную задачу линейного программирования, решение которой даст начальную базисную допустимую точку для исходной задачи.
    1. Угадать начальную базисную допустимую точку для вспомогательной задачи и решить её (симплекс методом).
    1. Использовать решение вспомогательной задачи как начальную базисную допустимую точку для исходной задачи.
    :::

    Задача линейного программирования с ограничениями в форме неравенств:

    Пусть $c \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, $A \in \mathbb{R}^{m \times n}$, тогда задача формулируется так:

    $$
    \begin{aligned}
    & \min_{x \in \mathbb{R}^n} && c^T x \\
    & \text{s.t.} && Ax \leq b
    \end{aligned}
    $$

    Шаги выполнения симплекс метода:

    1. **Поиск начальной базисной допустимой точки:**
        - Выберем начальную базисную точку (она является решением системы 
        $A_{\mathcal{B}} x_0 = b_{\mathcal{B}}$, где $\mathcal{B}$ - базис размера $n$, а матрица $A$ обычно имеет больше $n$ ограничений) допустимую ($Ax_0 \leq b$) точку 
        $x_0$ (искать ее будем через двухфазный симплекс-метод). $A_{\mathcal{B}} \in \mathbb{R}^{n \times n}$ - подматрица матрицы $A$ с базисными строками, $b_{\mathcal{B}} \in \mathbb{R}^n$ - подвектор вектора $b$ с базисными строками.

    2. **Проверка оптимальности:**
        - Разложение вектора 
        $c$ в данном базисе $\mathcal{B}$ с коэффициентами 
        $\lambda_{\mathcal{B}}$:
        $$
        \lambda_{\mathcal{B}}^\top A_{\mathcal{B}} = c^\top \quad \text{или} \quad \lambda_{\mathcal{B}}^\top = c^\top A_{\mathcal{B}}^{-1}
        $$
        - Если все компоненты 
        $\lambda_{\mathcal{B}}$ неположительны, текущий базис является оптимальным. Иначе далее меняем вершину симплекса.

    3. **Определение переменной для удаления из базиса:**
        - Если в разложении 
        $\lambda_{\mathcal{B}}$ есть положительные координаты, продолжаем оптимизацию. Пусть 
        $\lambda_{\mathcal{B}}^k > 0$. Необходимо исключить 
        $k$ из базиса. Рассчитаем направляющий вектор 
        $d$, идя вдоль которого изменим вершину следующим образом: во-первых, для всех векторов ограничений из базиса, которые мы оставляем, подбираемое направление должно быть им ортогонально (т.е. движение вдоль него не будет нарушать остальные ограничения), и, во-вторых, вдоль него значение, связанное с нашим ограничением, должно убывать:
        $$
        \begin{cases}
        A_{\mathcal{B} \backslash \{k\} } d = 0 \\
        a_k^\top d < 0
        \end{cases}
        $$

    4. **Вычисление шага вдоль выбранного направления $d$:**
        - Для всех 
        $j \notin \mathcal{B}$ считаем шаг:
        $$
        \mu_j = \frac{b_j - a_j^\top x_{\mathcal{B}}}{a_j^\top d}
        $$
        - Новая вершина, которую добавим в базис:
        $$
        t = \arg\min_j \{\mu_j \mid \mu_j > 0\}
        $$

    5. **Обновление базиса:**
        - Обновляем базис и текущее решение:
        $$
        \begin{aligned}
            \mathcal{B}' = \mathcal{B} \backslash \{k\} \cup \{t\}, \\
            x_{\mathcal{B}'} = x_{\mathcal{B}} + \mu_t d = A_{\mathcal{B}'}^{-1} b_{\mathcal{B}'}
        \end{aligned}
        $$
        - Изменение базиса приводит к уменьшению значения целевой функции:
        $$
        c^\top x_{B'} = c^\top (x_{\mathcal{B}} + \mu_t d) = c^\top x_{\mathcal{B}} + \mu_t c^\top d
        $$

    6. **Повторение:**
        - Далее повторяем шаги 2-5 до достижения оптимального решения или установления, что задача не имеет допустимого решения.

    Нахождение начальной базисной допустимой точки


    Целью является решение следующей задачи:
    $$
    \begin{split}
    &\min_{x \in \mathbb{R}^n} c^{\top}x \\
    \text{s.t. } & Ax \leq b
    \end{split}
    $$ {#eq-LP_ineq}
    Предложенный алгоритм требует начальной базисной допустимой точки и соответствующего базиса.

    Начнем с переформулирования задачи:
    $$
    \begin{split}
    &\min_{y \in \mathbb{R}^n, z \in \mathbb{R}^n} c^{\top}(y-z) \\
    \text{s.t. } & Ay - Az \leq b \\ 
    & y \geq 0, z \geq 0 \\ 
    \end{split}
    $$ {#eq-LP_ineq_new}

    Имея решение [задачи @eq-LP_ineq_new], решение [задачи @eq-LP_ineq] может быть однозначно получено и наоборот:
    $$
    x = y-z \qquad \Leftrightarrow \qquad y_i = \max(x_i, 0), \quad z_i = \max(-x_i, 0)
    $$
    Теперь мы попытаемся сформулировать новую задачу ЛП, решение которой будет базисной допустимой точкой для [задачи @eq-LP_ineq_new]. Это означает, что мы сначала запустим симплекс-метод для [задачи @eq-LP_ineq_new], а затем запустим [задачу @eq-LP_ineq] с известной начальной точкой. Обратите внимание, что базисная допустимая точка для [задачи @eq-LP_ineq_new] должна быть легко вычислима.

    $$
    \tag{Phase-2 (Main LP)}
    \begin{split}
    &\min_{y \in \mathbb{R}^n, z \in \mathbb{R}^n} c^{\top}(y-z) \\
    \text{s.t. } & Ay - Az \leq b \\ 
    & y \geq 0, z \geq 0 \\ 
    \end{split}
    $$
    $$
    \tag{Phase-1}
    \begin{split}
    &\min_{\xi \in \mathbb{R}^m, y \in \mathbb{R}^n, z \in \mathbb{R}^n} \sum\limits_{i=1}^m \xi_i \\
    \text{s.t. } & Ay - Az \leq b + \xi \\ 
    & y \geq 0, z \geq 0, \xi \geq 0 \\ 
    \end{split}
    $$

    * Если [задача @eq-LP_ineq_new] имеет допустимое решение, то оптимум [задачи @eq-LP_ineq_new] равен нулю (т.е. все переменные $\xi_i$ равны нулю).
        
        **Доказательство:** тривиальная проверка.
    * Если оптимум [задачи @eq-LP_ineq_new] равен нулю (т.е. все переменные $\xi_i$ равны нулю), то мы получаем допустимый базис для [задачи @eq-LP_ineq_new].
        
        **Доказательство:** тривиальная проверка.

    * Теперь мы знаем, что если мы можем решить [задачу @eq-LP_ineq_new], то мы либо найдем начальную точку для симплекс-метода в исходной задаче (если все переменные $\xi_i$ равны нулю), либо проверим, что исходная задача не имеет допустимого решения (если все переменные $\xi_i$ не равны нулю).
    * Но как решить [задачу @eq-LP_ineq_new]? Она имеет базисную допустимую точку (задача имеет $2n + m$ переменных и точка ниже гарантирует, что $2n + m$ неравенств выполняются как равенства (активны)).
    $$
    z = 0 \quad y = 0 \quad \xi_i = \max(0, -b_i)
    $$

1.  Теорема сходимости градиентного спуска для гладких PL функций.

    :::{.callout-note appearance="simple"}
    Рассмотрим задачу 
    $$
    f(x) \to \min_{x \in \mathbb{R}^d}
    $$
    и предположим, что $f$ удовлетворяет условию Поляка-Лоясиевича с константой $\mu$ и $L$-гладкости, для некоторых $L\geq \mu >0$.
    Пусть $(x_k)_{k \in \mathbb{N}}$ - последовательность, созданная градиентным спуском с постоянным шагом $\alpha$, $0<\alpha \leq \frac{1}{L}$. Тогда имеется линейная сходимость:
    $$
    f(x_k)-f^* \leq (1-\alpha \mu)^k (f(x_0)-f^*).
    $$
    :::

    1. Используя $L$-гладкость, вместе с правилом обновления алгоритма, можно записать:
        $$
        \begin{split}
        f(x_{k+1})& \leq f(x_k) + \langle \nabla f(x_k), x_{k+1}-x_k \rangle +\frac{L}{2} \| x_{k+1}-x_k\|^2\\ 
        &= f(x_k)-\alpha\Vert \nabla f(x_k) \Vert^2 +\frac{L \alpha^2}{2} \| \nabla f(x_k)\|^2 \\ 
        &= f(x_k) - \frac{\alpha}{2} \left(2 - L \alpha \right)\Vert \nabla f(x_k) \Vert^2 \\ 
        & \leq f(x_k) - \frac{\alpha}{2}\Vert \nabla f(x_k)\Vert^2,
        \end{split}
        $$
        В последнем неравенстве использовали предположение о шаге $0 < \alpha L \leq 1$.
    1. Используя свойство Поляка-Лоясиевича, можно записать:
        $$
        f(x_{k+1}) \leq f(x_k) - \alpha \mu (f(x_k) - f^*).
        $$
    1. Вычитая $f^*$ с обеих сторон и используя рекурсию, получаем:
        $$
        f(x_k) - f^* \leq (1-\alpha \mu)^k (f(x_0) - f^*).
        $$

1.  Теорема сходимости градиентного спуска для сильно выпуклых квадратичных функций. Оптимальные гиперпараметры.

    :::{.callout-note appearance="simple"}
    $$ f(x) \rightarrow \min\limits_{x \in \mathbb{R}^d} $$
    $$ f(x) = \frac{1}{2}x^TAx - b^Tx + c, \ A \in \mathbb{S}_{++} $$
    Тогда градиентный спуск с шагом $\alpha = \frac{2}{\mu + L}$ сходится линейно с показателем $\frac{L-\mu}{L+\mu}$
    $$
    f(x_k) - f^* \leqslant \left(\frac{L-\mu}{L+\mu}\right)^k(f(x_0) - f^*).
    $$
    :::

    $$
    \nabla f(x) = Ax - b \overset{\nabla f(x^*) = 0}{\Rightarrow} Ax^* = b
    $$
    Тогда шаг градиентного спуска имеет вид
    $$
    x_{k+1} = x_k - \alpha (Ax - b)
    $$
    Найдем $\alpha^*$. Воспользуемся $A = Q\Lambda Q^T$, где $\Lambda = \text{diag}\{\lambda_1, \ldots, \lambda_n\}, \ Q = \|q_1, \ldots, q_n\|$, $\lambda_i, q_i$ - собственное значение и собственный вектор соответственно.
    $$ x_{k+1}  = (I - \alpha A)x_k + \alpha A x^* \ | \ -x^*$$
    $$ x_{k + 1} - x^* = (I - \alpha A)(x_k - x^*) $$
    $$ x_{k + 1} - x^* = (I - \alpha Q\Lambda Q^T)(x_k - x^*) \ | \ \cdot Q^T $$
    $$ Q^T(x_{k+1} - x^*) = (Q^T - \alpha \Lambda Q^T)(x_k - x^*) = (I - \alpha \Lambda)Q^T(x_k - x^*) $$
    $$ \text{Замена: } \tilde{x} = Q^T(x - x^*) \Rightarrow \tilde{x}_{k+1} = (I - \alpha \Lambda)\tilde{x}_{k} \Leftrightarrow \tilde{x}_i^{(k+1)} = (1 - \alpha \lambda_i)\tilde{x}_i^{(k)} \ i=\overline{1,d} $$
    $$ \lambda_{\min} = \mu, \quad \lambda_{\max} = L $$
    Сходимость есть $\Leftrightarrow \max\limits_{i}|1 - \alpha \lambda_i| < 1$
    $$
    \left\{ 
    \begin{array}{rl}
        |1 - \lambda \mu| < 1 \Rightarrow &1 - \lambda \mu < 1 \Rightarrow \alpha > 0 \\
        &\alpha \mu - 1 < 1 \Rightarrow \alpha < \frac{2}{\mu} \\
        |1 - \alpha L| < 1 \Rightarrow &1 - \alpha L < 1 \Rightarrow \alpha > 0 \\
        &\alpha L - 1 < 1 \Rightarrow \alpha < \frac{2}{L}
    \end{array}
    \right\} \Rightarrow \alpha < \frac{2}{L}
    $$
    Радиус сходимости $\rho = \max (|1 - \alpha \mu|, |1 - \alpha L|)$ и $\rho \rightarrow \min \Leftrightarrow \alpha^*L - 1 = 1 - \alpha^*\mu \Rightarrow \alpha^* = \frac{2}{\mu + L}$ и $\rho^* = \frac{L - \mu}{L + \mu}$

    Итого получаем, что для градиентного спуска выполняется $\|x_k - x^*\| \leqslant \left(\frac{L-\mu}{L+\mu}\right)^{k}\|x_0 - x^*\|$.