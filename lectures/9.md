---
title: "Ускоренные градиентные методы. Идея метода сопряженных градиентов"
author: Даня Меркулов, Петр Остроухов
institute: Оптимизация для всех! ЦУ
format: 
    beamer:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
    beamer-cu:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader_cu.tex
      header-includes:
        - \newcommand{\cover}{../files/БАК_Методы вып_оптимизации_презентация_8.pdf}
    beamer-cu-maga:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader_cu.tex
      header-includes:
        - \newcommand{\cover}{../files/Методы вып_оптимизации_презентация_10.pdf}
header-includes:
 - \newcommand{\bgimage}{../files/back10.jpeg}
---

# Повторение

## Результаты сходимости градиентного спуска для гладких функций

$$
\text{Градиентный спуск:} \qquad \qquad \min_{x \in \mathbb{R}^n} f(x) \qquad \qquad x_{k+1} = x_k - \alpha_k \nabla f(x_k) \qquad \lambda \left( \nabla^2 f(x)\right) \in \left[ \mu, L \right], \varkappa = \tfrac{L}{\mu}
$$

|выпуклая (негладкая) | гладкая (невыпуклая) | гладкая & выпуклая | гладкая & сильно выпуклая |
|:------:|:-------:|:------:|:--------:|
| $f(x_k) - f^* =  \mathcal{O} \left( \tfrac{1}{\sqrt{k}} \right)$ | $\min\limits_{0 \leq i \leq k}\|\nabla f(x_i)\| = \mathcal{O} \left( \tfrac{1}{\sqrt{k}} \right)$ | $f(x_k) - f^* =  \mathcal{O} \left( \tfrac{1}{k} \right)$ | $\|x_k - x^*\|^2 = \mathcal{O} \left( \left(1 - \tfrac{\mu}{L}\right)^k \right)$ |
| $k_\varepsilon =  \mathcal{O} \left( \tfrac{1}{\varepsilon^2} \right)$ | $k_\varepsilon = \mathcal{O} \left( \tfrac{1}{\varepsilon^2} \right)$ | $k_\varepsilon =  \mathcal{O}  \left( \tfrac{1}{\varepsilon} \right)$ | $k_\varepsilon  = \mathcal{O} \left( \varkappa \log \tfrac{1}{\varepsilon}\right)$ |

## Нижние оценки для произвольных методов I порядка на классе гладких функций

$$
\text{Произвольный метод I порядка:} \qquad \min_{x \in \mathbb{R}^n} f(x) \qquad x_{k+1} = x_k - \sum_{i=0}^k\alpha_i \nabla f(x_i) \qquad \lambda \left( \nabla^2 f(x) \right) \in \left[ \mu, L \right], \varkappa = \tfrac{L}{\mu}
$$

| выпуклая (негладкая) | гладкая (невыпуклая)^[[Carmon, Duchi, Hinder, Sidford, 2017](https://arxiv.org/pdf/1710.11606.pdf)] | гладкая & выпуклая^[[Nemirovski, Yudin, 1979](https://fmin.xyz/assets/files/nemyud1979.pdf)] | гладкая & сильно выпуклая |
|:------:|:-------:|:------:|:--------:|
| $f(x_k) - f^* =  \Omega \left( \tfrac{1}{\sqrt{k}} \right)$ | $\min\limits_{0 \leq i \leq k}\|\nabla f(x_i)\| = \Omega \left( \tfrac{1}{\sqrt{k}} \right)$ | $f(x_k)-f^*=\Omega\!\left(\tfrac{1}{k^2}\right)$ | $f(x_k)-f^*=\Omega\!\left(\left(\tfrac{\sqrt{\varkappa}-1}{\sqrt{\varkappa}+1}\right)^{\!2k}\right)$ |
| $k_\varepsilon =  \Omega \left( \tfrac{1}{\varepsilon^2} \right)$ | $k_\varepsilon=\Omega\!\left(\tfrac{1}{\varepsilon^2}\right)$ | $k_\varepsilon=\Omega\!\left(\tfrac{1}{\sqrt{\varepsilon}}\right)$ | $k_\varepsilon=\Omega\!\big(\sqrt{\varkappa}\,\log\tfrac{1}{\varepsilon}\big)$ |




# Метод тяжёлого шарика

## Колебания и ускорение

$$
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1}).
$$

[![](GD_vs_HB_hor.pdf)](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/GD.ipynb)

## Метод тяжёлого шарика Поляка

:::: {.columns}

::: {.column width="25%"}
![](GD_HB.pdf)
:::

::: {.column width="75%"}
Давайте представим идею моментума (импульса, тяжёлого шарика), предложенную Б.Т. Поляком в 1964 году. Обновление метода тяжёлого шарика имеет вид
$$
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1}).
$$
\pause
Давайте теперь подставим в итерацию предыдущую итерацию, т.е. $x_{k} = x_{k-1} - \alpha \nabla f(x_{k-1}) + \beta (x_{k-1} - x_{k-2})$, а так же отметим, что $x_{k} - x_{k-1} = -\alpha \nabla f(x_{k-1}) + \beta (x_{k-1} - x_{k-2})$:
\pause
$$
\begin{aligned}
x_{k+1} &= x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1}) \pause \\ 
&= x_k - \alpha \nabla f(x_k) + \beta \left( -\alpha \nabla f(x_{k-1}) + \beta (x_{k-1} - x_{k-2}) \right) \pause \\ 
&= x_k - \alpha \left[ \nabla f(x_k) + \beta \nabla f(x_{k-1})\right] + \beta^2 (x_{k-1} - x_{k-2}) \pause \\ 
&= x_k - \alpha \left[ \nabla f(x_k) + \beta \nabla f(x_{k-1}) + \beta^2 \nabla f(x_{k-2})\right] + \beta^3 (x_{k-2} - x_{k-3}) \pause \\ 
&\vdots \pause \\ 
&= x_k - \alpha \left[ \nabla f(x_k) + \beta \nabla f(x_{k-1}) + \beta^2 \nabla f(x_{k-2}) + \cdots + \beta^{k} \nabla f(x_0) \right]
\end{aligned}
$$
\pause
Таким образом, метод тяжёлого шарика учитывает все предудущие градиенты с тем меньшим весом, чем старше итерация ($0 \leq \beta < 1$).
:::
::::



## Сходимость метода тяжёлого шарика для квадратичной функции

:::{.callout-theorem}
Предположим, что $f$ является $\mu$-сильно выпуклой и $L$-гладкой квадратичной функцией. Тогда метод тяжёлого шарика с параметрами
$$
\alpha = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \beta = \left(\dfrac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}\right)^2
$$

сходится линейно:

$$
\|x_k - x^*\|_2 \leq \left( \dfrac{\sqrt{\varkappa} - 1}{\sqrt{\varkappa} + 1} \right)^k \|x_0 - x^*\|
$$

:::

## Глобальная сходимость метода тяжёлого шарика ^[[Глобальная сходимость метода тяжёлого шарика для выпуклой оптимизации, Euhanna Ghadimi et al.](https://arxiv.org/abs/1412.7457)]

:::{.callout-theorem}
Предположим, что $f$ является гладкой и выпуклой и что

$$
\beta\in[0,1),\quad \alpha\in\biggl(0,\dfrac{2(1-\beta)}{L}\biggr).
$$

Тогда последовательность $\{x_k\}$, генерируемая итерациями тяжёлого шарика, удовлетворяет

$$
f(\overline{x}_T)-f^{\star} \leq  \left\{
\begin{array}[l]{ll}
\frac{\Vert x_{0}-x^\star\Vert^2}{2(T+1)}\biggl(\frac{L\beta}{1-\beta}+\frac{1-\beta}{\alpha}\biggr),\;\;\textup{if}\;\;
\alpha\in\bigl(0,\dfrac{1-\beta}{L}\bigr],\\
\frac{\Vert x_{0}-x^\star\Vert^2}{2(T+1)(2(1-\beta)-\alpha L)}\biggl({L\beta}+\frac{(1-\beta)^2}{\alpha}\biggr),\;\;\textup{if}\;\;
\alpha\in\bigl[\dfrac{1-\beta}{L},\dfrac{2(1-\beta)}{L}\bigr),
\end{array}
\right.
$$

где $\overline{x}_T$ среднее Чезаро последовательности итераций, т.е. 

$$
\overline{x}_T = \frac{1}{T+1}\sum_{k=0}^T x_k.
$$
:::


## Глобальная сходимость метода тяжёлого шарика ^[[Глобальная сходимость метода тяжёлого шарика для выпуклой оптимизации, Euhanna Ghadimi et al.](https://arxiv.org/abs/1412.7457)]

:::{.callout-theorem}
Предположим, что $f$ является гладкой и сильно выпуклой и что

$$
\alpha\in\biggl(0,\dfrac{2}{L}\biggr),\quad 0\leq  \beta<\dfrac{1}{2}\biggl( \dfrac{\mu \alpha}{2}+\sqrt{\dfrac{\mu^2\alpha^2}{4}+4(1-\frac{\alpha L}{2})} \biggr) .
$$

Тогда последовательность $\{x_k\}$, генерируемая итерациями методатяжёлого шарика, сходится линейно к единственному оптимальному решению $x^\star$. В частности,

$$
f(x_{k})-f^\star \leq q^k (f(x_0)-f^\star),
$$

где $q\in[0,1)$.
:::

## Итоги по методу тяжёлого шарика

* Обеспечивает ускоренную сходимость для сильно выпуклых квадратичных задач.
* Локально ускоренная сходимость была доказана в оригинальной статье.
* Недавно ^[[Provable non-accelerations of the heavy-ball method](https://arxiv.org/pdf/2307.11291)] было доказано, что глобального ускорения сходимости для метода не существует.
* Метод не был чрезвычайно популярен до ML-бума.
* Сейчас он фактически является стандартом для практического ускорения методов градиентного спуска, в том числе для невыпуклых задач (обучение нейронных сетей).

# Ускоренный градиентный метод Нестерова

## Концепция ускоренного градиентного метода Нестерова

:::: {.columns}

::: {.column width="27%"}
$$
x_{k+1} = x_k - \alpha \nabla f(x_k)
$$
:::
::: {.column width="34%"}
$$
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1})
$$
:::
::: {.column width="39%"}
$$
\begin{cases}y_{k+1} = x_k + \beta (x_k - x_{k-1}) \\ x_{k+1} = y_{k+1} - \alpha \nabla f(y_{k+1}) \end{cases}
$$
:::

::::

. . .

:::: {.columns}
::: {.column width="67%"}

Давайте определим следующие обозначения

$$
\begin{aligned}
x^+ &= x - \alpha \nabla f(x) \qquad &\text{Градиентный шаг} \\
d_k &= \beta_k (x_k - x_{k-1}) \qquad &\text{Импульс}
\end{aligned}
$$

Тогда мы можем записать:


$$
\begin{aligned}
x_{k+1} &= x_k^+ \qquad &\text{Градиентный спуск} \\
x_{k+1} &= x_k^+ + d_k \qquad &\text{Метод тяжёлого шарика} \\
x_{k+1} &= (x_k + d_k)^+ \qquad &\text{Ускоренный градиентный метод Нестерова}
\end{aligned}
$$
:::
::: {.column width="33%"}

![](AGD.pdf)

:::
::::
## Сходимость для выпуклых функций

:::{.callout-theorem}
Предположим, что $f : \mathbb{R}^n \rightarrow \mathbb{R}$ является выпуклой и $L$-гладкой. Ускоренный градиентный метод Нестерова (NAG) предназначен для решения задачи минимизации, начиная с начальной точки $x_0 = y_0 \in \mathbb{R}^n$ и $\lambda_0 = 0$. Алгоритм выполняет следующие шаги:
$$
\begin{aligned}
&\textbf{Обновление градиента: } &x_{k+1} &= y_k - \frac{1}{L} \nabla f(y_k) \\
& \textbf{Вес экстраполяции: } &\lambda_{k+1} &= \frac{1 + \sqrt{1 + 4\lambda_k^2}}{2} \\
& \quad &\gamma_k &= \frac{\lambda_k - 1}{\lambda_{k+1}} \\
&\textbf{Экстраполяция: } &y_{k+1} &= x_{k+1} + \gamma_k\left(x_{k+1} - x_k\right)
\end{aligned}
$$
Последовательность $\{f(x_k)\}_{k\in\mathbb{N}}$, генерируемая алгоритмом, сходится к оптимальному значению $f^*$ со скоростью $\mathcal{O}\left(\frac{1}{k^2}\right)$, в частности:
$$
f(x_k) - f^* \leq \frac{2L \|x_0 - x^*\|^2}{k^2}
$$
:::

## Ускоренная сходимость для сильно выпуклых функций

:::{.callout-theorem}
Предположим, что $f : \mathbb{R}^n \rightarrow \mathbb{R}$ является $\mu$-сильно выпуклой и $L$-гладкой. Ускоренный градиентный метод Нестерова (NAG) предназначен для решения задачи минимизации, начиная с начальной точки $x_0 = y_0 \in \mathbb{R}^n$ и $\lambda_0 = 0$. Алгоритм выполняет следующие шаги:
$$
\begin{aligned}
&\textbf{Обновление градиента: } &x_{k+1} &= y_k - \frac{1}{L} \nabla f(y_k) \\
&\textbf{Экстраполяция: } &y_{k+1} &= x_{k+1} +  \gamma \left(x_{k+1} - x_k\right) \\
&\textbf{Вес экстраполяции: } &\gamma &= \frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}
\end{aligned}
$$
Последовательность $\{f(x_k)\}_{k\in\mathbb{N}}$, генерируемая алгоритмом, сходится к оптимальному значению $f^*$ линейно:
$$
f(x_k) - f^* \leq \frac{\mu + L}{2}\|x_0 - x^*\|^2_2 \exp \left(-\frac{k}{\sqrt{\varkappa}}\right)
$$
:::

# Численные эксперименты

## Выпуклая квадратичная задача (линейная регрессия)

![](agd_random_0_10_60.pdf)

## Сильно выпуклая квадратичная задача (регуляризованная линейная регрессия)

![](agd_random_1_10_60.pdf)

## Сильно выпуклая квадратичная задача (регуляризованная линейная регрессия)

![](agd_random_1_1000_60.pdf)

## Сильно выпуклая квадратичная задача (регуляризованная линейная регрессия)

![](agd_random_1_1000_1000.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.1.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.2.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.3.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.4.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.5.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.6.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.7.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.8.pdf) 

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.9.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.95.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.99.pdf)

## Сильно выпуклая бинарная логистическая регрессия

![](agd_strongly_convex_logreg_0.25.pdf)

## Сильно выпуклая бинарная логистическая регрессия

![](agd_strongly_convex_logreg_0.5.pdf)

## Сильно выпуклая бинарная логистическая регрессия

![](agd_strongly_convex_logreg_0.7.pdf)

## Сильно выпуклая бинарная логистическая регрессия

![](agd_strongly_convex_logreg_0.9.pdf)


# Квадратичная задача оптимизации

## Сильно выпуклая квадратичная функция

:::: {.columns}

::: {.column width="60%"}
Рассмотрим следующую квадратичную задачу оптимизации:
$$
\min\limits_{x \in \mathbb{R}^n} f(x) =  \min\limits_{x \in \mathbb{R}^n} \dfrac{1}{2} x^\top  A x - b^\top  x + c, \text{ где }A \in \mathbb{S}^n_{++}.
$$ {#eq-main_problem}
:::
::: {.column width="40%"}
Условия оптимальности
$$
Ax^* = b
$$
:::
::::
![](SD_vs_CG.pdf)

## Наискорейший спуск aka точный линейный поиск

:::: {.columns}
::: {.column width="80%"}
$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_{k+1}) = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$
Более теоретический, чем практический подход к выбору шага. Он также позволяет анализировать сходимость, но точный линейный поиск может быть численно сложным, если вычисление функции занимает слишком много времени или требует слишком много ресурсов.

Интересное теоретическое свойство этого метода заключается в том, что каждая следующая итерация метода ортогональна предыдущей:
$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$

. . .

Условия оптимальности:

. . .

$$
\nabla f(x_k)^T\nabla f(x_{k+1})  = 0
$$

:::{.callout-caution}

### Оптимальное значение для квадратичных функций

$$
\nabla f(x_k)^\top A (x_k - \alpha \nabla f(x_k)) - \nabla f(x_k)^\top b = 0 \qquad \alpha_k = \frac{\nabla f(x_k)^T \nabla f(x_k)}{\nabla f(x_k)^T A \nabla f(x_k)}
$$
:::
:::
::: {.column width="20%"}

![Наискорейший спуск](GD_vs_Steepest.pdf)

[Открыть в Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Steepest_descent.ipynb)
:::
::::

# Ортогональность

## Сопряженные направления. $A$-ортогональность.

[![](A_orthogonality.pdf){#fig-aorth}](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/CG.ipynb)

## Сопряженные направления. $A$-ортогональность.

Предположим, у нас есть две системы координат и квадратичная функция $f(x) = \frac12 x^T I x$ выглядит так, как на левой части [изображения @fig-aorth], в то время как в других координатах она выглядит как $f(\hat{x}) = \frac12 \hat{x}^T A \hat{x}$, где $A \in \mathbb{S}^n_{++}$.

:::: {.columns}

::: {.column width="50%"}
$$
\frac12 x^T I x
$$
:::
::: {.column width="50%"}
$$
\frac12 \hat{x}^T A \hat{x}
$$
:::
::::
Поскольку $A = Q \Lambda Q^T$:
$$
\uncover<+->{ \frac12 \hat{x}^T A \hat{x} }\uncover<+->{ = \frac12 \hat{x}^T Q \Lambda Q^T \hat{x} }\uncover<+->{  = \frac12 \hat{x}^T Q \Lambda^{\frac12}\Lambda^{\frac12} Q^T \hat{x} }\uncover<+->{ = \frac12 x^T I x} \uncover<+->{\text{ и }  \hat{x} = Q \Lambda^{-\frac12} x}
$$

. . .

:::{.callout-caution}

### $A$-ортогональные векторы

Векторы $x \in \mathbb{R}^n$ и $y \in \mathbb{R}^n$ называются $A$-ортогональными (или $A$-сопряженными), если
$$
x^T A y = 0 \qquad \Leftrightarrow \qquad x \perp_A y 
$$
Когда $A = I$, $A$-ортогональность превращается в ортогональность.
:::

## Процесс Грама-Шмидта

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS1.pdf)

## Процесс Грама-Шмидта {.noframenumbering} 

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS2.pdf)

## Процесс Грама-Шмидта {.noframenumbering}

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS3.pdf)

## Процесс Грама-Шмидта {.noframenumbering}

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS4.pdf)

## Процесс Грама-Шмидта {.noframenumbering}

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.

![Иллюстрация процесса Грама-Шмидта](GS5.pdf)

## Процесс Грама-Шмидта

:::: {.columns}
::: {.column width="20%"}

![](GS5.pdf)

![](Projection.pdf)

:::

::: {.column width="80%"}

**Вход:** $n$ линейно независимых векторов $u_0, \ldots, u_{n-1}$.

. . .

**Выход:** $n$ линейно независимых `попарно ортогональных` векторов $d_0, \ldots, d_{n-1}$.
$$
\begin{aligned}
\uncover<+->{ d_0 &= u_0 \\ }
\uncover<+->{ d_1 &= u_1 - \pi_{d_0}(u_1) \\ }
\uncover<+->{ d_2 &= u_2 - \pi_{d_0}(u_2) - \pi_{d_1}(u_2) \\ }
\uncover<+->{ &\vdots \\ }
\uncover<+->{ d_k &= u_k - \sum\limits_{i=0}^{k-1}\pi_{d_i}(u_k) }
\end{aligned}
$$

. . .

$$
d_k = u_k + \sum\limits_{i=0}^{k-1}\beta_{ik} d_i \qquad \beta_{ik} = - \dfrac{\langle d_i, u_k \rangle}{\langle d_i, d_i \rangle}
$$ {#eq-GS}
:::
::::


# Метод сопряженных направлений (CD)

## Общая идея

* В изотропном случае $A=I$ метод наискорейшего спуска, запущенный из произвольной точки в $n$ ортогональных линейно независимых направлениях, сойдется за $n$ шагов в точных арифметических вычислениях. Мы пытаемся построить аналогичную процедуру в случае $A \neq I$ с использованием концепции $A$-ортогональности.
* Предположим, у нас есть набор из $n$ линейно независимых $A$-ортогональных направлений $d_0, \ldots, d_{n-1}$ (которые будут вычислены с помощью процесса Грама-Шмидта). 
* Мы хотим построить метод, который идет из $x_0$ в $x^*$ для квадратичной задачи с шагами $\alpha_i$, который, фактически, является разложением $x^* - x_0$ в некотором базисе:
    $$
    x^* = x_0 + \sum\limits_{i=0}^{n-1} \alpha_i d_i \qquad x^* - x_0 = \sum\limits_{i=0}^{n-1} \alpha_i d_i
    $$
* Мы докажем, что $\alpha_i$ и $d_i$ могут быть построены очень эффективно с вычислительной точки зрения (метод сопряженных градиентов).

## Идея метода сопряженных направлений (CD)

Таким образом, мы формулируем алгоритм:

<!-- 1. Пусть $k = 0$ и $x_k = x_0$, посчитаем $d_k = d_0 = -\nabla f(x_0)$.
2. С помощью процедуры точного линейного поиска находим оптимальную длину шага. Вычисляем $\alpha$ минимизируя $f(x_k + \alpha_k d_k)$ по формуле
    $$
    \alpha_k = -\frac{d_k^\top (A x_k - b)}{d_k^\top A d_k}
    $$ {#eq-line_search}
3. Выполняем шаг алгоритма:
    $$
    x_{k+1} = x_k + \alpha_k d_k
    $$
4. Обновляем направление: $d_{k+1} = -\nabla f(x_{k+1}) + \beta_k d_k$ в целях сохранения $d_{k+1} \perp_A d_k$, где $\beta_k$ вычисляется по формуле:
    $$
    \beta_k = \frac{\nabla f(x_{k+1})^\top A d_k}{d_k^\top A d_k}.
    $$
5. Повторяем шаги 2-4, пока не построим $n$ направлений, где $n$ - размерность пространства ($x$). -->
*Предположим, что нам заранее известны линейно‑независимые векторы $u_0,\dots,u_{n-1}$.*

1. $d_0 = -\nabla f(x_0)$.
2. С помощью процедуры точного линейного поиска находим оптимальную длину шага. Вычисляем $\alpha$ минимизируя $f(x_k + \alpha_k d_k)$ по формуле
    $$
    \alpha_k = -\frac{d_k^\top (A x_k - b)}{d_k^\top A d_k}
    $$ {#eq-line_search}
3. Выполняем шаг алгоритма:
    $$
    x_{k+1} = x_k + \alpha_k d_k
    $$
    
4. *Обновляем направление: $d_{k+1}$ получаем из $u_{k+1}$ с помощью модифицированной процедуры Грама–Шмидта в скалярном произведении $\langle v,w\rangle_A = v^\top A w$ относительно уже построенных $d_0,\dots,d_k$:*
    $$
    d_{k+1} = u_{k+1} - \sum_{i=0}^{k} \beta_{k+1, i}\ d_i,\quad \beta_{k+1,i} = \frac{u_{k+1}^\top A d_i}{d_i^\top A d_i}
    $$
    *что обеспечивает $d_{k+1}\perp_A d_j$ для всех $j\le k$.*
5. Повторяем шаги 2–4, пока не построим $n$ направлений, где $n$ — размерность пространства ($x$).



## Метод сопряженных направлений (CD)

::: {.callout-theorem}

## Лемма 1. Линейная независимость $A$-ортогональных векторов.

Если множество векторов $d_1, \ldots, d_n$ - попарно $A$-ортогональны (каждая пара векторов $A$-ортогональна), то эти векторы линейно независимы. $A \in \mathbb{S}^n_{++}$.

:::

. . .

**Доказательство**

Покажем, что если $\sum\limits_{i=1}^n\alpha_i d_i = 0$, то все коэффициенты должны быть равны нулю:

. . .

$$
\begin{aligned}
\uncover<+->{ 0 &= \sum\limits_{i=1}^n\alpha_i d_i \\ }
\uncover<+->{ (\text{Умножаем на } d_j^T A) \qquad &= d_j^\top A \left( \sum\limits_{i=1}^n\alpha_i d_i \right) }
\uncover<+->{ =  \sum\limits_{i=1}^n \alpha_i d_j^\top A d_i  \\ }
\uncover<+->{ &=  \alpha_j d_j^\top A d_j  + 0 + \ldots + 0 }
\end{aligned}
$$

. . .

Таким образом, $\alpha_j = 0$, для всех остальных индексов нужно проделать тот же процесс

## Доказательство сходимости

Введем следующие обозначения:

* $r_k = b - Ax_k$ - невязка
* $e_k = x_k - x^*$ - ошибка
* Поскольку $Ax^* = b$, имеем $r_k = b - Ax_k = Ax^* - Ax_k = -A (x_k - x^*)$
    $$
    r_k = -Ae_k.
    $$ {#eq-res_error}
* Также заметим, что поскольку $x_{k+1} = x_0 + \sum\limits_{i=0}^k\alpha_i d_i$, имеем 
    $$
    e_{k+1} = e_0 + \sum\limits_{i=0}^k\alpha_i d_i.
    $$ {#eq-err_decomposition}

## Доказательство сходимости

::: {.callout-theorem}

## Лемма 2. Сходимость метода сопряженных направлений.

Предположим, мы решаем $n$-мерную квадратичную сильно выпуклую задачу оптимизации ([-@eq-main_problem]). Метод сопряженных направлений
$$
x_{k+1} = x_0 + \sum\limits_{i=0}^k\alpha_i d_i
$$
с $\alpha_i = \frac{\langle d_i, r_i \rangle}{\langle d_i, Ad_i \rangle}$ взятым из точного линейного поиска, сходится за не более $n$ шагов алгоритма.
:::

. . .

:::: {.columns}
::: {.column width="33%"}
**Доказательство**
Пусть
$$
e_0 = x_0 - x^* =  \sum\limits_{i=0}^{n-1}\delta_i d_i
$$
Докажем, что $\delta_i = - \alpha_i$:
:::

. . .

::: {.column width="66%"}
Умножаем обе части слева на $d_k^T A$:
    $$
    \begin{gathered}
    \uncover<+->{ d_k^T Ae_0 = \sum\limits_{i=0}^{n-1}\delta_i d_k^T A d_i}\uncover<+->{  = \delta_k d_k^T A d_k \\}
    \uncover<+->{ d_k^T A e_k }\uncover<+->{ = d_k^T A\left(e_0 + \sum\limits_{i=0}^{k-1}\alpha_i d_i \right)}\uncover<+->{  \overset{\perp_A}{=} \delta_k d_k^T A d_k \\}
    \uncover<+->{ \delta_k = \frac{ d_k^T A e_k}{d_k^T A d_k }}\uncover<+->{ = -\frac{ d_k^T r_k}{d_k^T A d_k } }\uncover<+->{  \Leftrightarrow \delta_k = - \alpha_k }
    \end{gathered}
    $$
:::
::::


# Метод сопряженных градиентов (CG)

## Идея метода сопряженных градиентов (CG)

* Это буквально метод сопряженных направлений, в котором мы выбираем специальный набор $d_0, \ldots, d_{n-1}$, позволяющий значительно ускорить процесс Грама-Шмидта.
* Используется процесс Грама-Шмидта с $A$-ортогональностью вместо Евклидовой ортогональности, чтобы получить их из набора начальных векторов.
* На каждой итерации $r_0, \ldots, r_{n-1}$ используются в качестве начальных линейно-независимых векторов для процесса Грама-Шмидта.
* Основная идея заключается в том, что для произвольного метода CD процесс Грама-Шмидта вычислительно дорогой и требует квадратичного числа операций сложения векторов и скалярных произведений $\mathcal{O}\left( n^2\right)$, в то время как в случае CG мы покажем, что сложность этой процедуры может быть уменьшена до линейной $\mathcal{O}\left( n\right)$.

. . .

:::{.callout-caution appearance="simple"}
$$
\text{CG} = \text{CD} + r_0, \ldots, r_{n-1} \text{ как начальные векторы для процесса Грама-Шмидта} + A\text{-ортогональность.}
$$
:::

## Леммы для сходимости

::: {.callout-theorem}

## Лемма 5. Невязки ортогональны друг другу в методе CG

Все невязки в методе CG ортогональны друг другу:
$$
r_i^T r_k = 0 \qquad \forall i \neq k
$$ {#eq-res_orth_cg}

:::

::: {.callout-theorem}

## Лемма 7. Коэффициенты для процесса Грама-Шмидта для CG
В процессе Грама-Шмидта для CG 
$$
    \beta_{ji} = \frac{r_i^\top r_i}{r_{i - 1}^\top r_{i - 1}},\ i = j + 1.
$$
Все остальные коэффициенты равны нулю кроме $i = j$, но этот случай нам неинтересен.
:::

## Метод сопряженных градиентов (CG)

$$
\begin{aligned}
& r_0 := b - A x_0 \\
& \hbox{если } r_{0} \text{ достаточно мал, то вернуть } x_{0} \text{ как результат}\\
& d_0 := r_0 \\
& k := 0 \\
& \text{повторять} \\
& \qquad \alpha_k := \frac{r_k^\mathsf{T} r_k}{d_k^\mathsf{T} A d_k}  \\
& \qquad x_{k+1} := x_k + \alpha_k d_k \\
& \qquad r_{k+1} := r_k - \alpha_k A d_k \\
& \qquad \hbox{если } r_{k+1} \text{ достаточно мал, то выйти из цикла} \\
& \qquad \beta_k := \frac{r_{k+1}^\mathsf{T} r_{k+1}}{r_k^\mathsf{T} r_k} \\
& \qquad d_{k+1} := r_{k+1} + \beta_k d_k \\
& \qquad k := k + 1 \\
& \text{конец повторения} \\
& \text{вернуть } x_{k+1} \text{ как результат}
\end{aligned}
$$

## Закрываем квадратичный вопрос

[![](cg_cgd_gd.pdf)](https://fmin.xyz/docs/visualizations/cg_gd.mp4)

## Сходимость

**Теорема 1.** Если матрица $A$ имеет только $r$ различных собственных значений, то метод сопряженных градиентов сходится за $r$ итераций.

**Теорема 2.** Следующая оценка сходимости выполняется для метода сопряженных градиентов, как для итерационного метода в сильно выпуклой задаче:

$$
\| x_{k} - x^* \|_A \leq 2\left( \dfrac{\sqrt{\kappa(A)} - 1}{\sqrt{\kappa(A)} + 1} \right)^k \|x_0 - x^*\|_A,
$$

где $\|x\|^2_A = x^{\top}Ax$ и $\varkappa(A) = \frac{\lambda_1(A)}{\lambda_n(A)}$ - это число обусловленности матрицы $A$, $\lambda_1(A) \geq ... \geq \lambda_n(A)$ - собственные значения матрицы $A$

**Примечание:** Сравните коэффициент геометрической прогрессии с его аналогом в методе градиентного спуска.

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_random_0.001_100_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_random_10_100_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_random_10_1000_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_clustered_10_1000_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_clustered_10_1000_600.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_uniform spectrum_1_100_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](cg_Hilbert_1_10_60.pdf)

## Численные эксперименты

Посмотрим на видео с экспериментами в VS Code.

# Метод сопряженных градиентов для неквадратичных задач (Non-linear CG)

## Метод сопряженных градиентов для неквадратичных задач (Non-linear CG)

В случае, когда нет аналитического выражения для функции или ее градиента, мы, скорее всего, не сможем решить одномерную задачу минимизации аналитически. 
Поэтому $\alpha_k$ подбирается обычной процедурой линейного поиска. 
Но для выбора $\beta_k$ есть следующий математический трюк:

Для двух итераций справедливо:
$$
x_{k+1} - x_k = c d_k,
$$

. . .

где $c$ - некоторая константа. Тогда для квадратичного случая мы имеем:
$$ 
\nabla f(x_{k+1}) - \nabla f(x_k) = (A x_{k+1} - b) - (A x_k - b) = A(x_{k+1}-x_k) = cA d_k
$$

. . .

Выражая из этого уравнения величину $Ad_k = \dfrac{1}{c} \left( \nabla f(x_{k+1}) - \nabla f(x_k)\right)$, мы избавляемся от `знания` функции в определении $\beta_k$, тогда пункт 4 будет переписан как:
$$
\beta_k = \frac{\nabla f(x_{k+1})^\top (\nabla f(x_{k+1}) - \nabla f(x_k))}{d_k^\top (\nabla f(x_{k+1}) - \nabla f(x_k))}.
$$
Этот метод называется методом Полака-Рибьера.

## Численные эксперименты

$$
f(x) = \frac{\mu}{2} \|x\|_2^2 + \frac1m \sum_{i=1}^m \log (1 + \exp(- y_i \langle a_i, x \rangle)) \to \min_{x \in \mathbb{R}^n}
$$

![](cg_non_linear_1000_300_0_None.pdf)

## Численные эксперименты

$$
f(x) = \frac{\mu}{2} \|x\|_2^2 + \frac1m \sum_{i=1}^m \log (1 + \exp(- y_i \langle a_i, x \rangle)) \to \min_{x \in \mathbb{R}^n}
$$

![](cg_non_linear_1000_300_1_None.pdf)

## Численные эксперименты

$$
f(x) = \frac{\mu}{2} \|x\|_2^2 + \frac1m \sum_{i=1}^m \log (1 + \exp(- y_i \langle a_i, x \rangle)) \to \min_{x \in \mathbb{R}^n}
$$

![](cg_non_linear_1000_300_1_20.pdf)

## Численные эксперименты

$$
f(x) = \frac{\mu}{2} \|x\|_2^2 + \frac1m \sum_{i=1}^m \log (1 + \exp(- y_i \langle a_i, x \rangle)) \to \min_{x \in \mathbb{R}^n}
$$

![](cg_non_linear_1000_300_1_50.pdf)

## Численные эксперименты

$$
f(x) = \frac{\mu}{2} \|x\|_2^2 + \frac1m \sum_{i=1}^m \log (1 + \exp(- y_i \langle a_i, x \rangle)) \to \min_{x \in \mathbb{R}^n}
$$

![](cg_non_linear_1000_300_10_None.pdf)

## Численные эксперименты

$$
f(x) = \frac{\mu}{2} \|x\|_2^2 + \frac1m \sum_{i=1}^m \log (1 + \exp(- y_i \langle a_i, x \rangle)) \to \min_{x \in \mathbb{R}^n}
$$

![](cg_non_linear_1000_300_10_20.pdf)

# Бонус: дополнительные технические леммы и доказательства

## Леммы для сходимости

::: {.callout-theorem}

## Лемма 3. Разложение ошибки.

$$
e_i = \sum\limits_{j=i}^{n-1}-\alpha_j d_j 
$$ {#eq-err_decomposition}

:::

. . .

**Доказательство**

По определению
$$
\uncover<+->{ e_{i} = e_0 + \sum\limits_{j=0}^{i-1}\alpha_j d_j }\uncover<+->{ = x_0 - x^* + \sum\limits_{j=0}^{i-1}\alpha_j d_j }\uncover<+->{  = -\sum\limits_{j=0}^{n-1}\alpha_j d_j + \sum\limits_{j=0}^{i-1}\alpha_j d_j}\uncover<+->{  = \sum\limits_{j=i}^{n-1}-\alpha_j d_j }
$$

## Леммы для сходимости

::: {.callout-theorem}

## Лемма 4. Невязка ортогональна всем предыдущим направлениям для CD.

Рассмотрим невязку метода сопряженных направлений на $k$ итерации $r_k$, тогда для любого $i < k$:

$$
d_i^T r_k = 0
$$ {#eq-res_orth_dir}

:::

. . .

**Доказательство**

:::: {.columns}
::: {.column width="40%"}
Запишем ([-@eq-err_decomposition]) для некоторого фиксированного индекса $k$:

. . .

$$
e_k = \sum\limits_{j=k}^{n-1}-\alpha_j d_j 
$$

. . .

Умножаем обе части на $-d_i^TA \cdot$
$$
-d_i^TA e_k = \sum\limits_{j=k}^{n-1}\alpha_j d_i^TA d_j  = 0
$$
:::

::: {.column width="60%"}
![](CG_lem1.pdf)
Таким образом, $d_i^T r_k = 0$ и невязка $r_k$ ортогональна всем предыдущим направлениям $d_i$ для метода CD.
:::
::::


## Леммы для сходимости

::: {.callout-theorem}

## Лемма 5. Невязки ортогональны друг другу в методе CG

Все невязки в методе CG ортогональны друг другу:
$$
r_i^T r_k = 0 \qquad \forall i \neq k
$$ {#eq-res_orth_cg}

:::

. . .

:::: {.columns}
::: {.column width="40%"}

**Доказательство**

Запишем процесс Грама-Шмидта ([-@eq-GS]) с $\langle \cdot, \cdot \rangle$ замененным на $\langle \cdot, \cdot \rangle_A = x^T A y$

. . .

$$
d_i = u_i + \sum\limits_{j=0}^{i-1}\beta_{ji} d_j \;\; \beta_{ji} = - \dfrac{\langle d_j, u_i \rangle_A}{\langle d_j, d_j \rangle_A}
$$ {#eq-gs_cg1}

. . .

Тогда, мы используем невязки в качестве начальных векторов для процесса и $u_i = r_i$.

. . .

$$ 
d_i = r_i + \sum\limits_{j=0}^{i-1}\beta_{ji} d_j \;\; \beta_{ji} = - \dfrac{\langle d_j, r_i \rangle_A}{\langle d_j, d_j \rangle_A}
$$ {#eq-gs_cg2}
:::

::: {.column width="60%"}
![](CG_lem1.pdf)
Умножаем обе части ([-@eq-gs_cg1]) на $r_k^T \cdot$ для некоторого индекса $k$:
$$
r_k^Td_i = r_k^Tu_i + \sum\limits_{j=0}^{i-1}\beta_{ji} r_k^Td_j 
$$

. . .

Если $j < i < k$, то имеем лемму 4 с $d_i^T r_k = 0$ и $d_j^T r_k = 0$. Имеем:
$$
r_k^Tu_i= 0 \;\text{ для CD} \;\; r_k^Tr_i = 0 \;\text{ для CG}
$$
:::
::::

## Леммы для сходимости

Более того, если $k=i$:
$$
\uncover<+->{ r_k^Td_k = r_k^Tu_k + \sum\limits_{j=0}^{k-1}\beta_{jk} r_k^Td_j}\uncover<+->{  = r_k^Tu_k + 0,}
$$

. . .

и мы имеем для любого $k$ (из-за произвольного выбора $i$):
$$
r_k^Td_k = r_k^Tu_k.
$$ {#eq-lemma5}

. . .

## Леммы для сходимости

::: {.callout-theorem}
## Лемма 6. Пересчет невязки

$$
r_{k+1} = r_k - \alpha_k A d_k 
$$ {#eq-res_recalculation}

:::

. . .

$$
r_{k+1} = -A e_{k+1} = -A \left( e_{k} + \alpha_k d_k \right) = -A e_{k} - \alpha_k A d_k = r_k - \alpha_k A d_k 
$$

Наконец, все эти вышеуказанные леммы достаточны для доказательства, что $\beta_{ji} = 0$ для всех $i,j$, кроме соседних.



## Леммы для сходимости

::: {.callout-theorem}
## Лемма 7. Коэффициенты для процесса Грама-Шмидта для CG
В процессе Грама-Шмидта для CG 
$$
    \beta_{ji} = \frac{\left\langle r_i, r_i \right\rangle}{r_{i - 1}, r_{i - 1}},\ i = j + 1.
$$
Все остальные коэффициенты равны нулю кроме $i = j$, но этот случай нам неинтересен.
:::

Рассмотрим процесс Грам-Шмидта в методе CG
$$
\uncover<+->{ \beta_{ji} = - \dfrac{\langle d_j, u_i \rangle_A}{\langle d_j, d_j \rangle_A} }\uncover<+->{  = - \dfrac{ d_j^T A u_i }{ d_j^T A d_j }}\uncover<+->{  = - \dfrac{ d_j^T A r_i }{ d_j^T A d_j }}\uncover<+->{  = - \dfrac{r_i^T A d_j}{ d_j^T A d_j }.}
$$

. . .

Рассмотрим скалярное произведение $\langle r_i, r_{j+1} \rangle$ используя ([-@eq-res_recalculation]):
$$
\begin{aligned}
\uncover<+->{ \langle r_i, r_{j+1} \rangle}\uncover<+->{  &= \langle r_i, r_j - \alpha_j A d_j  \rangle }\uncover<+->{ = \langle r_i, r_j \rangle - \alpha_j\langle r_i, A d_j  \rangle \\}
\uncover<+->{ \alpha_j\langle r_i, A d_j  \rangle }\uncover<+->{  &= \langle r_i, r_j \rangle - \langle r_i, r_{j+1} \rangle }
\end{aligned}
$$

## Леммы для сходимости

1. Если $i=j$: $\alpha_i\langle r_i, A d_i  \rangle = \langle r_i, r_i \rangle - \langle r_i, r_{i+1} \rangle = \langle r_i, r_i \rangle$. Этот случай не интересен по построению процесса Грам-Шмидта.
2. Соседний случай $i=j + 1$: $\alpha_j\langle r_i, A d_j \rangle = \langle r_i, r_{i-1} \rangle - \langle r_i, r_{i} \rangle = - \langle r_i, r_i \rangle$
3. Для любого другого случая: $\alpha_j\langle r_i, A d_j \rangle = 0$, потому что все невязки ортогональны друг другу.

. . .

Наконец, мы имеем формулу для $i=j + 1$:
$$
\uncover<+->{ \beta_{ji} = - \dfrac{r_i^T A d_j}{ d_j^T A d_j}}\uncover<+->{  = \dfrac{1}{\alpha_j}\dfrac{\langle r_i, r_i \rangle}{ d_j^T A d_j} }\uncover<+->{  =  \dfrac{d_j^T A d_j}{d_j^T r_j}\dfrac{\langle r_i, r_i \rangle}{ d_j^T A d_j} }\uncover<+->{ = \dfrac{\langle r_i, r_i \rangle}{\langle r_j, r_j \rangle} }\uncover<+->{ = \dfrac{\langle r_i, r_i \rangle}{\langle r_{i-1}, r_{i-1} \rangle}}
$$

. . .

И для направления $d_{k+1} = r_{k+1} + \beta_{k,k+1} d_k, \qquad  \beta_{k,k+1} = \beta_k = \dfrac{\langle r_{k+1}, r_{k+1} \rangle}{\langle r_{k}, r_{k} \rangle}.$


## Бонус: нижние оценки для методов I порядка [(\faLink источник)](https://opt-eng-ana.github.io/blog/2025/opt-summary/)

\tiny
| Тип задачи                                   | Критерий            | Нижняя оценка                                   | Верхняя оценка                         | Ссылка (Ниж.)                | Ссылка (Верх.)           |
|:-----------------------------------------------|:------------------------:|:---------------------:|:-------------------:|:------------------|:--------------------|
| $L$-гладкая выпуклая                         | Зазор оптимальности  | $\Omega\!\left(\sqrt{L\,\varepsilon^{-1}}\right)$ | \faCheck (точное совпадение)                  | [1], Теорема 2.1.7          | [1], Теорема 2.2.2      |
| $L$-гладкая $\mu$-сильно выпуклая            | Зазор оптимальности  | $\Omega\!\left(\sqrt{\varkappa}\,\log \tfrac{1}{\varepsilon}\right)$ | \faCheck                                   | [1], Теорема 2.1.13         | [1], Теорема 2.2.2      |
| Негладкая $G$-липшицева выпуклая                    | Зазор оптимальности  | $\Omega\!\left(G^{2}\,\varepsilon^{-2}\right)$  | \faCheck (точное совпадение)                 | [1], Теорема 3.2.1          | [1], Теорема 3.2.2      |
| Негладкая $G$-липшицева $\mu$-сильно выпуклая       | Зазор оптимальности  | $\Omega\!\left(G^{2}\,(\mu\varepsilon)^{-1}\right)$ | \faCheck         | [1], Теорема 3.2.5          | [3], Теорема 3.9         |
| $L$-гладкая выпуклая (сходимость по функции)        | Стационарность      | $\Omega\!\left(\sqrt{\Delta L}\,\varepsilon^{-1}\right)$ | \faCheck (с точностью до логарифмического множителя) | [2], Теорема 1              | [2], Приложение A.1     |
| $L$-гладкая выпуклая (сходимость по аргументу)        | Стационарность      | $\Omega\!\left(\sqrt{D L}\,\varepsilon^{-1/2}\right)$ | \faCheck                                   | [2], Теорема 1              | [6], Раздел 6.5         |
| $L$-гладкая невыпуклая                       | Стационарность      | $\Omega\!\left(\Delta L\,\varepsilon^{-2}\right)$ | \faCheck                                   | [5], Теорема 1              | [7], Теорема 10.15      |
| Негладкая $G$-липшицева $\rho$-слабо выпуклая (WC)  | Квази-стационарность| Неизвестно                                     | $\mathcal{O}\!\left(\varepsilon^{-4}\right)$ | /                       | [8], Следствие 2.2      |
| $L$-гладкая $\mu$-PL                         | Зазор оптимальности  | $\Omega\!\left(\varkappa \log \tfrac{1}{\varepsilon}\right)$ | \faCheck                                  | [9], Теорема 3              | [10], Теорема 1          |

::::{.columns}
:::{.column width="50%"}

:::{.nonincremental}
Источники:

* [1] - Lectures on Convex Optimization, Y. Nesterov.
* [2] - Lower bounds for finding stationary points II: first-order methods, Y. Carmon, J.C. Duchi, O. Hinder, A. Sidford.
* [3] - Convex optimization: Algorithms and complexity, S. Bubeck, others.
* [4] - Optimizing the efficiency of first-order methods for decreasing the gradient of smooth convex functions D. Kim, J.A. Fessler.
* [5] - Lower bounds for finding stationary points I, Y. Carmon, J.C. Duchi, O. Hinder, A. Sidford.
* [6] - Optimizing the efficiency of first-order methods for decreasing the gradient of smooth convex functions, D. Kim, J.A. Fessler.
* [7] - First-order methods in optimization, A. Beck. SIAM. 2017.
* [8] - Stochastic subgradient method converges at the rate $ O (k^{-1/4}) $ on weakly convex functions, D. Davis, D. Drusvyatskiy.
* [9] - On the lower bound of minimizing Polyak-Lojasiewicz functions, P. Yue, C. Fang, Z. Lin.
* [10] - Linear convergence of gradient and proximal-gradient methods under the Polyak-Lojasiewicz condition, H. Karimi, J. Nutini, M. Schmidt.
:::

:::
:::{.column width="50%"}

:::{.nonincremental}

Обозначения:

* Зазор оптимальности: $f(x_k) - f^* \leq \varepsilon$
* Стационарность: $\|\nabla f(x_k)\| \leq \varepsilon$
* Квази-стационарность: $\|\nabla f_\lambda(x_k)\| \leq \varepsilon$, где $f_\lambda(x) = \inf\limits_{y \in \mathbb{R}^n} \left(f(y) + \frac{1}{2\lambda}\|y - x\|^2\right)$
* Липшицевость функции: $|f(x) - f(y)| \leq G \|x - y\| \forall x, y \in \mathbb{R}^n$
* Липшицевость градиента ($L$-гладкость): $\|\nabla f(x) - \nabla f(y)\| \leq L \|x - y\| \forall x, y \in \mathbb{R}^n$
* $\mu$-сильная выпуклость: $f(\lambda x + (1 - \lambda)y) \le \lambda f(x) + (1 - \lambda)f(y) - \frac{\mu}{2} \lambda (1 - \lambda)\|x - y\|^2$
* $\rho$-слабо выпуклая функция: $f(\lambda x + (1 - \lambda)y) \le \lambda f(x) + (1 - \lambda)f(y) + \rho \lambda (1 - \lambda)\|x - y\|^2 \forall x, y \in \mathbb{R}^n$
* Число обусловленности: $\varkappa = \frac{L}{\mu}$
* Зазор в начальной точке: $f(x_0) - f^* \leq \Delta$
* Зазор по аргументу: $D = \|x_0 - x^*\|$
:::

:::

::::
\normalsize


# Бонус: нижние оценки для градиентных методов

## Чёрный ящик

Итерация градиентного спуска:
$$
\begin{aligned}
x_{k+1} &= x_k - \alpha_k \nabla f(x_k) \pause \\ 
&= x_{k-1} - \alpha_{k-1} \nabla f(x_{k-1}) - \alpha_k \nabla f(x_k) \pause \\ 
& \;\;\vdots \pause \\ 
&= x_0 - \sum\limits_{i=0}^k \alpha_{k-i} \nabla f(x_{k-i})
\end{aligned}
$$

. . .

Рассмотрим семейство методов первого порядка, где
$$
\begin{aligned}
x_{k+1} &\in x_0 + \text{Lin} \left\{\nabla f(x_0), \nabla f(x_1), \ldots, \nabla f(x_k)\right\} \; & f \text{ — гладкая} \\
x_{k+1} &\in x_0 + \text{Lin} \left\{g_{0}, g_{1}, \ldots, g_{k}\right\} \text{, где }
g_{i} \in \partial f(x_{i}) \; & f \text{ — негладкая}
\end{aligned}
$$ {#eq-fom}

. . .

Чтобы построить нижнюю оценку, нам нужно найти функцию $f$ из соответствующего класса, такую, что любой метод из семейства (-@eq-fom) будет работать не быстрее этой нижней оценки.


## Гладкий случай

:::{.callout-theorem}
Существует $L$-гладкая и выпуклая функция $f$, такая, что любой метод (-@eq-fom) для всех $k$, $1 \leq k \leq \frac{n-1}{2}$, удовлетворяет:
$$
f(x_k) - f^* \geq \frac{3L \|x_0 - x^*\|_2^2}{32(k+1)^2}
$$
:::

. . .

* Какой бы метод из семейства методов первого порядка вы ни использовали, найдётся функция $f$, на которой скорость сходимости не лучше $\mathcal{O}\left(\frac{1}{k^2}\right)$.
* Ключом к доказательству является явное построение специальной функции $f$.
* Обратите внимание, что эта граница $\mathcal{O}\left(\frac{1}{k^2}\right)$ не соответствует скорости градиентного спуска $\mathcal{O}\left(\frac{1}{k}\right)$. Два возможных варианта:
    a. Нижняя оценка не является точной.
    b. \textbf<7>{Метод градиентного спуска не является оптимальным для этой задачи.}


## Наихудшая функция Нестерова

:::: {.columns}

::: {.column width="50%"}
* Пусть $n=2k+1$ и $A \in \mathbb{R}^{n \times n}$.
    $$
    A = \begin{bmatrix}
        2 & -1 & 0 & 0 & \cdots & 0 \\
        -1 & 2 & -1 & 0 & \cdots & 0 \\
        0 & -1 & 2 & -1  & \cdots & 0 \\
        0 & 0 & -1 & 2  & \cdots & 0 \\
        \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & 0 & \cdots & 2  \\
    \end{bmatrix}
    $$
* Обратите внимание, что
    $$
    x^T A x = x_1^2 + x_n^2 + \sum_{i=1}^{n-1} (x_i - x_{i+1})^2,
    $$
    Следовательно, $x^T A x \geq 0$. Также легко увидеть, что $0 \preceq A \preceq 4I$.
:::

. . .

::: {.column width="50%"}
Пример, когда $n=3$:
$$
A = \begin{bmatrix}
    2 & -1 & 0 \\
    -1 & 2 & -1 \\
    0 & -1 & 2 \\
    \end{bmatrix}
$$

. . .

Нижняя оценка:
$$
\begin{aligned}
x^T A x &= 2x_1^2 + 2x_2^2 + 2x_3^2 - 2x_1x_2 - 2x_2x_3 \\
&= x_1^2 + x_1^2 - 2x_1x_2 + x_2^2 + x_2^2 - 2x_2x_3 + x_3^2 + x_3^2 \\
&= x_1^2 + (x_1 - x_2)^2 + (x_2 - x_3)^2 + x_3^2 \geq 0
\end{aligned}
$$

. . .

Верхняя оценка
$$
\begin{aligned}
x^T A x &= 2x_1^2 + 2x_2^2 + 2x_3^2 - 2x_1x_2 - 2x_2x_3 \pause\\
& \leq 4(x_1^2 + x_2^2 + x_3^2) \pause\\
0 &\leq 2x_1^2 + 2x_2^2 + 2x_3^2 + 2x_1x_2 + 2x_2x_3 \pause\\
0 &\leq x_1^2 + x_1^2 + 2x_1x_2 + x_2^2 + x_2^2 + 2x_2x_3 + x_3^2 + x_3^2 \pause\\
0 &\leq x_1^2 + (x_1 + x_2)^2 + (x_2 + x_3)^2 + x_3^2
\end{aligned}
$$

:::
::::


## Наихудшая функция Нестерова
* Определим следующую $L$-гладкую выпуклую функцию: $f(x) = \frac{L}{4}\left(\frac{1}{2} x^T A x - e_1^T x \right) = \frac{L}{8} x^T A x - \frac{L}{4} e_1^T x.$

* Оптимальное решение $x^*$ удовлетворяет $Ax^* = e_1$, и решение этой системы уравнений дает:
    $$
    \begin{bmatrix}
        2 & -1 & 0 & 0 & \cdots & 0 \\
        -1 & 2 & -1 & 0 & \cdots & 0 \\
        0 & -1 & 2 & -1  & \cdots & 0 \\
        0 & 0 & -1 & 2  & \cdots & 0 \\
        \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & 0 & \cdots & 2  \\
    \end{bmatrix} \begin{bmatrix}
        x_1^* \\
        x_2^* \\
        x_3^* \\
        \vdots \\
        x_{n}^* \\
    \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} \quad \begin{cases} 2x_1^* - x_2^* = 1 \\ -x_{i-1}^* + 2x_{i}^* - x_{i+1}^* = 0, \; i = 2, \ldots, n-1 \\ -x_{n-1}^* + 2x_n^* = 0 \end{cases}
    $$
* Гипотеза: $x_i^* = a+bi$ (вдохновлённая физикой). Проверьте, что выполнено второе уравнение, в то время как $a$ и $b$ вычисляются из первого и последнего уравнений.
* Решение:
    $$
    x^*_i = 1 - \frac{i}{n+1},
    $$
* И значение функции равно
    $$
    f(x^*) =  \frac{L}{8} {x^*}^T A x^* - \frac{L}{4}\langle x^*, e_1 \rangle = -\frac{L}{8} \langle x^*, e_1 \rangle = -\frac{L}{8} \left(1 - \frac{1}{n+1}\right).
    $$


## Гладкий случай (доказательство)

:::: {.columns}

::: {.column width="45%"}
* Предположим, что мы начинаем с $x_0 = 0$. Запросив у оракула градиент, мы получаем $g_0 = -\tfrac{L}{4}e_1$. Тогда, $x_1$ должен лежать на линии, генерируемой $e_1$. В этой точке все компоненты $x_1$ равны нулю, кроме первой, поэтому
    $$
    x_1 = \begin{bmatrix} \bullet \\ 0 \\ \vdots \\ 0 \end{bmatrix}.
    $$
* На второй итерации оракул возвращает градиент $g_1 = \tfrac{L}{4}\left(Ax_1 - e_1\right)$. Тогда, $x_2$ должен лежать на линии, генерируемой $e_1$ и $Ax_1 - e_1$. Все компоненты $x_2$ равны нулю, кроме первых двух, поэтому
    $$
    \begin{bmatrix}
        2 & -1 & 0  & \cdots & 0 \\
        -1 & 2 & -1 & \cdots & 0 \\
        0 & -1 & 2 & \cdots & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \cdots & 2  \\
    \end{bmatrix} \begin{bmatrix} \bullet \\ 0 \\ \vdots \\ 0 \end{bmatrix} \Rightarrow x_2 = \begin{bmatrix} \bullet \\ \bullet \\ 0 \\ \vdots \\ 0 \end{bmatrix}.
    $$
:::

::: {.column width="55%"}
* Из-за структуры матрицы $A$ можно показать, что после $k$ итераций все последние $n-k$ компоненты $x_k$ равны нулю.
    $$
    x_{k} =
    \begin{bmatrix} 
        \bullet \\ 
        \bullet \\ 
        \vdots \\ 
        \bullet \\ 
        0 \\ 
        \vdots \\ 
        0 
    \end{bmatrix}
    \begin{array}{l}
        1 \\ 
        2 \\ 
        \vdots \\ 
        k \\ 
        k+1 \\ 
        \vdots \\ 
        n 
    \end{array}
    $$
* Однако, поскольку каждая итерация $x_k$, произведенная нашим методом, лежит в $S_k = \text{Lin}\{e_1, e_2, \ldots, e_{k}\}$ (т.е. имеет нули в координатах $k+1,\dots,n$), она не может "достичь" полного оптимального вектора $x^*$. Другими словами, даже если бы мы выбрали лучший возможный вектор из $S_k$, обозначаемый
$$
\tilde{x}_k=\arg\min_{x\in S_k} f(x),
$$
значение функции в нём $f(\tilde{x}_k)$ будет выше, чем $f(x^*)$.
:::
::::

## Гладкий случай (доказательство)

* Поскольку $x_k\in S_k = \text{Lin}\{e_1, e_2, \ldots, e_{k}\}$ и $\tilde{x}_k$ является лучшим возможным приближением к $x^*$ в $S_k$, мы имеем
    $$
    f(x_k)\ge f(\tilde{x}_k).
    $$
* Следовательно, 
    $$
    f(x_k)-f(x^*)\ge f(\tilde{x}_k)-f(x^*).
    $$
* Аналогично, для оптимума исходной функции, мы имеем $\tilde{x}_{k_{(i)}} = 1 - \frac{i}{k+1}$ и $f(\tilde{x}_k) = -\frac{L}{8} \left(1 - \frac{1}{k+1}\right)$.
* Теперь мы имеем:
    $$
    \begin{aligned}
    f(x_k)-f(x^*) &\ge f(\tilde{x}_k)-f(x^*) \\
    \uncover<+->{&= -\frac{L}{8} \left(1 - \frac{1}{k+1}\right) - \left(-\frac{L}{8} \left(1 - \frac{1}{n+1}\right)\right) \\}
    \uncover<+->{&= \frac{L}{8} \left(\frac{1}{k+1} - \frac{1}{n+1}\right) = \frac{L}{8} \left(\frac{n-k}{(k+1)(n+1)}\right) \\}
    \uncover<+->{&\overset{n = 2k+1}{=} \frac{L }{16(k+1)}}
    \end{aligned}
    $$ {#eq-lb1}

## Гладкий случай (доказательство)

:::: {.columns}

::: {.column width="70%"}

* Теперь мы ограничиваем $R = \|x_0 - x^*\|_2$:
    $$
    \begin{aligned}
    \|x_0 - x^*\|_2^2 &= \|0 - x^*\|_2^2 = \|x^*\|_2^2 = \sum_{i=1}^n \left( 1 - \frac{i}{n+1} \right)^2 \\
    \uncover<+->{&= n - \frac{2}{n+1} \sum_{i=1}^{n} i + \frac{1}{(n+1)^2} \sum_{i=1}^{n} i^2 \\}
    \uncover<+->{&\leq n - \frac{2}{n+1} \cdot \frac{n(n+1)}{2} + \frac{1}{(n+1)^2} \cdot \frac{(n+1)^3}{3} \\}
    \uncover<+->{&= \frac{n+1}{3} \overset{n = 2k+1}{=} \frac{2(k+1)}{3}.}
    \end{aligned}
    $$

* Следовательно, 
    $$
    k+1 \geq \frac{3}{2}\|x_0 - x^*\|_2^2 = \frac32 R^2
    $${#eq-lb2}
:::

. . .

::: {.column width="30%"}
Заметим, что
$$
\begin{aligned}
\sum_{i=1}^{n} i &= \frac{n(n+1)}{2} \\
\sum_{i=1}^{n} i^2 &= \frac{n(n+1)(2n+1)}{6} \\
&\leq \frac{(n+1)^3}{3}
\end{aligned}
$$
:::

::::

## Гладкий случай (доказательство)

Наконец, используя ([-@eq-lb1]) и ([-@eq-lb2]), мы получаем:
$$
\begin{aligned}
f(x_k) - f(x^*) &\geq \frac{L}{16(k+1)}  = \frac{L (k+1)}{16(k+1)^2} \pause \\
&\geq \frac{L}{16(k+1)^2} \frac{3}{2} R^2  \pause \\
&= \frac{3L R^2}{32 (k+1)^2}
\end{aligned}
$$

. . .

Это завершает доказательство с желаемой скоростью $\mathcal{O}\left( \frac{1}{k^2}\right)$.

## Нижние оценки для гладкого случая

:::{.callout-theorem}

### Гладкий выпуклый случай
Существует $L$-гладкая выпуклая функция $f$, такая, что любой [метод в форме @eq-fom] для всех $k$, $1 \leq k \leq \frac{n-1}{2}$, удовлетворяет:
$$
f(x_k) - f^* \geq \frac{3L \|x_0 - x^*\|_2^2}{32(k+1)^2}
$$
:::

:::{.callout-theorem}

### Гладкий сильно выпуклый случай

Для любого $x_0$ и любого $\mu > 0$, $\varkappa = \frac{L}{\mu} > 1$, существует $L$-гладкая и $\mu$-сильно выпуклая функция $f$, такая, что для любого метода [в форме @eq-fom] выполняются неравенства:

$$
\begin{aligned}
\|x_k - x^*\|_2 &\geq \left( \frac{\sqrt{\varkappa}-1}{\sqrt{\varkappa}+1}\right)^{k} \|x_0 - x^*\|_2 \\
f(x_k) - f^* &\geq \frac{\mu}{2} \left( \frac{\sqrt{\varkappa}-1}{\sqrt{\varkappa}+1}\right)^{2k} \|x_0 - x^*\|_2^2 
\end{aligned}
$$
:::


# Бонус: ускорение для квадратичных функций

## Результат сходимости для квадратичных функций

Предположим, что мы решаем задачу минимизации сильно выпуклой квадратичной функции, с помощью метода градиентного спуска:
$$
f(x) = \frac{1}{2} x^T A x - b^T x \qquad x_{k+1} = x_k - \alpha_k \nabla f(x_k).
$$

. . .

:::{.callout-theorem}
Градиентный спуск с шагом $\alpha_k = \frac{2}{\mu + L}$ сходится к оптимальному решению $x^*$ со следующей гарантией:
$$
\|x_{k+1} - x^*\|_2 \le \left( \frac{\varkappa-1}{\varkappa+1}\right)^k \|x_0 - x^*\|_2 \qquad f(x_{k+1}) - f(x^*) \le \left( \frac{\varkappa-1}{\varkappa+1}\right)^{2k} \left(f(x_0) - f(x^*)\right)
$$
где $\varkappa = \frac{L}{\mu}$ является числом обусловленности $A$.
:::

## Число обусловленности $\varkappa$

[![](condition_number_gd.pdf)](https://fmin.xyz/docs/visualizations/condition_number_gd.mp4)

## Ускорение из первых принципов

$$
f(x) = \frac{1}{2} x^T A x - b^T x \qquad x_{k+1} = x_k - \alpha_k \nabla f(x_k).
$$

Пусть $x^*$ будет единственным решением системы линейных уравнений $Ax=b$ и пусть $e_k = x_k - x^*$, где $x_{k+1}=x_k - \alpha_k (Ax_k-b)$ определяется рекурсивно, начиная с некоторого $x_0$, а $\alpha_k$ — шаг, который мы определим позже.
$$
e_{k+1} = (I-\alpha_k A)e_k.
$$

. . .

### Полиномы

:::: {.columns}
::: {.column width="50%"}

Вышеуказанный расчет дает нам $e_k = p_k(A)e_0,$

где $p_k$ является полиномом
$$
p_k(a) = \prod_{i=1}^k (1-\alpha_ia).
$$

. . .

Мы можем ограничить норму ошибки как
$$
\|e_k\|\le \|p_k(A)\|\cdot\|e_0\|\,.
$$
:::

. . .

::: {.column width="50%"}
Поскольку $A$ является симметричной матрицей с собственными значениями в $[\mu,L],$:
$$
\|p_k(A)\|\le \max_{\mu\le a\le L} \left|p_k(a)\right|\,.
$$
Это приводит к интересной постановке задачи: среди всех полиномов, удовлетворяющих $p_k(0)=1$, мы ищем полином, значение которого как можно меньше отклоняется от нуля на интервале $[\mu,L]$.
:::
::::

## Наивное полиномиальное решение

:::: {.columns}
::: {.column width="50%"}

Наивное решение состоит в том, чтобы выбрать равномерный шаг $\alpha_k=\frac{2}{\mu+L}$. Благодаря этому $|p_k(\mu)| = |p_k(L)|$.
$$
\|e_k\|\le \left(\frac{\varkappa-1}{\varkappa+1}\right)^k\|e_0\|
$$
Это точно та же скорость, которую мы доказали в предыдущей лекции для любой гладкой и сильно выпуклой функции.

Давайте посмотрим на этот полином поближе. На правом рисунке мы выбираем $\mu=1$ и $L=10$ так, что $\varkappa=10$. Следовательно, соответствующий интервал равен $[1,10]$.

Можем ли мы сделать лучше? Ответ — да.
:::

::: {.column width="50%"}
\includegraphics<1>[width=\columnwidth]{gd_polynom_2_ru.pdf}
\includegraphics<2>[width=\columnwidth]{gd_polynom_3_ru.pdf}
\includegraphics<3>[width=\columnwidth]{gd_polynom_4_ru.pdf}
\includegraphics<4>[width=\columnwidth]{gd_polynom_5_ru.pdf}
\includegraphics<5>[width=\columnwidth]{gd_polynom_6_ru.pdf}
:::
::::

## Полиномы Чебышева


:::: {.columns}
::: {.column width="50%"}

Полиномы Чебышёва дают оптимальный ответ на поставленный вопрос. При соответствующем шкалировании они минимизируют абсолютное значение на заданном интервале $[\mu,L]$  , одновременно удовлетворяя нормировочному условию $p(0)=1$.
$$
\begin{aligned}
T_0(x) &= 1\\
T_1(x) &= x\\
T_k(x) &=2xT_{k-1}(x)-T_{k-2}(x),\qquad k\ge 2.\\
\end{aligned}
$$

Давайте построим стандартные полиномы Чебышёва (без масштабирования):
:::

::: {.column width="50%"}
\includegraphics<1>[width=\columnwidth]{gd_polynom_cheb_1_ru.pdf}
\includegraphics<2>[width=\columnwidth]{gd_polynom_cheb_2_ru.pdf}
\includegraphics<3>[width=\columnwidth]{gd_polynom_cheb_3_ru.pdf}
\includegraphics<4>[width=\columnwidth]{gd_polynom_cheb_4_ru.pdf}
\includegraphics<5>[width=\columnwidth]{gd_polynom_cheb_5_ru.pdf}

:::
::::

## Отшкалированные полиномы Чебышёва

Оригинальные полиномы Чебышёва определены на интервале $[-1,1]$. Чтобы использовать их для наших целей, мы должны отшкалировать их на интервал $[\mu,L]$. 

. . .

:::: {.columns} 
::: {.column width="50%"}

Мы будем использовать следующее аффинное преобразование:
$$
x = \frac{L + \mu - 2a}{L - \mu}, \quad a \in [\mu,L], \quad x \in [-1,1]. 
$$
:::

::: {.column width="50%"}
Обратите внимание, что $x=1$ соответствует $a=\mu$, $x=-1$ соответствует $a=L$ и $x=0$ соответствует $a=\frac{\mu+L}{2}$. Это преобразование гарантирует, что поведение полинома Чебышёва на интервале $[-1,1]$ транслируется на интервал $[\mu, L]$.
:::

::::

. . .

В нашем анализе ошибок мы требуем, чтобы полином был равен 1 в 0 (т.е. $p_k(0)=1$). После применения преобразования значение $T_k$ в точке, соответствующей $a=0$, может не быть 1. Следовательно, мы умножаем на обратную величину $T_k$ в точке
$$
\frac{L+\mu}{L-\mu}, \qquad \text{что обеспечивает} \qquad P_k(0)= T_k\left(\frac{L+\mu-0}{L-\mu}\right) \cdot T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1} = 1.
$$

. . .

Построим отшкалированные полиномы Чебышёва
$$
P_k(a) = T_k\left(\frac{L+\mu-2a}{L-\mu}\right) \cdot T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1}
$$
и увидим, что они больше подходят для нашей задачи, чем наивные полиномы на интервале $[\mu,L]$.

## Отшкалированные полиномы Чебышёва

\includegraphics<1>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_1_ru.pdf}
\includegraphics<2>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_2_ru.pdf}
\includegraphics<3>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_3_ru.pdf}
\includegraphics<4>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_4_ru.pdf}
\includegraphics<5>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_5_ru.pdf}
\includegraphics<6>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_6_ru.pdf}
\includegraphics<7>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_7_ru.pdf}
\includegraphics<8>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_8_ru.pdf}
\includegraphics<9>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_9_ru.pdf}
\includegraphics<10>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_10_ru.pdf}

## Верхняя оценка для полиномов Чебышёва

Мы можем видеть, что максимальное значение полинома Чебышёва на интервале $[\mu,L]$ достигается на концах отрезка в точках $a=\mu$ и $a=L$. Следовательно, мы можем использовать следующую верхнюю оценку:
$$
\|P_k(A)\|_2 \le P_k(\mu) = T_k\left(\frac{L+\mu-2\mu}{L-\mu}\right) \cdot T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1} = T_k\left(1\right) \cdot T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1} = T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1}
$$

. . .

Используя определение числа обусловленности $\varkappa = \frac{L}{\mu}$, мы получаем:
$$
\|P_k(A)\|_2 \le T_k\left(\frac{\varkappa+1}{\varkappa-1}\right)^{-1} = T_k\left(1 + \frac{2}{\varkappa-1}\right)^{-1} = T_k\left(1 + \epsilon\right)^{-1}, \quad \epsilon = \frac{2}{\varkappa-1}.
$$

. . .

Именно в этот момент явно возникнет ускорение. Мы ограничим значение $\|P_k(A)\|_2$ сверху величиной $\left(\frac{1}{1 + \sqrt{\epsilon}}\right)^k$. Для этого детально изучим величину $|T_k(1 + \epsilon)|$.

## Верхняя оценка для полиномов Чебышёва

Чтобы ограничить $|P_k|$ сверху, мы должны ограничить $|T_k(1 + \epsilon)|$ снизу.

. . .

:::: {.columns}
::: {.column width="50%"}

1. Для любого $x\ge 1$, полиномы Чебышёва первого рода могут быть записаны как
   $$
   \begin{aligned}
   T_k(x)&=\cosh\left(k\,\mathrm{arccosh}(x)\right)\\
   T_k(1+\epsilon)&=\cosh\left(k\,\mathrm{arccosh}(1+\epsilon)\right).
   \end{aligned}
   $$

2. Помните, что:
    $$
    \cosh(x)=\frac{e^x+e^{-x}}{2} \quad \mathrm{arccosh}(x) = \ln(x + \sqrt{x^2-1}).
    $$

3. Теперь, пусть $\phi=\mathrm{arccosh}(1+\epsilon)$,
    $$
    e^{\phi}=1+\epsilon + \sqrt{2\epsilon+\epsilon^2} \geq 1+\sqrt{\epsilon}.
    $$
:::

::: {.column width="50%"}

4. Следовательно,
    $$
    \begin{aligned}
    T_k(1+\epsilon)&=\cosh\left(k\,\mathrm{arccosh}(1+\epsilon)\right) \\
    &= \cosh\left(k\phi\right) \\
    &= \frac{e^{k\phi} + e^{-k\phi}}{2} \geq\frac{e^{k\phi}}{2} \\
    &= \frac{\left(1+\sqrt{\epsilon}\right)^k}{2}.
    \end{aligned}
    $$

5. Наконец, мы получаем:
    $$
    \begin{aligned}
    \|e_k\| &\leq \|P_k(A)\| \|e_0\| \leq \frac{2}{\left(1 + \sqrt{\epsilon}\right)^k} \|e_0\| \\ 
    &\leq 2 \left(1 + \sqrt{\frac{2}{\varkappa-1}}\right)^{-k} \|e_0\| \\
    &\leq 2 \exp\left( - \sqrt{\frac{2}{\varkappa-1}} k\right) \|e_0\|
    \end{aligned}
    $$
    
:::
::::    

## Ускоренный метод [1/2]

Из-за рекурсивного определения полиномов Чебышёва мы непосредственно получаем итерационную схему ускоренного алгоритма. Переформулируя рекурсию в терминах наших  отшкалированных полиномов Чебышёва, мы получаем:
$$
T_{k+1}(x) =2xT_{k}(x)-T_{k-1}(x)
$$

. . .

Принимая во внимание, что $x = \frac{L+\mu-2a}{L-\mu}$, и:

:::: {.columns}
::: {.column width="50%"}
$$
\begin{aligned}
P_k(a) &= T_k\left(\frac{L+\mu-2a}{L-\mu}\right) T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1} \pause \\
T_k\left(\frac{L+\mu-2a}{L-\mu}\right) &= P_k(a) T_k\left(\frac{L+\mu}{L-\mu}\right) 
\end{aligned}
$$
:::

. . .

::: {.column width="50%"}
$$
\begin{aligned}
T_{k-1}\left(\frac{L+\mu-2a}{L-\mu}\right) &= P_{k-1}(a) T_{k-1}\left(\frac{L+\mu}{L-\mu}\right) \\
T_{k+1}\left(\frac{L+\mu-2a}{L-\mu}\right) &= P_{k+1}(a) T_{k+1}\left(\frac{L+\mu}{L-\mu}\right)
\end{aligned}
$$
:::
::::
\pause
$$
\begin{aligned}
P_{k+1}(a) t_{k+1} &= 2 \frac{L+\mu-2a}{L-\mu} P_{k}(a) t_{k} - P_{k-1}(a) t_{k-1} \text{, где } t_{k} = T_{k}\left(\frac{L+\mu}{L-\mu}\right) \pause\\
P_{k+1}(a) &= 2 \frac{L+\mu-2a}{L-\mu} P_{k}(a) \frac{t_{k}}{t_{k+1}} - P_{k-1}(a) \frac{t_{k-1}}{t_{k+1}}
\end{aligned}
$$

. . .

Поскольку мы имеем $P_{k+1}(0) = P_{k}(0) = P_{k-1}(0) = 1$, получаем рекуррентную формулу вида:
$$
P_{k+1}(a) = (1 - \alpha_k a) P_k(a) + \beta_k \left(P_{k}(a) - P_{k-1}(a) \right).
$$

## Ускоренный метод [2/2]

:::: {.columns}
::: {.column width="50%"}
Перегруппируя члены, мы получаем:
$$
\begin{aligned}
P_{k+1}(a) &= (1 + \beta_k) P_k(a) - \alpha_k a P_k(a) - \beta_k P_{k-1}(a), \pause \\
P_{k+1}(a) &= 2 \frac{L+\mu}{L-\mu}  \frac{t_{k}}{t_{k+1}} P_{k}(a) - \frac{4a}{L-\mu}  \frac{t_{k}}{t_{k+1}}P_{k}(a) - \frac{t_{k-1}}{t_{k+1}} P_{k-1}(a)
\end{aligned}
$$
:::

. . .

::: {.column width="50%"}
$$
\begin{cases}
\beta_k = \dfrac{t_{k-1}}{t_{k+1}}, \\[6pt]
\alpha_k = \dfrac{4}{L-\mu} \dfrac{t_k}{t_{k+1}}, \\[6pt]
1 + \beta_k = 2 \dfrac{L + \mu}{L - \mu} \dfrac{t_k}{t_{k+1}}
\end{cases}
$$


:::
::::

. . .

Мы почти закончили :) Помним, что $e_{k+1} = P_{k+1}(A) e_0$. Также обратим внимание, что мы работаем с квадратичной задачей, поэтому мы можем предположить $x^* = 0$ без ограничения общности. В этом случае $e_0 = x_0$ и $e_{k+1} = x_{k+1}$. \pause
$$
\begin{aligned}
x_{k+1} &= P_{k+1}(A) x_0 \pause =  (I - \alpha_k A) P_k(A) x_0 + \beta_k \left(P_{k}(A) - P_{k-1}(A) \right) x_0 \pause \\
&= (I - \alpha_k A) x_k + \beta_k \left(x_k - x_{k-1}\right)
\end{aligned}
$$

. . .

Для квадратичной задачи мы имеем $\nabla f(x_k) = A x_k$, поэтому мы можем переписать обновление как:
$$
\boxed{
x_{k+1} = x_k - \alpha_k \nabla f(x_k) + \beta_k \left(x_k - x_{k-1}\right)
}
$$

## Ускорение из первых принципов

[![](chebyshev_gd.pdf)](https://fmin.xyz/docs/visualizations/chebyshev_gd.mp4)


# Бонус: анализ сходимости метода тяжёлого шарика

## Метод тяжёлого шарика Поляка

:::: {.columns}

::: {.column width="25%"}
![](GD_HB.pdf)
:::

::: {.column width="75%"}
Давайте представим идею моментума (импульса, тяжёлого шарика), предложенную Б.Т. Поляком в 1964 году. Обновление метода тяжёлого шарика имеет вид

$$
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1}).
$$

. . .

В нашем (квадратичном) случае это
$$
\hat{x}_{k+1} = \hat{x}_k - \alpha \Lambda \hat{x}_k + \beta (\hat{x}_k - \hat{x}_{k-1}) = (I - \alpha \Lambda + \beta I) \hat{x}_k - \beta \hat{x}_{k-1}
$$

. . .

Это можно переписать как

$$
\begin{split}
&\hat{x}_{k+1} = (I - \alpha \Lambda + \beta I) \hat{x}_k - \beta \hat{x}_{k-1}, \\
&\hat{x}_{k} = \hat{x}_k.
\end{split}
$$

. . .

Давайте введем следующее обозначение: $\hat{z}_k = \begin{bmatrix} 
\hat{x}_{k+1} \\
\hat{x}_{k}
\end{bmatrix}$. Следовательно, $\hat{z}_{k+1} = M \hat{z}_k$, где матрица итерации $M$ имеет вид:

. . .

$$
M = \begin{bmatrix} 
I - \alpha \Lambda + \beta I & - \beta I \\
I & 0_{d}
\end{bmatrix}.
$$

:::
::::

## Сведение к скалярному случаю

Обратим внимание, что $M$ является матрицей $2d \times 2d$ с четырьмя блочно-диагональными матрицами размера $d \times d$ внутри. Это означает, что мы можем изменить порядок координат, чтобы сделать $M$ блочно-диагональной. Обратите внимание, что в уравнении ниже матрица $M$ обозначает то же самое, что и в обозначении выше, за исключением описанной перестановки строк и столбцов. Мы используем эту небольшую перегрузку обозначений для простоты. 

. . .

:::: {.columns}

::: {.column width="40%"}

![Иллюстрация перестановки матрицы $M$](Rearranging_squares.pdf)

:::
:::{.column width="60%"}
$$
\begin{aligned}
\begin{bmatrix} 
\hat{x}_{k}^{(1)} \\
\vdots \\
\hat{x}_{k}^{(d)} \\
\addlinespace 
\hat{x}_{k-1}^{(1)} \\
\vdots \\
\hat{x}_{k-1}^{(d)}
\end{bmatrix} \to 
\begin{bmatrix} 
\hat{x}_{k}^{(1)} \\
\addlinespace 
\hat{x}_{k-1}^{(1)} \\
\vdots \\
\hat{x}_{k}^{(d)} \\
\addlinespace 
\hat{x}_{k-1}^{(d)}
\end{bmatrix} \quad M = \begin{bmatrix}
M_1\\
&M_2\\
&&\ldots\\
&&&M_d
\end{bmatrix}
\end{aligned}
$$
:::
::::
где $\hat{x}_{k}^{(i)}$ является $i$-й координатой вектора $\hat{x}_{k} \in \mathbb{R}^d$ и $M_i$ обозначает $2 \times 2$ матрицу. Переупорядочение позволяет нам исследовать динамику метода независимо от размерности. Асимптотическая скорость сходимости $2d$-мерной последовательности векторов $\hat{z}_k$ определяется наихудшей скоростью сходимости среди его блока координат. Следовательно, достаточно исследовать оптимизацию в одномерном случае.

## Сведение к скалярному случаю

Для $i$-й координаты, где $\lambda_i$ — $i$-е собственное значение матрицы $A$, имеем: 

$$
M_i = \begin{bmatrix} 
1 - \alpha \lambda_i + \beta & -\beta \\
1 & 0
\end{bmatrix}.
$$

. . .

Метод будет сходиться, если $\rho(M) < 1$, и оптимальные параметры могут быть вычислены путем оптимизации спектрального радиуса
$$
\alpha^*, \beta^* = \arg \min_{\alpha, \beta} \max_{i} \rho(M_i), \quad \alpha^* = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \quad \beta^* = \left(\dfrac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}\right)^2.
$$

. . .

Можно показать, что для таких параметров матрица $M$ имеет комплексные собственные значения, которые образуют комплексно-сопряжённую пару, поэтому расстояние до оптимума (в этом случае $\| z_k \|$) обычно не убывает монотонно. 

## Сходимость метода тяжёлого шарика для квадратичной функции

Мы можем явно вычислить собственные значения $M_i$:

$$
\lambda^M_1, \lambda^M_2 = \lambda \left( \begin{bmatrix} 
1 - \alpha \lambda_i + \beta & -\beta \\
1 & 0
\end{bmatrix}\right) = \dfrac{1+\beta - \alpha \lambda_i \pm \sqrt{(1+\beta - \alpha\lambda_i)^2 - 4\beta}}{2}.
$$

. . .

Когда $\alpha$ и $\beta$ оптимальны ($\alpha^*, \beta^*$), собственные значения являются комплексно-сопряженной парой $(1+\beta - \alpha\lambda_i)^2 - 4\beta \leq 0$, т.е. $\beta \geq (1 - \sqrt{\alpha \lambda_i})^2$.

. . .

$$
\text{Re}(\lambda^M) = \dfrac{L + \mu - 2\lambda_i}{(\sqrt{L} + \sqrt{\mu})^2}, \quad \text{Im}(\lambda^M) = \dfrac{\pm 2\sqrt{(L - \lambda_i)(\lambda_i - \mu)}}{(\sqrt{L} + \sqrt{\mu})^2}, \quad \lvert \lambda^M \rvert = \dfrac{L - \mu}{(\sqrt{L} + \sqrt{\mu})^2}.
$$

. . .

И скорость сходимости не зависит от шага и равна $\sqrt{\beta^*}$.