---
title: "Стохастический градиентный спуск."
author: Даня Меркулов, Петр Остроухов
institute: Оптимизация для всех! ЦУ
format: 
    beamer:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
    beamer-cu:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader_cu.tex
      header-includes:
        - \newcommand{\cover}{../files/БАК_Методы вып_оптимизации_презентация_13.pdf}
    beamer-cu-maga:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader_cu.tex
      header-includes:
        - \newcommand{\cover}{../files/Методы вып_оптимизации_презентация_14.pdf}
header-includes:
 - \newcommand{\bgimage}{../files/back15.jpeg}
---

# Задача с конечной суммой

## Задача с конечной суммой

Рассмотрим классическую задачу минимизации среднего по конечной выборке:
$$
\min_{x \in \mathbb{R}^p} f(x) = \min_{x \in \mathbb{R}^p}\frac{1}{n} \sum_{i=1}^n f_i(x)
$$
\pause
Градиентный спуск для этой задачи записывается следующим образом:
$$
\tag{GD}
x_{k+1} = x_k - \frac{\alpha_k}{n} \sum_{i=1}^n \nabla f_i(x)
$$
\pause

* Сходимость с постоянным $\alpha$ или линейным поиском.
* Стоимость итерации линейна по $n$. Для ImageNet $n\approx 1.4 \cdot 10^7$, для WikiText $n \approx 10^8$. Для FineWeb $n \approx 15 \cdot 10^{12}$ токенов.

\pause

Давайте перейдем от полного вычисления градиента к его несмещенной оценке, когда мы случайным образом выбираем индекс $i_k$ точки на каждой итерации равномерно:
$$
\tag{SGD}
x_{k+1} = x_k - \alpha_k  \nabla f_{i_k}(x_k)
$$
С $p(i_k = i) = \frac{1}{n}$, стохастический градиент является несмещенной оценкой градиента, которая задается следующим образом:
$$
\mathbb{E}[\nabla f_{i_k}(x)] = \sum_{i=1}^{n} p(i_k = i) \nabla f_i(x) = \sum_{i=1}^{n} \frac{1}{n} \nabla f_i(x) = \frac{1}{n} \sum_{i=1}^{n} \nabla f_i(x) = \nabla f(x)
$$
Это означает, что математическое ожидание стохастического градиента равно фактическому градиенту $f(x)$.


## Результаты для градиентного спуска

Стохастические итерации в $n$ раз быстрее, но сколько итераций потребуется для достижения заданной точности?

Если $\nabla f$ является липшицевым, то мы получаем:

\begin{center}
\begin{tabular}{c c c}
\toprule
Предположение   & Детерминированный градиентный спуск & Стохастический градиентный спуск \\ 
\midrule
PL           & $\mathcal{O}\left(\log(1/\varepsilon)\right)$       & \uncover<2->{$\mathcal{O}\left(1/\varepsilon\right)$}          \\
Выпуклый       & $\mathcal{O}\left(1/\varepsilon\right)$             & \uncover<2->{$\mathcal{O}\left(1/\varepsilon^2\right)$}        \\
Невыпуклый   & $\mathcal{O}\left(1/\varepsilon\right)$             & \uncover<2->{$\mathcal{O}\left(1/\varepsilon^2\right)$}        \\
\bottomrule
\end{tabular}
\end{center}

. . .

* Стохастический градиентный спуск имеет низкую стоимость итерации, но медленную скорость сходимости. 
  * Сублинейная скорость даже в сильно выпуклом случае.
  * Оценки скорости не могут быть улучшены при стандартных предположениях.
  * Оракул возвращает несмещенную аппроксимацию градиента с ограниченной дисперсией.

* Методы с моментом и квази-Ньютоновские методы не улучшают скорость в стохастическом случае, а только могут улучшить константные множители (бутылочное горлышко — дисперсия, а не число обусловленности).

# Стохастический градиентный спуск (SGD)

## Типичное поведение

[!["Divergence"](sgd_lin_reg_divergence.jpeg){width=90%}](https://fmin.xyz/docs/visualizations/sgd_divergence.mp4)


## Гладкий PL-случай с постоянным шагом

:::{.callout-note appearance="simple"}
Пусть $f$ — $L$-гладкая функция, удовлетворяющая условию Поляка-Лоясиевича (PL) с константой $\mu>0$, а дисперсия стохастического градиента ограничена: $\mathbb{E}[\|\nabla f_i(x_k)\|^2] \leq \sigma^2$. Тогда стохастический градиентный спуск с постоянным шагом $\alpha < \frac{1}{2\mu}$ гарантирует
$$
\mathbb{E}[f(x_{k})-f^{*}] \leq (1-2\alpha\mu)^{k} [f(x_{0})-f^{*}] + \frac{L\sigma^{2}\alpha}{4\mu}.
$$
:::

## Гладкий **выпуклый** случай

### Вспомогательные обозначения  
Для (возможно) неконстантной последовательности шагов $(\alpha_t)_{t\ge0}$ определим *взвешенное среднее*  
$$
\bar x_k \;\; \stackrel{\text{def}}{=}\;\; \frac{1}{\sum_{t=0}^{k-1}\alpha_t}\;\sum_{t=0}^{k-1}\alpha_t\,x_t\,,\qquad k\ge1 .
$$  
Везде ниже $f^{*}\equiv\min_x f(x)$ и $x^{*}\in\arg\min_x f(x)$.

---

## Гладкий выпуклый случай с **постоянным** шагом

:::{.callout-note appearance="simple"}
Пусть $f$ — выпуклая функция (не обязательно гладкая), а дисперсия стохастического градиента ограничена
$\mathbb{E}\!\left[\|\nabla f_{i_k}(x_k)\|^{2}\right]\le \sigma^{2}\;\;\forall k$. Если SGD использует постоянный шаг $\alpha_t\equiv\alpha > 0$, то для любого $k\ge1$
$$
\boxed{\;
\mathbb{E}\!\left[f(\bar x_k)-f^{*}\right]\;\le\;
\frac{\|x_0-x^{*}\|^{2}}{2\alpha\,k}\;+\;\frac{\alpha\,\sigma^{2}}{2}}
$$
где $\bar x_k = \frac{1}{k}\sum_{t=0}^{k-1} x_t$.

При выборе постоянного $\displaystyle \alpha=\frac{\|x_0-x^{*}\|}{\sigma\sqrt{k}}$ (зависящего от $k$) имеем
$$
\mathbb{E}\!\left[f(\bar x_k)-f^{*}\right]\le\frac{\|x_0-x^{*}\|\sigma}{\sqrt{k}}=\mathcal{O}\!\Bigl(\tfrac{1}{\sqrt{k}}\Bigr).
$$
:::

## Гладкий выпуклый случай с **убывающим** шагом  
$\displaystyle\alpha_k=\frac{\alpha_0}{\sqrt{k+1}},\quad 0<\alpha_0\le\frac{1}{4L}$

:::{.callout-note appearance="simple"}
При тех же предположениях, но с убывающим шагом $\alpha_k=\frac{\alpha_0}{\sqrt{k+1}}$
$$
\boxed{\;
\mathbb{E}\!\left[f(\bar x_k)-f^{*}\right]
\;\le\;
\frac{5\|x_0-x^{*}\|^{2}}{4\alpha_0\sqrt{k}}
\;+\;
5\alpha_0\sigma^{2}\,\frac{\log(k+1)}{\sqrt{k}}\;}
=\;\mathcal{O}\!\Bigl(\tfrac{\log k}{\sqrt{k}}\Bigr).
$$
:::

# Мини-батч SGD

## Мини-батч SGD

### Подход 1: контролировать размер выборки
Детерминированный метод использует все $n$ градиентов:
$$
\nabla f(x_k) = \frac{1}{n} \sum_{i=1}^{n} \nabla f_i(x_k).
$$

Стохастический метод аппроксимирует это, используя только 1 элемент:
$$
\nabla f_{ik}(x_k) \approx \frac{1}{n} \sum_{i=1}^{n} \nabla f_i(x_k).
$$

Распространнёный вариант — использовать выборку элементов $B_k$ ("мини-батч"):
$$
\frac{1}{|B_k|} \sum_{i \in B_k} \nabla f_i(x_k) \approx \frac{1}{n} \sum_{i=1}^{n} \nabla f_i(x_k),
$$
особенно полезно для векторизации и параллелизации.

Например, с 16 ядрами установите $|B_k| = 16$ и вычислите 16 градиентов одновременно.

## Мини-батч как градиентный спуск с ошибкой

Метод SG с выборкой $B_k$ ("мини-батч") использует итерации:
$$
x_{k+1} = x_k - \alpha_k \left(\frac{1}{|B_k|} \sum_{i \in B_k} \nabla f_i(x_k)\right).
$$

Посмотрим на это как на "градиентный метод с ошибкой":
$$
x_{k+1} = x_k - \alpha_k(\nabla f(x_k) + e_k),
$$
где $e_k$ — разница между аппроксимированным и истинным градиентом.

Если вы используете $\alpha_k = \frac{1}{L}$, то используя лемму о спуске, этот алгоритм имеет:
$$
f(x_{k+1}) \leq f(x_k) - \frac{1}{2L} \|\nabla f(x_k)\|^2 + \frac{1}{2L} \|e_k\|^2,
$$
для любой ошибки $e_k$.

## Влияние ошибки на скорость сходимости

Оценка прогресса с $\alpha_k = \frac{1}{L}$ и ошибкой в градиенте $e_k$ выглядит следующим образом:
$$
f(x_{k+1}) \leq f(x_k) - \frac{1}{2L} \|\nabla f(x_k)\|^2 + \frac{1}{2L} \|e_k\|^2.
$$

## Идея SGD и батчей

![](batches_1.pdf)

## Идея SGD и батчей {.noframenumbering}

![](batches_2.pdf)

## Идея SGD и батчей {.noframenumbering}

![](batches_3.pdf)

## Идея SGD и батчей {.noframenumbering}

![](batches_4.pdf)

## Идея SGD и батчей {.noframenumbering}

![](batches_5.pdf)

## Основная проблема SGD

$$
f(x) = \frac{\mu}{2} \|x\|_2^2 + \frac1m \sum_{i=1}^m \log (1 + \exp(- y_i \langle a_i, x \rangle)) \to \min_{x \in \mathbb{R}^n}
$$

![](sgd_problems.pdf)

## Основные результаты сходимости SGD

:::{.callout-note appearance="simple"}
Пусть $f$ - $L$-гладкая $\mu$-сильно выпуклая функция, а дисперсия стохастического градиента конечна ($\mathbb{E}[\|\nabla f_i(x_k)\|^2] \leq \sigma^2$). Тогда траектория стохастического градиентного спуска с постоянным шагом $\alpha < \frac{1}{2\mu}$ будет гарантировать:

$$
\mathbb{E}[f(x_{k+1}) - f^*] \leq (1 - 2\alpha \mu)^k[f(x_{0}) - f^*]  + \frac{L \sigma^2 \alpha }{ 4 \mu}.
$$
:::

. . .

:::{.callout-note appearance="simple"}
Пусть $f$ - $L$-гладкая $\mu$-сильно выпуклая функция, а дисперсия стохастического градиента конечна ($\mathbb{E}[\|\nabla f_i(x_k)\|^2] \leq \sigma^2$). Тогда стохастический градиентный шум с уменьшающимся шагом $\alpha_k = \frac{2k + 1 }{ 2\mu(k+1)^2}$ будет сходиться сублинейно:

$$
\mathbb{E}[f(x_{k+1}) - f^*] \leq \frac{L \sigma^2}{ 2 \mu^2 (k+1)}
$$
:::

## Summary

* SGD с постоянным шагом не сходится даже для PL (сильно выпуклого) случая
* SGD достигает сублинейной сходимости с скоростью $\mathcal{O}\left(\frac{1}{k}\right)$ для PL-случая. 
* Ускорения Нестерова/Поляка не улучшают скорость сходимости
* Двухфазный Ньютоновский метод достигает $\mathcal{O}\left(\frac{1}{k}\right)$ без сильной выпуклости.

# Адаптивность или масштабирование

## Adagrad (Duchi, Hazan, and Singer 2010/Streeter and MacMahan 2010)

Очень популярный адаптивный метод. Пусть $g^{(k)} = \nabla f_{i_k}(x^{(k-1)})$, правило обновления для $j = 1, \dots, p$:

$$
v^{(k)}_j = v^{k-1}_j + (g_j^{(k)})^2
$$
$$
x_j^{(k)} = x_j^{(k-1)} - \alpha \frac{g_j^{(k)}}{\sqrt{v^{(k)}_j  + \epsilon}}
$$

**Заметки:**

* AdaGrad не требует настройки шага обучения: $\alpha > 0$ — фиксированная константа, и скорость обучения естественно уменьшается по итерациям.
* Шаг обучения для редких информативных признаков убывает медленно.
* Может существенно превосходить SGD на разреженных задачах.
* Основной недостаток — монотонное накопление градиентов в знаменателе. AdaDelta, Adam, AMSGrad и др. улучшают это, популярны в обучении глубоких нейронных сетей.
* Константа $\epsilon$ обычно устанавливается в $10^{-6}$ для обеспечения отсутствия деления на ноль или слишком больших шагов.

## RMSProp (Tieleman and Hinton, 2012)

Улучшение AdaGrad, которое устраняет его агрессивный, монотонно убывающий шаг обучения. Использует скользящее среднее квадратов градиентов для настройки шага обучения для каждого веса. Пусть $g^{(k)} = \nabla f_{i_k}(x^{(k-1)})$ и правило обновления для $j = 1, \dots, p$:
$$
v^{(k)}_j = \gamma v^{(k-1)}_j + (1-\gamma) (g_j^{(k)})^2
$$
$$
x_j^{(k)} = x_j^{(k-1)} - \alpha \frac{g_j^{(k)}}{\sqrt{v^{(k)}_j + \epsilon}}
$$

**Заметки:**

* RMSProp делит шаг обучения для веса на скользящее среднее величин недавних градиентов для этого веса.
* Обеспечивает более тонкую настройку шагов обучения, чем AdaGrad, что делает его подходящим для нестационарных задач.
* Широко используется при обучении нейронных сетей, особенно рекуррентных.

<!-- ## Adadelta (Zeiler, 2012)

Расширение RMSProp, нацеленное на снижение зависимости от вручную заданного глобального шага обучения. Вместо накопления всех прошлых квадратов градиентов Adadelta ограничивает окно накопленных прошлых градиентов фиксированным размером $w$. Механизм обновления не требует шага обучения $\alpha$:
$$
v^{(k)}_j = \gamma v^{(k-1)}_j + (1-\gamma) (g_j^{(k)})^2
$$
$$
\tilde{g}_j^{(k)} = \frac{\sqrt{{\Delta x_j^{(k-1)}} + \epsilon}}{\sqrt{v^{(k)}_j+ \epsilon}} g_j^{(k)}
$$
$$
x_j^{(k)} = x_j^{(k-1)} - \tilde{g}_j^{(k)}
$$
$$
\Delta x_j^{(k)} = \rho \Delta x_j^{(k-1)} + (1-\rho) (\tilde{g}_j^{(k)})^2
$$

**Заметки:**

* Adadelta адаптирует шаги обучения на основе скользящего окна обновлений градиентов, а не накопления всех прошлых градиентов. Таким образом, настроенные шаги обучения более устойчивы к изменениям динамики модели.
* Метод не требует начального установления шага обучения, что упрощает настройку.
* Часто используется в глубоком обучении, где масштабы параметров существенно различаются между слоями. -->

## Adam (Kingma and Ba, 2014) ^[[Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)] ^[[On the Convergence of Adam and Beyond](https://arxiv.org/abs/1904.09237)]

Объединяет элементы из AdaGrad и RMSProp. Учитывает экспоненциально убывающее среднее прошлых градиентов и квадратов градиентов.

::::{.columns}
:::{.column width="55%"}
\begin{tabular}{ll}
EMA: & $m_j^{(k)} = \beta_1 m_j^{(k-1)} + (1-\beta_1) g_j^{(k)}$ \\[1ex]
                           & $v_j^{(k)} = \beta_2 v_j^{(k-1)} + (1-\beta_2) \left(g_j^{(k)}\right)^2$ \\[1ex]
Коррекция смещения:         & $\hat{m}_j = \dfrac{m_j^{(k)}}{1-\beta_1^k}$ \\[1ex]
                           & $\hat{v}_j = \dfrac{v_j^{(k)}}{1-\beta_2^k}$ \\[1ex]
Обновление:                   & $x_j^{(k)} = x_j^{(k-1)} - \alpha\,\dfrac{\hat{m}_j}{\sqrt{\hat{v}_j} + \epsilon}$ \\
\end{tabular}
:::
:::{.column width="45%"}
**Заметки:**

* Компенсирует смещение к нулю в начальных моментах, наблюдаемое в других методах (например, RMSProp), что делает оценки более точными.
* Одна из самых цитируемых научных работ в мире.
* В 2018-2019 годах вышли статьи, указывающие на ошибку в оригинальной статье
* Не сходится для некоторых простых задач (даже выпуклых)
* Почему-то очень хорошо работает для некоторых сложных задач
* Гораздо лучше работает для языковых моделей, чем для задач компьютерного зрения - почему?
:::
::::

## AdamW (Loshchilov & Hutter, 2017)

Устраняет распространенную проблему с $\ell_2$-регуляризацией в адаптивных оптимизаторах, таких как Adam. Стандартная $\ell_2$-регуляризация добавляет $\lambda \|x\|^2$ к функции потерь, что приводит к градиентному слагаемому $\lambda x$. В Adam это слагаемое масштабируется адаптивным шагом обучения $\left(\sqrt{\hat{v}_j} + \epsilon\right)$, связывая затухание весов (weight decay) с величинами градиента. AdamW разделяет затухание весов от шага адаптации градиентов.

Правило обновления:
$$
m_j^{(k)} = \beta_1 m_j^{(k-1)} + (1-\beta_1) g_j^{(k)}
$$
$$
v_j^{(k)} = \beta_2 v_j^{(k-1)} + (1-\beta_2) (g_j^{(k)})^2
$$
$$
\hat{m}_j = \frac{m_j^{(k)}}{1-\beta_1^k}, \quad \hat{v}_j = \frac{v_j^{(k)} }{1-\beta_2^k}
$$
$$
x_j^{(k)} = x_j^{(k-1)} - \alpha \left( \frac{\hat{m}_j}{\sqrt{\hat{v}_j} + \epsilon} + \lambda x_j^{(k-1)} \right)
$$

**Заметки:**

* Слагаемое затухания весов $\lambda x_j^{(k-1)}$ добавляется *после* адаптивного шага по градиенту.
* Широко используется в обучении трансформаторов и других крупных моделей. Вариант по умолчанию для Hugging Face Trainer.

## Shampoo (Gupta, Anil, et al., 2018; Anil et al., 2020)

Расшифровывается как **S**tochastic **H**essian-**A**pproximation **M**atrix **P**reconditioning for **O**ptimization **O**f deep networks: стохастическое предобуславливание матрицей, основанной на аппроксимации гессиана, для оптимизации глубоких сетей. Это метод, вдохновлённый оптимизацией второго порядка и рассчитанный на крупномасштабное глубокое обучение.

**Основная идея:** аппроксимировать полноматричный предобуславливатель AdaGrad с помощью эффективных матричных структур, в частности произведений Кронекера.

Для матрицы весов $W \in \mathbb{R}^{m \times n}$, обновление включает предобуславливание с использованием приближений статистических матриц $L \approx \sum_k G_k G_k^T$ и $R \approx \sum_k G_k^T G_k$, где $G_k$ — градиенты.

Упрощённая концепция:

1. Вычислить градиент $G_k$.
2. Обновить статистику $L_k = \beta L_{k-1} + (1-\beta) G_k G_k^T$ и $R_k = \beta R_{k-1} + (1-\beta) G_k^T G_k$.
3. Вычислить предобуславливатели $P_L = L_k^{-1/4}$ и $P_R = R_k^{-1/4}$. (Обратный корень матрицы)
4. Update: $W_{k+1} = W_k - \alpha P_L G_k P_R$.

## Shampoo (Gupta, Anil, et al., 2018; Anil et al., 2020)

**Заметки:**

* Цель — эффективнее учитывать информацию о кривизне, чем методы первого порядка.
* Вычислительно дороже, чем Adam, но может сходиться быстрее или приводить к лучшим решениям по числу шагов.
* Требует аккуратной реализации для эффективности (например, эффективного вычисления корней из обратной матрицы, обработки больших матриц).
* Существуют варианты для разных форм тензоров (например, для свёрточных слоёв).


## Muon ^[[Deriving Muon](https://jeremybernste.in/writing/deriving-muon)]

$$
\begin{aligned}
W_{t+1} &= W_t - \eta (G_tG_t^\top)^{-1/4}G_t(G_t^\top G_t)^{-1/4} \\
&= W_t - \eta (US^2U^\top)^{-1/4} (USV^\top) (VS^2V^\top)^{-1/4} \\
&= W_t - \eta (US^{-1/2}U^\top) (USV^\top) (VS^{-1/2}V^\top) \\
&= W_t - \eta US^{-1/2}SS^{-1/2}V^\top \\
&= W_t - \eta UV^\top
\end{aligned}
$$

## Много методов

[![](nag_gs.pdf){width=100%}](https://fmin.xyz/docs/visualizations/nag_gs.mp4){fig-align="center"}


<!-- ## SGD расходится с любым шагом обучения для LLS -->

