---
title: Автоматическое дифференцирование
author: Даня Меркулов 
institute: Оптимизация для всех! ЦУ
format: 
   beamer:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
#    beamer-cu:
#       pdf-engine: xelatex
#       aspectratio: 169
#       fontsize: 9pt
#       section-titles: true
#       incremental: true
#       include-in-header: ../files/xeheader_cu.tex
header-includes:
 - \newcommand{\bgimage}{../files/back3.jpeg}
---

# Автоматическое дифференцирование

## {.plain}
![Когда вы поняли идею](autograd_expectations.jpeg)

## {.plain}
![Это не autograd](avtograd.jpeg){width=65%}

## Задача

Предположим, что мы хотим решить следующую задачу:
$$
L(w) \to \min_{w \in \mathbb{R}^d}
$$

. . .

* Такие задачи обычно возникают в машинном обучении, когда нам нужно найти подходящие параметры $w$ модели (например, обучить нейронную сеть). 
* Существуют разные методы решения этой задачи. Однако, размерность задач сегодня может достигать сотен миллиардов или даже триллионов переменных. Такие задачи очень тяжело решать без знания градиентов, то есть методами нулевого порядка. 
* Поэтому было бы полезно уметь вычислять вектор градиента $\nabla_w L = \left( \frac{\partial L}{\partial w_1}, \ldots, \frac{\partial L}{\partial w_d}\right)^T$. 
* Обычно методы первого порядка работают лучше в больших задачах, в то время как методы второго порядка требуют слишком много памяти.

## Пример: задача многомерного шкалирования

Предположим, что у нас есть матрица расстояний для $N$ $d$-мерных объектов $D \in \mathbb{R}^{N \times N}$. Используя эту матрицу, мы хотим восстановить исходные координаты $W_i \in \mathbb{R}^d, \; i = 1, \ldots, N$.

. . .

$$
L(W) = \sum_{i, j = 1}^N \left(\|W_i - W_j\|^2_2 - D_{i,j}\right)^2 \to \min_{W \in \mathbb{R}^{N \times d}}
$$

. . .

Ссылка на визуализацию [$\clubsuit$](http://www.benfrederickson.com/numerical-optimization/), где можно увидеть, что безградиентные методы оптимизации решают эту задачу намного медленнее, особенно в пространствах большой размерности.

:::{.callout-question}
Связано ли это с PCA?
:::

## Пример: многомерное масштабирование

![[Ссылка на анимацию](https://fmin.xyz/docs/visualizations/mds.mp4)](mds.png){width=40%}

## Пример: градиентный спуск без градиента

:::: {.columns}
::: {.column width="50%"}
Предположим, что мы хотим решить следующую задачу:
$$
L(w) \to \min_{w \in \mathbb{R}^d}
$$

. . .

с помощью алгоритма градиентного спуска (GD):
$$
w_{k+1} = w_k - \alpha_k \nabla_w L(w_k)
$$

. . .

Можно ли заменить $\nabla_w L(w_k)$ используя только информацию нулевого порядка? 

. . .

Да, но за определенную цену.

. . .

Рассмотрим двухточечную оценку градиента^[рекомендуется [хорошая](https://scholar.harvard.edu/files/yujietang/files/slides_2019_zero-order_opt_tutorial.pdf) презентация о безградиентных методах] $G$:
$$
G = d\dfrac{L(w + \varepsilon v)- L(w - \varepsilon v)}{2 \varepsilon}v, 
$$
где $v$ сферически симметричен.
:::

. . .

::: {.column width="50%"}
!["Иллюстрация двухточечной оценки градиентного спуска"](zgd_2p.pdf)
:::

::::


## Пример: конечные разности

:::: {.columns}
::: {.column width="50%"}

$$
w_{k+1} = w_k - \alpha_k G
$$

. . .
 
Также рассмотрим идею конечных разностей:
$$
G =  \sum\limits_{i=1}^d\dfrac{L(w+\varepsilon e_i) - L(w-\varepsilon e_i)}{2\varepsilon} e_i
$$
[Открыть в Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Zero_order_GD.ipynb)

:::

::: {.column width="50%"}
!["Иллюстрация работы метода оценки градиента с помощью метода конечных разностей"](zgd_fd.pdf)
:::

::::

## Проклятие размерности для методов нулевого порядка ^[[Оптимальные скорости для нулевого порядка выпуклой оптимизации: сила двух оценок функции](https://arxiv.org/pdf/1312.2139)]
$$
\min_{x \in \mathbb{R}^n} f(x)
$$

. . .

$$
\text{GD: } x_{k+1} = x_k - \alpha_k \nabla f(x_k) \qquad \qquad \text{Zero order GD: } x_{k+1} = x_k - \alpha_k G,
$$

где $G$ - оценка градиента 2-точечная или многоточечная.

. . .

|  | $f(x)$ - гладкая | $f(x)$ - гладкая и выпуклая | $f(x)$ - гладкая и сильно выпуклая |
|:-:|:---:|:----:|:-------:|
| GD | $\|\nabla f(x_k)\|^2 \approx \mathcal{O} \left( \dfrac{1}{k} \right)$ | $f(x_k) - f^* \approx  \mathcal{O} \left( \dfrac{1}{k} \right)$ | $\|x_k - x^*\|^2 \approx \mathcal{O} \left( \left(1 - \dfrac{\mu}{L}\right)^k \right)$ |
| GD нулевого порядка | $\|\nabla f(x_k)\|^2 \approx \mathcal{O} \left( \dfrac{n}{k} \right)$ | $f(x_k) - f^* \approx  \mathcal{O} \left( \dfrac{n}{k} \right)$ | $\|x_k - x^*\|^2 \approx \mathcal{O} \left( \left(1 - \dfrac{\mu}{n L}\right)^k \right)$ |

Для 2-точечных оценок, мы не можем сделать зависимость лучше, чем от $\sqrt{n}$ !


## Конечные разности

Наивный подход к получению приблизительных значений градиентов - это подход **конечных разностей**. Для каждой координаты, можно вычислить приближенное значение частной производной:
$$
\dfrac{\partial L}{\partial w_k} (w) \approx \dfrac{L(w+\varepsilon e_k) - L(w)}{\varepsilon}, \quad e_k = (0, \ldots, \underset{{\tiny k}}{1}, \ldots, 0)
$$

. . .

:::{.callout-question}
Если время, необходимое для одного вычисления $L(w)$ равно $T$, то какое время необходимо для вычисления $\nabla_w L$ с этим подходом?

. . .

**Ответ** $2dT$, что очень долго для больших задач. Кроме того, этот метод нестабилен, что означает, что нам придется выбирать между точностью и стабильностью.

. . .

**Теорема**

Существует алгоритм для вычисления $\nabla_w L$ за $\mathcal{O}(T)$. ^[Linnainmaa S. The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors.  Master’s Thesis (in Finnish), Univ. Helsinki, 1970.]

:::

## Прямой режим автоматического дифференцирования

Чтобы глубже понять идею автоматического дифференцирования, рассмотрим простую функцию для вычисления производных: 
$$
L(w_1, w_2) = w_2 \log w_1 + \sqrt{w_2 \log w_1}
$$

. . .

Давайте нарисуем *вычислительный граф* этой функции:

![Иллюстрация вычислительного графа для функции $L(w_1, w_2)$](comp_graph.pdf)

. . .

Давайте пойдем от начала графа к концу и вычислим производную $\dfrac{\partial L}{\partial w_1}$.

## Прямой режим автоматического дифференцирования{.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](comp_graph1.pdf)

:::: {.columns}

::: {.column width="50%"}
### Функция 

$w_1 = w_1, w_2 = w_2$
:::

. . .

::: {.column width="50%"}
### Производная

$\dfrac{\partial w_1}{\partial w_1} = 1, \dfrac{\partial w_2}{\partial w_1} = 0$ 
:::

::::




## Прямой режим автоматического дифференцирования{.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](comp_graph2.pdf)

. . .


:::: {.columns}

::: {.column width="50%"}
### Функция 

$v_1 = \log w_1$ 
:::

. . .


::: {.column width="50%"}
### Производная

$\frac{\partial v_1}{\partial w_1} = \frac{\partial v_1}{\partial w_1} \frac{\partial w_1}{\partial w_1} = \frac{1}{w_1} 1$
:::

::::

## Прямой режим автоматического дифференцирования{.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](comp_graph3.pdf)

. . .


:::: {.columns}

::: {.column width="50%"}
### Функция 

$v_2 = w_2 v_1$
:::

. . .


::: {.column width="50%"}
### Производная

$\frac{\partial v_2}{\partial w_1} = \frac{\partial v_2}{\partial v_1}\frac{\partial v_1}{\partial w_1} + \frac{\partial v_2}{\partial w_2}\frac{\partial w_2}{\partial w_1} = w_2\frac{\partial v_1}{\partial w_1} + v_1\frac{\partial w_2}{\partial w_1}$
:::

::::

## Прямой режим автоматического дифференцирования{.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](comp_graph4.pdf)

. . .


:::: {.columns}

::: {.column width="50%"}
### Функция 

$v_3 = \sqrt{v_2}$
:::

. . .


::: {.column width="50%"}
### Производная

$\frac{\partial v_3}{\partial w_1} = \frac{\partial v_3}{\partial v_2}\frac{\partial v_2}{\partial w_1} = \frac{1}{2\sqrt{v_2}}\frac{\partial v_2}{\partial w_1}$
:::

::::

## Прямой режим автоматического дифференцирования{.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](comp_graph5.pdf)

. . .


:::: {.columns}

::: {.column width="50%"}
### Функция 

$L = v_2 + v_3$ 
:::

. . .


::: {.column width="50%"}
### Производная

$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial v_2}\frac{\partial v_2}{\partial w_1} + \frac{\partial L}{\partial v_3}\frac{\partial v_3}{\partial w_1} = 1\frac{\partial v_2}{\partial w_1} + 1\frac{\partial v_3}{\partial w_1}$
:::

::::

## Сделайте аналогичные вычисления для $\dfrac{\partial L}{\partial w_2}$

![Иллюстрация вычислительного графа для функции $L(w_1, w_2)$](comp_graph.pdf)


## Пример прямого режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](cgraph_ex_1.pdf)

:::: {.columns}

::: {.column width="50%"}
### Функция 

$w_1 = w_1, w_2 = w_2$
:::

::: {.column width="50%"}
### Производная

$\dfrac{\partial w_1}{\partial w_2} = 0, \dfrac{\partial w_2}{\partial w_2} = 1$
:::

::::

## Пример прямого режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](cgraph_ex_2.pdf)

:::: {.columns}

::: {.column width="50%"}
### Функция 

$v_1 = \log w_1$
:::

::: {.column width="50%"}
### Производная

$\frac{\partial v_1}{\partial w_2} = \frac{\partial v_1}{\partial w_1} \frac{\partial w_1}{\partial w_2}= \frac{1}{w_1} \cdot 0$
:::

::::

## Пример прямого режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](cgraph_ex_3.pdf)

:::: {.columns}

::: {.column width="50%"}
### Функция 

$v_2 = w_2 v_1$
:::

::: {.column width="50%"}
### Производная

$\frac{\partial v_2}{\partial w_2} = \frac{\partial v_2}{\partial v_1}\frac{\partial v_1}{\partial w_2} + \frac{\partial v_2}{\partial w_2}\frac{\partial w_2}{\partial w_2} = w_2\frac{\partial v_1}{\partial w_2} + v_1\frac{\partial w_2}{\partial w_2}$ 
:::

::::

## Пример прямого режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](cgraph_ex_4.pdf)

:::: {.columns}

::: {.column width="50%"}
### Функция 

$v_3 = \sqrt{v_2}$
:::

::: {.column width="50%"}
### Производная

$\frac{\partial v_3}{\partial w_2} = \frac{\partial v_3}{\partial v_2}\frac{\partial v_2}{\partial w_2} = \frac{1}{2\sqrt{v_2}}\frac{\partial v_2}{\partial w_2}$
:::

::::

## Пример прямого режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](cgraph_ex_5.pdf)

:::: {.columns}

::: {.column width="50%"}
### Функция 

$L = v_2 + v_3$
:::

::: {.column width="50%"}
### Производная

$\frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial v_2}\frac{\partial v_2}{\partial w_2} + \frac{\partial L}{\partial v_3}\frac{\partial v_3}{\partial w_2} = 1\frac{\partial v_2}{\partial w_2} + 1\frac{\partial v_3}{\partial w_2}$
:::

::::

## Алгоритм прямого режима автоматического дифференцирования


:::: {.columns}

::: {.column width="50%"}

Предположим, что у нас есть вычислительный граф $v_i, i \in [1; N]$. Наша цель - вычислить производную выхода этого графа по некоторой входной переменной $w_k$, т.е. $\dfrac{\partial v_N}{\partial w_k}$. Эта идея предполагает распространение градиента по входной переменной от начала к концу, поэтому мы можем ввести обозначение: 

. . .

$$
\overline{v_i} = \dfrac{\partial v_i}{\partial w_k}
$$
![Иллюстрация прямого режима автоматического дифференцирования](auto_diff_forward.pdf){width=80%}

:::

. . .

::: {.column width="50%"}

* Для $i = 1, \ldots, N$:
    * Вычислить $v_i$ как функцию его предков $x_1, \ldots, x_{t_i}$:
 $$
        v_i = v_i(x_1, \ldots, x_{t_i})
        $$
    * Вычислить производную $\overline{v_i}$ используя формулу производной сложной функции:
 $$
        \overline{v_i} = \sum_{j = 1}^{t_i}\dfrac{\partial v_i}{\partial x_j}\dfrac{\partial x_j}{\partial w_k}
        $$

. . .

Обратите внимание, что этот подход не требует хранения всех промежуточных вычислений, но можно видеть, что для вычисления производной $\dfrac{\partial L}{\partial w_k}$ нам нужно $\mathcal{O}(T)$ операций. Это означает, что для всего градиента, нам нужно $d\mathcal{O}(T)$ операций, что то же самое, что и для конечных разностей, но теперь у нас нет проблем со стабильностью или неточностями(формулы выше точны).

:::

::::

## {.plain}
![](yoda.jpg)


## Обратный режим автоматического дифференцирования

Мы рассмотрим ту же функцию с вычислительным графом:

![Иллюстрация вычислительного графа для функции $L(w_1, w_2)$](comp_graph.pdf)

. . .


Предположим, что у нас есть некоторые значения параметров $w_1, w_2$ и мы уже выполнили прямой проход (т.е. вычисление значений всех промежуточных узлов вычислительного графа). Предположим также, что мы как-то сохранили все промежуточные значения $v_i$. Давайте пойдем от конца графа к началу и вычислим производные $\dfrac{\partial L}{\partial w_1}, \dfrac{\partial L}{\partial w_2}$:

## Пример обратного режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация обратного режима автоматического дифференцирования](revad1.pdf)

. . .

### Производные

. . .

$$
\dfrac{\partial L}{\partial L} = 1
$$

## Пример обратного режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация обратного режима автоматического дифференцирования](revad2.pdf)

. . .

### Производные

. . .

$$
\begin{aligned}\frac{\partial L}{\partial v_3} &= \frac{\partial L}{\partial L} \frac{\partial L}{\partial v_3} &= \frac{\partial L}{\partial L} 1\end{aligned}
$$ 

## Пример обратного режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация обратного режима автоматического дифференцирования](revad3.pdf)

. . .

### Производные

. . .

$$
\begin{aligned}\frac{\partial L}{\partial v_2} &= \frac{\partial L}{\partial v_3}\frac{\partial v_3}{\partial v_2} + \frac{\partial L}{\partial L}\frac{\partial L}{\partial v_2} &= \frac{\partial L}{\partial v_3}\frac{1}{2\sqrt{v_2}} +  \frac{\partial L}{\partial L}1\end{aligned}
$$

## Пример обратного режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация обратного режима автоматического дифференцирования](revad4.pdf)

. . .

### Производные

. . .

$$
\begin{aligned}\frac{\partial L}{\partial v_1} &=\frac{\partial L}{\partial v_2}\frac{\partial v_2}{\partial v_1}  &= \frac{\partial L}{\partial v_2}w_2\end{aligned}
$$

## Пример обратного режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация обратного режима автоматического дифференцирования](revad5.pdf)

. . .

### Производные

. . .

$$
\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial v_1}\frac{\partial v_1}{\partial w_1} = \frac{\partial L}{\partial v_1}\frac{1}{w_1} \qquad \qquad \frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial v_2}\frac{\partial v_2}{\partial w_2} = \frac{\partial L}{\partial v_1}v_1
$$

## Обратный режим автоматического дифференцирования

:::{.callout-question}
Обратите внимание, что для того же количества вычислений, что и в прямом режиме, мы получаем полный вектор градиента $\nabla_w L$. Какова стоимость ускорения?

. . .

**Ответ** Обратите внимание, что для использования обратного режима AD вам нужно хранить все промежуточные вычисления из прямого прохода. Эта проблема может быть частично решена с помощью чекпоинтинга, при котором мы сохраняем только часть промежуточных значений, а остальные пересчитываем заново по мере необходимости. Это позволяет значительно уменьшить объём требуемой памяти при обучении больших моделей машинного обучения.
:::


## Алгоритм обратного режима автоматического дифференцирования


:::: {.columns}

::: {.column width="50%"}

Предположим, что у нас есть вычислительный граф $v_i, i \in [1; N]$. Наша цель - вычислить производную выхода этого графа по всем входным переменным $w$, т.е. $\nabla_w v_N =  \left( \frac{\partial v_N}{\partial w_1}, \ldots, \frac{\partial v_N}{\partial w_d}\right)^T$. Эта идея предполагает распространение градиента функции по промежуточным переменным от конца к началу, поэтому мы можем ввести обозначение: 
$$
\overline{v_i}  = \dfrac{\partial L}{\partial v_i} = \dfrac{\partial v_N}{\partial v_i}
$$
![Иллюстрация обратного режима автоматического дифференцирования](auto_diff_reverse.pdf){width=60%}

:::

::: {.column width="50%"}

* **ПРЯМОЙ ПРОХОД** 

    Для $i = 1, \ldots, N$:

    * Вычислить и сохранить значения $v_i$ как функцию его предков

* **ОБРАТНЫЙ ПРОХОД**
    
    Для $i = N, \ldots, 1$:

    * Вычислить производную $\overline{v_i}$ используя формулу производной сложной функции и информацию от всех потомков (выходов):
        $$
        \overline{v_i} = \dfrac{\partial L}{\partial v_i} = \sum_{j = 1}^{t_i} \dfrac{\partial L}{\partial x_j} \dfrac{\partial x_j}{\partial v_i}
        $$

:::

::::


## Choose your fighter


:::: {.columns}

::: {.column width="40%"}

![Какой режим вы бы выбрали для вычисления градиентов?](ad_choose.pdf)
:::

::: {.column width="60%"}
:::{.callout-question}
Какой из режимов AD вы бы выбрали (прямой/обратный) для следующего вычислительного графа арифметических операций? Предположим, что вам нужно вычислить якобиан $J = \left\{ \dfrac{\partial L_i}{\partial w_j} \right\}_{i,j}$
:::

. . .

**Ответ** Обратите внимание, что время вычислений в обратном режиме пропорционально количеству выходов, тогда как время работы прямого режима пропорционально количеству входов. Поэтому было бы хорошей идеей рассмотреть прямой режим AD. 

:::



::::

## Choose your fighter

![ [$\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Autograd_and_Jax.ipynb) График иллюстрирует идею выбора между режимами автоматического дифференцирования. Размерность входа $n = 100$ фиксирована, измерено время вычисления якобиана в зависимости от соотношения размерностей выхода и входа для разных размерностей выхода $m$.](forward_vs_reverse_ad.pdf){width=88%}

## Choose your fighter

:::: {.columns}

::: {.column width="40%"}

![Какой режим вы бы выбрали для вычисления градиентов?](ad_mixed.pdf)

:::

::: {.column width="60%"}
:::{.callout-question}
Какой из режимов AD вы бы выбрали (прямой/обратный) для следующего вычислительного графа арифметических операций? Предположим, что вам нужно вычислить якобиан $J = \left\{ \dfrac{\partial L_i}{\partial w_j} \right\}_{i,j}$. Обратите внимание, что $G$ - это произвольный вычислительный граф
:::
. . .

**Ответ** В общем случае невозможно ответить без некоторого знания о конкретной структуре графа $G$. Следует отметить, что существуют продвинутые подходы, смешивающие прямой и обратный режим AD в зависимости от конкретной структуры графа $G$.

:::


::::

## Архитектура прямого распространения

:::: {.columns}

::: {.column width="50%"}

**ПРЯМОЙ ПРОХОД**

* $v_0 = x$ на вход обычно подаётся батч данных $x$
* Для $k = 1, \ldots, t-1, t$: 
    
    * $v_k = \sigma(v_{k-1}w_k)$. Обратите внимание, что на практике, данные имеют размерность $x  \in \mathbb{R}^{b \times d}$, где $b$ - размер батча (для одного объекта из выборки $b=1$). В то время как матрица весов $w_k$ $k$ слоя имеет размер $n_{k-1} \times n_k$, где $n_k$ - размер внутреннего представления данных. 

* $L = L(v_t)$ - вычислить функцию потерь.

**ОБРАТНЫЙ ПРОХОД**

* $v_{t+1} = L, \dfrac{\partial L}{\partial L} = 1$
* Для $k = t, t-1, \ldots, 1$: 
    
    * $\underset{b \times n_k}{\dfrac{\partial L}{\partial v_k}} = \underset{b \times n_{k+1}}{\dfrac{\partial L}{\partial v_{k+1}}} \underset{n_{k+1} \times n_k}{\dfrac{\partial v_{k+1}}{\partial v_{k}}}$
    * $\underset{b \times n_{k-1} \cdot n_k}{\dfrac{\partial L}{\partial w_k}} = \underset{b \times n_{k+1}}{\dfrac{\partial L}{\partial v_{k+1}}} \cdot  \underset{n_{k+1} \times n_{k-1} \cdot n_k}{\dfrac{\partial v_{k+1}}{\partial w_{k}}}$


:::

::: {.column width="50%"}

![Архитектура прямого распространения нейронной сети](feedforward.pdf)

:::

::::

## Произведение Гессиана на вектор без вычисления самого Гессиана



Когда вам нужна некоторая информация о кривизне функции, обычно вам нужно работать с гессианом. Однако, это трудно делать, когда размерность задачи велика. Для скалярной функции $f : \mathbb{R}^n \to \mathbb{R}$, гессиан в точке $x \in \mathbb{R}^n$ записывается как $\nabla^2 f(x)$. Тогда произведение вектора на гессиан можно записать как

. . .

$$
v \mapsto \nabla^2 f(x) \cdot v
$$

. . .

для любого вектора $v \in \mathbb{R}^n$. Мы можем использовать тождество
$$
\nabla^2 f (x) v = \nabla [x \mapsto \nabla f(x)^T \cdot v] = \nabla g(x),
$$
где $g(x) = \nabla f(x)^T \cdot v$ - новая функция, которая скалярно умножает градиент $f$ в $x$ на вектор $v$.

. . .

```python
import jax.numpy as jnp

def hvp(f, x, v):
    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)
```

## Динамика обучения нейронной сети через спектр Гессиана и hvp ^[[Некоторые исследования в оптимизации нейронных сетей через спектр собственных значений Гессиана](https://arxiv.org/abs/1901.10159)]

![Большие по модулю отрицательные собственные значения гессиана исчезли после обучения ResNet-32](ResNet_32_before_After.png)

## Идея Хадчинсона для оценки следа матрицы  ^[[A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines - M.F. Hutchinson, 1990](https://www.tandfonline.com/doi/abs/10.1080/03610919008812866)]

Метод Хатчинсона позволяет оценить след гессиана с помощью операций вычисления умножения гессиана на произвольный вектор:

Пусть $X \in \mathbb{R}^{d \times d}$ и $v \in \mathbb{R}^d$ - случайный вектор такой, что $\mathbb{E}[vv^T] = I$. Тогда,

:::: {.columns}
::: {.column width="30%"}

$$
\mathrm{Tr}(X) = \mathbb{E}[v^TXv] = \frac{1}{V}\sum_{i=1}^{V}v_i^TXv_i.
$$

:::
::: {.column width="70%"}
![[Источник](https://docs.backpack.pt/en/master/use_cases/example_trace_estimation.html)](Hutchinson_trace_est.pdf){width=80%}
:::
::::


## Чекпоинтинг

Анимация вышеуказанных подходов [\faGithub](https://github.com/cybertronai/gradient-checkpointing)

Пример использования контрольных точек градиента [\faGithub](https://colab.research.google.com/github/oseledets/dl2023/blob/main/seminars/seminar-10/Large_model_training_practice.ipynb)

. . .

В качестве примера рассмотрим обучение **GPT-2**^[[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)]: 

   * Активации в простом режиме могут занимать гораздо больше памяти: для последовательности длиной 1K и размера батча $32$, $60$ GB нужно для хранения всех промежуточных активаций. 
   * Чекпоинтинг может снизить потребление до 8 GB, пересчитывая их (33% дополнительных вычислений) 


## Чем автоматическое дифференцирование (AD) не является:

:::: {.columns}

::: {.column width="40%"}

* AD не является методом конечных разностей
* AD не является символьным вычислением производных
* AD не является только правилом вычисления производной сложной функции
* AD (обратный режим) является времяэффективным и численно стабильным
* AD (обратный режим) не является эффективным по памяти (нужно хранить все промежуточные вычисления из прямого прохода)

:::

::: {.column width="60%"}

![Различные подходы для взятия производных](differentiation_scheme.pdf)

:::

::::

## Дополнительные материалы

* Рекомендую прочитать официальную книгу по Jax Autodiff. [Open In Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Autograd_and_Jax.ipynb)
* Распространение градиента через линейные наименьшие квадраты [семинар]
* Распространение градиента через SVD [семинар]
* Контрольные точки активаций [семинар]

# Итоги

## Итоги

:::: {.columns .nonincremental}

::: {.column width="50%"}

### Определения

1. Формула для приближенного вычисления производной функции $f(x): \mathbb{R}^n \to \mathbb{R}$ по $k$-ой координате с помощью метода конечных разностей.
1. Пусть $f = f(x_1(t), \ldots, x_n(t))$. Формула для вычисления $\frac{\partial f}{\partial t}$ через $\frac{\partial x_i}{\partial t}$ (Forward chain rule).
1. Пусть $L$ - функция, возвращающая скаляр, а $v_k$ - функция, возвращающая вектор $x \in \mathbb{R}^t$. Формула для вычисления $\frac{\partial L}{\partial v_k}$ через $\frac{\partial L}{\partial x_i}$ (Backward chain rule).
1. Идея Хатчинсона для оценки следа матрицы с помощью matvec операций.

:::

::: {.column width="50%"}

### Теоремы

1. Автоматическое дифференцирование. Вычислительный граф. Forward/ Backward mode (в этом вопросе нет доказательств, но необходимо подробно описать алгоритмы).

:::

::::




<!-- ## Gradient propagation through the linear least squares

:::: {.columns}

::: {.column width="40%"}

![$x$ could be found as a solution of linear system](linear_least_squares_layer.pdf)

:::

::: {.column width="60%"}

Suppose, we have an invertible matrix $A$ and a vector $b$, the vector $x$ is the solution of the linear system $Ax = b$, namely one can write down an analytical solution $x = A^{-1}b$, in this example we will show, that computing all derivatives $\dfrac{\partial L}{\partial A}, \dfrac{\partial L}{\partial b}, \dfrac{\partial L}{\partial x}$, i.e. the backward pass, costs approximately the same as the forward pass.

. . .

It is known, that the differential of the function does not depend on the parametrization:

$$
dL = \left\langle\dfrac{\partial L}{\partial x}, dx \right\rangle = \left\langle\dfrac{\partial L}{\partial A}, dA \right\rangle + \left\langle\dfrac{\partial L}{\partial b}, db \right\rangle
$$

. . .

Given the linear system, we have:

$$
\begin{split}
Ax &= b \\
dAx + Adx = db &\to dx = A^{-1}(db - dAx)
\end{split}
$$

:::

::::

## Gradient propagation through the linear least squares

:::: {.columns}

::: {.column width="40%"}

![$x$ could be found as a solution of linear system](linear_least_squares_layer.pdf)

:::

::: {.column width="60%"}

The straightforward substitution gives us:

$$
\left\langle\dfrac{\partial L}{\partial x}, A^{-1}(db - dAx) \right\rangle = \left\langle\dfrac{\partial L}{\partial A}, dA \right\rangle + \left\langle\dfrac{\partial L}{\partial b}, db \right\rangle
$$

. . .

$$
\left\langle -A^{-T}\dfrac{\partial L}{\partial x} x^T, dA \right\rangle + \left\langle A^{-T}\dfrac{\partial L}{\partial x},db \right\rangle = \left\langle\dfrac{\partial L}{\partial A}, dA \right\rangle + \left\langle\dfrac{\partial L}{\partial b}, db \right\rangle
$$

. . .

Therefore:

$$
\dfrac{\partial L}{\partial A} = -A^{-T}\dfrac{\partial L}{\partial x} x^T \quad \dfrac{\partial L}{\partial b} =  A^{-T}\dfrac{\partial L}{\partial x}
$$

. . .

It is interesting, that the most computationally intensive part here is the matrix inverse, which is the same as for the forward pass. Sometimes it is even possible to store the result itself, which makes the backward pass even cheaper.

:::

::::

## Gradient propagation through the SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

Suppose, we have the rectangular matrix $W \in \mathbb{R}^{m \times n}$, which has a singular value decomposition:

$$
W = U \Sigma V^T, \quad U^TU = I, \quad V^TV = I, \quad \Sigma = \text{diag}(\sigma_1, \ldots, \sigma_{\min(m,n)})
$$

1. Similarly to the previous example:

 $$
 \begin{split}
 W &= U \Sigma V^T \\
 dW &= dU \Sigma V^T + U d\Sigma V^T + U \Sigma dV^T \\
 U^T dW V &= U^TdU \Sigma V^TV + U^TU d\Sigma V^TV + U^TU \Sigma dV^TV \\
 U^T dW V &= U^TdU \Sigma + d\Sigma + \Sigma dV^TV
 \end{split}
 $$

:::

::::

## Gradient propagation through the SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

2. Note, that $U^T U = I \to dU^TU + U^T dU = 0$. But also $dU^TU = (U^T dU)^T$, which actually involves, that the matrix $U^TdU$ is antisymmetric:

 $$
 (U^T dU)^T +  U^T dU = 0 \quad \to \quad \text{diag}( U^T dU) = (0, \ldots, 0)
 $$

 The same logic could be applied to the matrix $V$ and

 $$
 \text{diag}(dV^T V) = (0, \ldots, 0)
 $$

3. At the same time, the matrix $d \Sigma$ is diagonal, which means (look at the 1.) that

 $$
 \text{diag}(U^T dW V) = d \Sigma 
 $$

 Here on both sides, we have diagonal matrices.

:::

::::

## Gradient propagation through the SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

4. Now, we can decompose the differential of the loss function as a function of $\Sigma$ - such problems arise in ML problems, where we need to restrict the matrix rank:

 $$
 \begin{split}
 dL &= \left\langle\dfrac{\partial L}{\partial \Sigma}, d\Sigma \right\rangle \\
 &= \left\langle\dfrac{\partial L}{\partial \Sigma}, \text{diag}(U^T dW V)\right\rangle \\
 &= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T \text{diag}(U^T dW V) \right)
 \end{split}
 $$

:::

::::

## Gradient propagation through the SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

5. As soon as we have diagonal matrices inside the product, the trace of the diagonal part of the matrix will be equal to the trace of the whole matrix:

 $$
 \begin{split}
 dL &= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T \text{diag}(U^T dW V) \right) \\
 &= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T U^T dW V \right)  \\
 &= \left\langle\dfrac{\partial L}{\partial \Sigma}, U^T dW V \right\rangle \\
 &= \left\langle U \dfrac{\partial L}{\partial \Sigma} V^T, dW \right\rangle 
 \end{split}
 $$

:::

::::

## Gradient propagation through the SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

6. Finally, using another parametrization of the differential

 $$
 \left\langle U \dfrac{\partial L}{\partial \Sigma} V^T, dW \right\rangle = \left\langle\dfrac{\partial L}{\partial W}, dW \right\rangle
 $$

 $$
 \dfrac{\partial L}{\partial W} =  U \dfrac{\partial L}{\partial \Sigma} V^T,
 $$

 This nice result allows us to connect the gradients $\dfrac{\partial L}{\partial W}$ and $\dfrac{\partial L}{\partial \Sigma}$.
:::

::::
 -->