---
title: "Негладкая оптимизация. Нижние оценки. Субградиентный метод"
author: Даня Меркулов
institute: Оптимизация для всех! ЦУ
format: 
    beamer:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
    beamer-cu:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader_cu.tex
      header-includes:
        - \newcommand{\cover}{../files/БАК_Методы вып_оптимизации_презентация_12.pdf}
    beamer-cu-maga:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader_cu.tex
      header-includes:
        - \newcommand{\cover}{../files/Методы вып_оптимизации_презентация_12.pdf}
header-includes:
 - \newcommand{\bgimage}{../files/back13.jpeg}
---

# Негладкие задачи

## Задача наименьших квадратов с $\ell_1$-регуляризацией

[![](l1_regularization.jpeg)](https://fmin.xyz/assets/Notebooks/Regularization_horizontal.mp4)

## Нормы не являются гладкими

$$
\min_{x \in \mathbb{R}^n} f(x),
$$

Рассмотрим задачу выпуклой оптимизации. Будем считать $f(x)$ выпуклой, но не обязательно гладкой.

![Конусы норм для разных $p$-норм не являются гладкими](norm_cones.pdf){width=90%}

## Пример Вульфа

![Пример Вульфа. [\faPython Открыть в Colab](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/subgrad.ipynb)](wolfe_3d.pdf){width=90%}

# Вычисление субградиента

## Линейная нижняя оценка для выпуклых функций

:::: {.columns}

::: {.column width="60%"}
![Аппроксимация Тейлора первого порядка служит глобальной нижней оценкой для выпуклой функции](Subgrad.pdf)
:::

::: {.column width="40%"}
Важное свойство выпуклой функции $f(x)$: для любой точки $x_0$ и для всех $x \in \operatorname{dom} f$ выполняется неравенство:
$$
f(x) \geq f(x_0) + \langle g, x - x_0 \rangle
$$

. . .

для некоторого вектора $g$, то есть касательная к графику функции является *глобальной* нижней оценкой для функции. 

* Если $f(x)$ дифференцируема, то $g = \nabla f(x_0)$.
* Не все непрерывные выпуклые функции дифференцируемы.

. . .

Мы хотим сохранить это полезное свойство и для негладких функций.
:::

::::

## Субградиент и субдифференциал

Вектор $g$ называется **субградиентом** функции $f(x): S \to \mathbb{R}$ в точке $x_0$, если $\forall x \in S$:
$$
f(x) \geq f(x_0) + \langle g, x - x_0 \rangle
$$

. . .

Множество всех субградиентов функции $f(x)$ в точке $x_0$ называется **субдифференциалом** функции $f$ в точке $x_0$ и обозначается $\partial f(x_0)$.

. . .

![Субдифференциал — это множество всех возможных субградиентов](Subdifferential.pdf)

## Субградиент и субдифференциал

Найдите $\partial f(x)$, если $f(x) = |x|$

. . .

![Субдифференциал $\vert x \vert$](subgradmod.pdf){width=85%}

## Свойства субдифференциала

:::: {.columns}
::: {.column width="50%"}
* Если $x_0 \in \operatorname{ri}(S)$, то $\partial f(x_0)$ является выпуклым компактным множеством.
* Выпуклая функция $f(x)$ дифференцируема в точке $x_0 \Rightarrow \partial f(x_0) = \{\nabla f(x_0)\}$.
* Если $\partial f(x_0) \neq \emptyset \quad \forall x_0 \in S$, то $f(x)$ выпукла на $S$.

. . .


::: {.callout-theorem}

### Субдифференциал дифференцируемой функции

Пусть $f : S \to \mathbb{R}$ — функция, определенная на множестве $S$ в евклидовом пространстве $\mathbb{R}^n$. Если $x_0 \in \operatorname{ri}(S)$ и $f$ дифференцируема в точке $x_0$, то либо $\partial f(x_0) = \emptyset$, либо $\partial f(x_0) = \{\nabla f(x_0)\}$. Более того, если функция $f$ выпукла, то первая ситуация невозможна.
:::
:::

. . .

::: {.column width="50%"}

**Доказательство**

1. Пусть $s \in \partial f(x_0)$ для некоторого $s \in \mathbb{R}^n$, отличного от $\nabla f(x_0)$. Пусть $v \in  \mathbb{R}^n$ — единичный вектор. Поскольку $x_0$ — внутренняя точка $S$, существует $\delta > 0$, такое что $x_0 + tv \in S$ для всех $0 < t < \delta$. По определению субградиента:
 $$
    f(x_0 + tv) \geq f(x_0) + t \langle s, v \rangle
 $$

. . .

что влечёт:
$$
\frac{f(x_0 + tv) - f(x_0)}{t} \geq \langle s, v \rangle
$$
для всех $0 < t < \delta$. Переходя к пределу при $t \to 0$ и используя определение градиента, получаем:
$$
\langle \nabla f(x_0), v \rangle = \lim_{{t \to 0; 0 < t < \delta}} \frac{f(x_0 + tv) - f(x_0)}{t} \geq \langle s, v \rangle
$$
:::
::::

## Свойства субдифференциала

2. Отсюда $\langle s - \nabla f(x_0), v \rangle \leq 0$. В силу произвольности $v$ можно выбрать 
 $$
    v = \frac{s - \nabla f(x_0)}{\| s - \nabla f(x_0) \|},
    $$ 
 которое приводит к $s = \nabla f(x_0)$ (противоречие, если $s \neq \nabla f(x_0)$ и мы предполагали строгое неравенство, но здесь проще: $\langle \nabla f(x_0) - s, v \rangle \ge 0$ для всех $v \implies \nabla f(x_0) = s$).

3. Более того, если функция $f$ выпукла, то согласно критерию выпуклости дифференцируемой функции: $f(x) \geq f(x_0) + \langle \nabla f(x_0), x - x_0 \rangle$ для всех $x \in S$. Это в точности означает, что $\nabla f(x_0) \in \partial f(x_0)$.


## Вычисление субдифференциалов

:::: {.columns}
::: {.column width="50%"}
::: {.callout-theorem}
### Теорема Моро — Рокафеллара (субдифференциал линейной комбинации)
Пусть $f_i(x)$ — выпуклые функции на выпуклых множествах $S_i, \; i = 1, \dots, n$. Тогда если $\bigcap\limits_{i=1}^n \operatorname{ri} (S_i) \neq \emptyset$, то функция $f(x) = \sum\limits_{i=1}^n a_i f_i(x), \; a_i > 0$ имеет субдифференциал $\partial_S f(x)$ на множестве $S = \bigcap\limits_{i=1}^n S_i$ и 
$$
\partial_S f(x) = \sum\limits_{i=1}^n a_i \partial_{S_i} f_i(x).
$$
:::
:::

. . .

::: {.column width="50%"}
::: {.callout-theorem}

### Теорема Дубовицкого — Милютина (субдифференциал поточечного максимума) 

Пусть $f_i(x)$ — выпуклые функции на открытом выпуклом множестве $S \subseteq \mathbb{R}^n, \; x_0 \in S$, и поточечный максимум определяется как $f(x) = \underset{i}{\operatorname{max}} f_i(x)$. Тогда:
$$
\partial_S f(x_0) = \mathbf{conv}\left\{  \bigcup\limits_{i \in I(x_0)} \partial_S f_i(x_0) \right\},
$$
$$
\quad I(x) = \{ i \in \{1, \dots, m\}: f_i(x) = f(x)\}
$$
:::
:::
::::

## Вычисление субдифференциала

* $\partial (\alpha f)(x) = \alpha \partial f(x)$ для $\alpha \geq 0$
* $\partial (\sum f_i)(x) = \sum \partial f_i (x)$, $f_i$ — выпуклые функции
* $\partial (f(Ax + b))(x) = A^T\partial f(Ax + b)$, $f$ — выпуклая функция
* $z \in \partial f(x)$ тогда и только тогда, когда $x \in \partial f^*(z)$.

# Субградиентный метод

## Алгоритм

Вектор $g$ называется **субградиентом** функции $f(x): S \to \mathbb{R}$ в точке $x_0$, если $\forall x \in S$:
$$
f(x) \geq f(x_0) + \langle g, x - x_0 \rangle
$$

. . .

Идея метода: заменить градиент $\nabla f(x_k)$ на произвольный субградиент $g_k \in \partial f(x_k)$:
$$
x_{k+1} = x_k - \alpha_k g_k,
$$
где $g_k$ — произвольный субградиент функции $f(x)$ в точке $x_k$.

. . .

Субградиентный метод не является методом спуска: значение функции может расти ($f(x_{k+1}) > f(x_k)$), так как антисубградиент не обязательно является направлением убывания.

Поэтому в качестве приближения решения берут лучшее найденное значение:
$$
f_k^{\text{best}} = \min\limits_{i=1,\ldots,k} f(x_i).
$$

## Сходимость

:::: {.columns}

::: {.column width="60%"}

$$
\begin{split}
\uncover<+->{\| x_{k+1} - x^* \|^2 & = \|x_k - x^* - \alpha_k g_k\|^2 = \\ }
\uncover<+->{& =   \| x_k - x^* \|^2 + \alpha_k^2 \|g_k\|^2 - 2 \alpha_k \langle g_k, x_k - x^* \rangle \\ }
\uncover<+->{&\leq \| x_k - x^* \|^2 + \alpha_k^2 \|g_k\|^2 - 2 \alpha_k (f(x_k) - f(x^*)) \\ }
\uncover<+->{2 \alpha_k (f(x_k) - f(x^*)) &\leq \| x_k - x^* \|^2 - \| x_{k+1} - x^* \|^2 + \alpha_k^2 \|g_k\|^2 }
\end{split}
$$

. . .

Просуммируем неравенство по $k$ от $0$ до $T-1$:
$$
\begin{split}
\uncover<+->{ \sum\limits_{k = 0}^{T-1}2\alpha_k (f(x_k) - f(x^*)) &\leq  \| x_0 - x^* \|^2 - \| x_{T} - x^* \|^2 + \sum\limits_{k=0}^{T-1}\alpha_k^2 \|g_k\|^2 \\ }
\uncover<+->{ &\leq \| x_0 - x^* \|^2 + \sum\limits_{k=0}^{T-1}\alpha_k^2 \|g_k\|^2 \\ }
\uncover<+->{&\leq R^2 + G^2\sum\limits_{k=0}^{T-1}\alpha_k^2}
\end{split}
$$

:::
::: {.column width="40%"}

* Оценим расстояние до оптимума $x^* = \arg\min\limits_{x \in \mathbb{R}^n} f(x)$ на последней итерации.
* Для субградиента: $\langle g_k, x^* - x_k \rangle \leq f(x^*) - f(x_k)$.
* Дополнительно предположим, что $\|g_k\|^2 \leq G^2$.
* Используем обозначение $R = \|x_0 - x^*\|_2$.
:::
::::

## Сходимость

* Заметим, что:
 $$
    \sum\limits_{k = 0}^{T-1}2\alpha_k (f(x_k) - f(x^*)) \geq
    \sum\limits_{k = 0}^{T-1}2\alpha_k (f_k^{\text{best}} - f(x^*)) = 
    (f_k^{\text{best}} - f(x^*))\sum\limits_{k = 0}^{T-1}2\alpha_k
    $$
* Это приводит к основной оценке сходимости:
 $$
    \boxed{
    f_T^{\text{best}} - f(x^*) \leq \frac{R^2 + G^2\sum\limits_{k=0}^{T-1}\alpha_k^2}{2\sum\limits_{k = 0}^{T-1}\alpha_k}}
    $$
* Отсюда следует сходимость метода при выполнении условий:
 $$
    \sum\limits_{k = 0}^{T-1}\alpha_k^2 < \infty, \quad \sum\limits_{k = 0}^{T-1}\alpha_k = \infty,
    $$
 то субградиентный метод сходится (шаг должен быть убывающим, но не слишком быстрым).

## Различные стратегии выбора шага

![](subgrad_step_sizes.pdf){fig-align="center" width=88%}

## Различные стратегии выбора шага

![](subgrad_fraction.pdf){fig-align="center" width=88%}

## Сходимость: выпуклый случай, постоянный шаг

:::{.callout-theorem}
Пусть $f$ — выпуклая $G$-липшицева функция и $R = \|x_0 - x^*\|_2$. Для постоянного шага $\alpha$, субградиентный метод удовлетворяет
$$
f_k^{\text{best}} - f(x^*) \leq \frac{R^2}{2\alpha k} + \frac{\alpha}{2}G^2
$$
:::

* Заметим, что с любым постоянным шагом первое слагаемое правой части убывает, но второе остается постоянным.
* Замечание: существуют вариации метода, работающие и без предположения об ограниченности субградиентов (например, с нормировкой шага). См. ^[B. Polyak. Introduction to Optimization. Optimization Software, Inc., 1987.] или ^[N. Shor. Minimization Methods for Non-diﬀerentiable Functions. Springer Series in Computational Mathematics. Springer, 1985.].
* Найдем оптимальный шаг $\alpha$, который минимизирует правую часть неравенства.

## Сходимость: выпуклый случай, постоянный шаг

:::{.callout-theorem}
Пусть $f$ — выпуклая $G$-липшицева функция и $R = \|x_0 - x^*\|_2$. Для постоянного шага $\alpha = \frac{R}{G}\sqrt{\frac{1}{k}}$, субградиентный метод удовлетворяет
$$
f_k^{\text{best}} - f(x^*) \leq \frac{G R}{\sqrt{k}}
$$
:::

* Эта версия требует знания числа итераций заранее, что не всегда практично.
* Примечательно, что оптимальный постоянный шаг дает ту же оценку, что и оптимальная последовательность шагов.
* Это связано с симметрией и выпуклостью правой части относительно $\alpha_i$.

## Сходимость: выпуклый случай, постоянный шаг

:::{.callout-theorem}
Пусть $f$ — выпуклая $G$-липшицева функция и $R = \|x_0 - x^*\|_2$. Для постоянного шага $\gamma = \alpha_k \|g_k\|_2$, т.е. $\alpha_k = \frac{\gamma}{\|g_k\|_2}$, субградиентный метод удовлетворяет
$$
f_k^{\text{best}} - f(x^*) \leq \frac{GR^2}{2\gamma k} + \frac{G \gamma}{2}
$$
:::

* Критерий остановки: норма субградиента не подходит (пример $f(x)=|x|$). Обычно используют фиксированное число итераций.

## Сходимость: выпуклый случай, практическая стратегия

:::{.callout-theorem}
Пусть $f$ — выпуклая $G$-липшицева функция и $R = \|x_0 - x^*\|_2$. Для убывающей стратегии шага $\alpha_k = \frac{R}{G\sqrt{k+1}}$, субградиентный метод удовлетворяет
$$
f_k^{\mathrm{best}} - f(x^*) \le \frac{GR(2+\ln k)}{4\sqrt{k+1}}
$$
:::
1. Оценка сумм:
   $$
   \uncover<+->{\sum_{i=0}^{k-1}\alpha_i^2 = \frac{R^2}{G^2}\sum_{i=1}^{k}\frac{1}{i} \le \frac{R^2}{G^2}\bigl(1+\ln k\bigr);} \uncover<+->{\qquad \sum_{i=0}^{k-1}\alpha_i = \frac{R}{G}\sum_{i=1}^{k}\frac{1}{\sqrt{i}} \ge \frac{R}{G}\int_{1}^{k+1}\frac{1}{\sqrt{t}}\,dt = \frac{2R}{G}\bigl(\sqrt{k+1}-1\bigr).}
   \uncover<+->{f_k^{\text{best}} - f(x^*) \leq \frac{R^2 + G^2\sum\limits_{i=0}^{k-1}\alpha_i^2}{2\sum\limits_{i = 0}^{k-1}\alpha_i}}\uncover<+->{ \leq \frac{R^2 + R^2 (1 + \ln k)}{4\frac{R}{G}\bigl(\sqrt{k+1}\bigr)}}\uncover<+->{ = \frac{GR(2 + \ln k)}{4\sqrt{k+1}}}
    $$

## Сильно выпуклый случай

![](non_smooth_convex_strongly_convex_ru.pdf){fig-align="center"}

. . .

:::: {.columns}

::: {.column width="50%"}
$$
\mathcal{O}\left(\frac{1}{\sqrt{k}}\right)
$$
:::

::: {.column width="50%"}
$$
\mathcal{O}\left(\frac{1}{k}\right)
$$
:::

::::

## Негладкий сильно выпуклый случай

:::{.callout-theorem}
Пусть $f$ — $\mu$-сильно выпуклая на выпуклом множестве и $x, y$ — произвольные точки. Тогда для любого $g\in\partial f(x)$,
$$
\langle g,x-y\rangle \ge f(x)-f(y)+\frac{\mu}{2}\|x-y\|^2.
$$
:::

1. Для любого $\lambda\in[0,1)$, из $\mu$-сильной выпуклости,
   $$
   f(\lambda x+(1-\lambda)y) \le \lambda f(x)+(1-\lambda)f(y)-\frac{\mu}{2}\lambda(1-\lambda)\|x-y\|^2.
   $$
1. Из неравенства субградиента в $x$, мы получаем
   $$
    f(\lambda x+(1-\lambda)y) \ge f(x) + \langle g,\lambda x+(1-\lambda)y-x\rangle \quad \to \quad f(\lambda x+(1-\lambda)y) \ge f(x)-(1-\lambda)\langle g,x-y\rangle.
    $$
1. Таким образом,
   $$
   \begin{aligned}
   f(x)-(1-\lambda)\langle g,x-y\rangle &\le \lambda f(x)+(1-\lambda)f(y)-\frac{\mu}{2}\lambda(1-\lambda)\|x-y\|^2 \\
   (1 - \lambda) f(x) &\le (1 - \lambda) f(y) + (1 - \lambda) \langle g,x-y\rangle - \frac{\mu}{2}\lambda(1-\lambda)\|x-y\|^2 \\
   f(x) &\le f(y) + \langle g,x-y\rangle - \frac{\mu}{2}\lambda\|x-y\|^2 \\
   \end{aligned}
   $$
1. Переходя к пределу при $\lambda\to 1^-$ получаем $f(x) \le f(y) + \langle g,x-y\rangle - \frac{\mu}{2}\|x-y\|^2 \to \langle g,x-y\rangle \ge f(x)-f(y)+\frac{\mu}{2}\|x-y\|^2$.

## Сходимость: сильно выпуклый случай

:::{.callout-theorem}
Пусть $f$ — $\mu$-сильно выпуклая функция (возможно негладкая) с минимумом $x^*$. Предположим, что субградиенты ограничены на траектории: $\|g_k\| \le G$. Используя шаг $\alpha_k = \frac{2}{\mu (k+1)}$, субградиентный метод гарантирует для $k > 0$ что:
$$
f_k^{\text{best}} - f(x^*) \leq \frac{2G^2}{\mu k}.
$$
:::

1. Начнем с формулировки метода как раньше:
 $$
    \begin{aligned}
    \uncover<+->{\| x_{k+1} - x^* \|^2 & = \|x_k - x^* - \alpha_k g_k\|^2 = \\ }
    \uncover<+->{& =   \| x_k - x^* \|^2 + \alpha_k^2 \|g_k\|^2 - 2 \alpha_k \langle g_k, x_k - x^* \rangle \\ }
    \uncover<+->{&\leq \| x_k - x^* \|^2 + \alpha_k^2 \|g_k\|^2 - 2 \alpha_k (f(x_k) - f(x^*)) - \alpha_k \mu  \|x_k - x^*\|^2 \\} 
    \uncover<+->{&= (1 - \mu \alpha_k)\| x_k - x^* \|^2 + \alpha_k^2 \|g_k\|^2 - 2 \alpha_k \left(f(x_k) - f(x^*)\right) \\}
    \uncover<+->{2\alpha_k\left(f(x_k)-f(x^*)\right) &\le (1-\mu\alpha_k)\|x_k-x^*\|^2-\|x_{k+1}-x^*\|^2+\alpha_k^2\|g_k\|^2 \\}
    \uncover<+->{f(x_k)-f(x^*) &\le \frac{1-\mu\alpha_k}{2\alpha_k}\|x_k-x^*\|^2-\frac{1}{2\alpha_k}\|x_{k+1}-x^*\|^2+\frac{\alpha_k}{2}\|g_k\|^2}
    \end{aligned}
    $$

## Сходимость: сильно выпуклый случай. Доказательство

2. Подставим шаг $\alpha_k = \frac{2}{\mu (k+1)}$ в неравенство:
 $$
    \begin{aligned}
    \uncover<+->{f(x_k)-f(x^*) &\le \frac{\mu (k-1)}{4}\|x_k-x^*\|^2-\frac{\mu(k+1)}{4}\|x_{k+1}-x^*\|^2+\frac{1}{\mu(k+1)}\|g_k\|^2 \\}
    \uncover<+->{f(x_k)-f(x^*) &\le \frac{\mu (k-1)}{4}\|x_k-x^*\|^2-\frac{\mu(k+1)}{4}\|x_{k+1}-x^*\|^2+\frac{1}{\mu k}\|g_k\|^2 \\}
    \uncover<+->{k \left(f(x_k)-f(x^*)\right) &\le \frac{\mu k(k-1)}{4}\|x_k-x^*\|^2-\frac{\mu k(k+1)}{4}\|x_{k+1}-x^*\|^2+\frac{1}{\mu}\|g_k\|^2}
    \end{aligned}
    $$

3. Суммируя неравенства для всех $k = 0, 1, \ldots, T-1$, мы получаем:
 $$
    \begin{aligned}
    \uncover<+->{\sum_{k=0}^{T-1} k \left(f(x_k)-f(x^*)\right) &\le 0 -\frac{\mu (T-1)T}{4}\|x_{T}-x^*\|^2+\frac{1}{\mu}\sum_{k=0}^{T-1}\|g_k\|^2}\uncover<+->{ \leq \frac{G^2 T}{\mu} \\}
    \uncover<+->{\left(f^{\text{best}}_{T-1}-f(x^*)\right) \sum_{k=0}^{T-1} k = \sum_{k=0}^{T-1} k \left(f^{\text{best}}_{T-1}-f(x^*)\right) }\uncover<+->{&\le \sum_{k=0}^{T-1} k \left(f(x_k)-f(x^*)\right)}\uncover<+->{ \leq \frac{G^2 T}{\mu} \\}
    \uncover<+->{f^{\text{best}}_{T-1}-f(x^*) &\leq \frac{G^2 T}{\mu \sum_{k=0}^{T-1} k }}\uncover<+->{ = \frac{2G^2 T}{\mu T (T-1) }}\uncover<+->{ \qquad  f_k^{\text{best}} - f(x^*) \leq \frac{2G^2}{\mu k}.}
    \end{aligned}
    $$

## Summary. Субградиентный метод

| Тип задачи                          | Стратегия шага          | Скорость сходимости   | Сложность итераций |
|:---------------------------------------:|:----------------------:|:------------------:|:--------------------:|
| Выпуклые липшицевы функции          | $\alpha \sim \dfrac{1}{\sqrt{k}}$ | $\mathcal{O}\left(\dfrac{1}{\sqrt{k}}\right)$  | $\mathcal{O}\left(\dfrac{1}{\varepsilon^2}\right)$ |
| Сильно выпуклые функции | $\alpha \sim \dfrac{1}{k}$        | $\mathcal{O}\left(\dfrac{1}{k}\right)$          | $\mathcal{O}\left(\dfrac{1}{\varepsilon}\right)$   |


## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Гладкий выпуклый случай. Сублинейная сходимость, не сходится в области определения](lasso_m_1000_n_100_mu_0_lambda_0.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий выпуклый случай. Маленькое значение $\lambda$ приводит к негладкости. Не сходится с постоянным шагом](lasso_m_1000_n_100_mu_0_lambda_0.1.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий выпуклый случай. При большем значении $\lambda$ проявляется немонотонность $f(x_k)$. Видно, что меньшая постоянная длина шага приводит к более низкому стационарному уровню функции.](lasso_m_1000_n_100_mu_0_lambda_1.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий выпуклый случай. Убывающая длина шага приводит к сходимости для $f^{\text{best}}_k$](lasso_m_100_n_100_mu_0_lambda_1_k.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий выпуклый случай. $\frac{\alpha_0}{\sqrt{k}}$ шаг приводит к сходимости для $f^{\text{best}}_k$](lasso_m_100_n_100_mu_0_lambda_1_sqrtk.pdf)


## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий выпуклый случай. $\frac{\alpha_0}{\sqrt{k}}$ шаг приводит к сходимости для $f^{\text{best}}_k$](lasso_m_100_n_100_mu_0_lambda_1_sqrtk_long.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий сильно выпуклый случай. $\frac{\alpha_0}{k}$ шаг приводит к сходимости для $f^{\text{best}}_k$](lasso_m_100_n_100_mu_1_lambda_1_1_k.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий сильно выпуклый случай. Шаг $\frac{\alpha_0}{\sqrt{k}}$ работает хуже](lasso_m_100_n_100_mu_1_lambda_1_1_sqrtk.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{m}\sum_{i=1}^{m} \log(1 + \exp(-b_i(A_i x))) + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A_i \in \mathbb{R}^n, \quad b_i \in \{-1,1\}
$$

![Логистическая регрессия с $\ell_1$ регуляризацией](logreg_lasso_small_short.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{m}\sum_{i=1}^{m} \log(1 + \exp(-b_i(A_i x))) + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A_i \in \mathbb{R}^n, \quad b_i \in \{-1,1\}
$$

![Логистическая регрессия с $\ell_1$ регуляризацией](logreg_lasso_small_long.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{m}\sum_{i=1}^{m} \log(1 + \exp(-b_i(A_i x))) + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A_i \in \mathbb{R}^n, \quad b_i \in \{-1,1\}
$$

![Логистическая регрессия с $\ell_1$ регуляризацией](logreg_lasso_medium_short.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{m}\sum_{i=1}^{m} \log(1 + \exp(-b_i(A_i x))) + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A_i \in \mathbb{R}^n, \quad b_i \in \{-1,1\}
$$

![Логистическая регрессия с $\ell_1$ регуляризацией](logreg_lasso_medium_long.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{m}\sum_{i=1}^{m} \log(1 + \exp(-b_i(A_i x))) + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A_i \in \mathbb{R}^n, \quad b_i \in \{-1,1\}
$$

![Логистическая регрессия с $\ell_1$ регуляризацией](logreg_lasso_high_short.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{m}\sum_{i=1}^{m} \log(1 + \exp(-b_i(A_i x))) + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A_i \in \mathbb{R}^n, \quad b_i \in \{-1,1\}
$$

![Логистическая регрессия с $\ell_1$ регуляризацией](logreg_lasso_high_long.pdf)


# Экстра: Нижние оценки

## Нижние оценки

| выпуклые (негладкие) ^[[Nesterov, Lectures on Convex Optimization](https://fmin.xyz/assets/files/Nesterov_the_best.pdf)] | гладкие (невыпуклые)^[[Carmon, Duchi, Hinder, Sidford, 2017](https://arxiv.org/pdf/1710.11606.pdf)] | гладкие и выпуклые^[[Nemirovski, Yudin, 1979](https://fmin.xyz/assets/files/nemyud1979.pdf)] | гладкие и сильно выпуклые (или PL)$^1$ |
|:-----:|:-----:|:----:|:---------:|
| $\mathcal{O} \left( \dfrac{1}{\sqrt{k}} \right)$ | $\mathcal{O} \left( \dfrac{1}{k^2} \right)$ |  $\mathcal{O} \left( \dfrac{1}{k^2} \right)$ | $\mathcal{O} \left( \left(\dfrac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^k \right)$ |
| $k_\varepsilon \sim  \mathcal{O} \left( \dfrac{1}{\varepsilon^2} \right)$  | $k_\varepsilon \sim  \mathcal{O}  \left( \dfrac{1}{\sqrt{\varepsilon}} \right)$ | $k_\varepsilon \sim  \mathcal{O}  \left( \dfrac{1}{\sqrt{\varepsilon}} \right)$ | $k_\varepsilon  \sim \mathcal{O} \left( \sqrt{\kappa} \log \dfrac{1}{{\varepsilon}}\right)$ |

## Модель черного ящика

Итерация градиентного спуска:
$$
\begin{aligned}
x_{k+1} &= x_k - \alpha_k \nabla f(x_k)\\
&= x_{k-1} - \alpha_{k-1} \nabla f(x_{k-1}) - \alpha_k \nabla f(x_k) \\
& \;\;\vdots \\
&= x_0 - \sum\limits_{i=0}^k \alpha_{k-i} \nabla f(x_{k-i})
\end{aligned}
$$

. . .

Рассмотрим семейство методов первого порядка, где
$$
\begin{aligned}
x_{k+1} &\in x_0 + \operatorname{span} \left\{\nabla f(x_{0}), \nabla f(x_{1}), \ldots, \nabla f(x_{k})\right\} \; & f \text{ - smooth} \\
x_{k+1} &\in x_0 + \operatorname{span} \left\{g_{0}, g_{1}, \ldots, g_{k}\right\} \text{, where }
g_{i} \in \partial f(x_{i}) \; & f \text{ - non-smooth}
\end{aligned}
$$ {#eq-fom}

. . .

Для получения нижней оценки построим «худшую» функцию $f$ из соответствующего класса, на которой любой метод из семейства @eq-fom сходится медленно.



## Негладкий выпуклый случай

:::{.callout-theorem}
Существует функция $f$, которая является $G$‑липшицевой и выпуклой, такая, что любой метод [@eq-fom] удовлетворяет
$$
\min_{1 \le i \le k} f(x_i) - \min_{x \in \mathbb{B}(R)} f(x) \geq \frac{GR}{2(1 + \sqrt{k})}
$$
для $R > 0$ и $k \leq n$, где $n$ — размерность задачи.
:::

. . .

**Идея доказательства:** построить такую функцию $f$, что для любого метода [@eq-fom] получаем
$$
x_k \in \operatorname{span} \left\{e_{1}, e_{2}, \ldots, e_{k}\right\}
$$
где $e_i$ — $i$-й стандартный базисный вектор. На итерации $k\leq n$, есть по крайней мере $n-k$ координат $x$, равных $0$. Это позволяет нам получить оценку на ошибку.

## Негладкий выпуклый случай (доказательство)

Рассмотрим функцию:
$$
f(x) = \beta \max_{1 \le i \le k} x^{(i)} + \frac{\alpha}{2} \|x\|_2^2,
$$
где $\alpha, \beta \in \mathbb{R}$ — параметры, и $x^{(1)}, \dots, x^{(k)}$ — первые $k$ компонент $x$.

. . .

**Свойства:**

* Функция $f(x)$ является $\alpha$‑сильно выпуклой из-за квадратичного члена $\frac{\alpha}{2} \|x\|_2^2$.
* Функция негладкая, поскольку первое слагаемое вносит недифференцируемость в точке, соответствующей максимальной координате $x$.

. . .

Рассмотрим субдифференциал $f(x)$ в $x$:

:::: {.columns}

::: {.column width="50%"}
$$
\begin{aligned}
\partial f(x) &=   \partial \left( \beta\max_{1 \le i \le k} x^{(i)} \right) + \partial \left( \frac{\alpha}{2} \|x\|_2^2 \right) \\
&=\beta \partial \left(\max_{1 \le i \le k} x^{(i)} \right) + \alpha x\\
&= \beta \operatorname{conv}\left\{e_i \mid i: x^{(i)} =  \max_{j} x^{(j)} \right\} + \alpha x
\end{aligned}
$$

:::

. . .

::: {.column width="50%"}
Легко видеть, что если $g \in \partial f(x)$ и $\|x\|\leq R$, то
$$
\|g\| \leq \alpha R + \beta
$$

Таким образом, $f$ является $\alpha R + \beta$‑липшицевой на $B(R)$.
:::
::::

## Негладкий выпуклый случай (доказательство)

Далее опишем оракул первого порядка для этой функции. При запросе субградиента в точке $x$ оракул возвращает:
$$
\alpha x + \gamma e_{i},
$$
где $i$ — *наименьший* индекс, на котором достигается максимум $x^{(i)} = \max_{1 \leq j \leq k} x^{(j)}$. 

* Пусть $x_0 = 0$.
* При запросе оракула в $x_0=0$, он возвращает $e_1$. Следовательно, $x_1$ лежит на прямой, порождённой $e_1$. 
* Индукцией по $i$ можно показать, что $x_i \in \operatorname{span}\{e_1, \dots, e_i\}$. В частности, для $i \leq k$, $k+1$-я координата $x_i$ равна нулю и вследствие структуры $f(x)$:
 $$
    f(x_i) \geq 0.
    $$

## Негладкий выпуклый случай (доказательство)

* Найдем минимум функции $f$. Определим точку $y\in\mathbb{R}^n$:
 $$
    y^{(i)} = - \frac{\beta}{\alpha k} \quad \text{for } 1 \leq i \leq k,\qquad y^{(i)} = 0 \quad \text{for } k+1 \leq i \leq n.
    $$
* Заметим, что $0 \in \partial f(y)$:
 $$
    \begin{aligned}
    \partial f(y) &= \alpha y + \beta \operatorname{conv}\left\{e_i \mid i: y^{(i)} =  \max_{j} y^{(j)} \right\} \\
    &= \alpha y + \beta \operatorname{conv}\left\{e_i \mid i: y^{(i)} =  0 \right\} \\
    0 &\in \partial f(y).
    \end{aligned}
    $$
* Следовательно, минимальное значение $f = f(y) = f(x^*)$ равно
 $$
    f(y) = - \frac{\beta^2}{\alpha k} + \frac{\alpha}{2} \cdot \frac{\beta^2}{\alpha^2 k} = - \frac{\beta^2}{2 \alpha k}.
    $$ 
* Теперь мы получаем:
 $$
    f(x_i) - f(x^*) \geq 0 - \left( - \frac{\beta^2}{2 \alpha k} \right) \geq \frac{\beta^2}{2 \alpha k}.
    $$

## Негладкий выпуклый случай (доказательство)

Имеем оценку снизу $f(x_i) - f(x^*) \geq \frac{\beta^2}{2 \alpha k}$, что (при правильном выборе параметров) совпадает с требуемой $\frac{GR}{2(1 + \sqrt{k})}$.  

. . .

:::: {.columns}

::: {.column width="50%"}
### Выпуклый случай
$$
\alpha = \frac{G}{R}\frac{1}{1 + \sqrt{k}} \quad \beta = \frac{\sqrt{k}}{1 + \sqrt{k}}
$$

$$
\frac{\beta^2}{2\alpha} = \frac{GRk}{2(1 + \sqrt{k})}
$$
Заметим, в частности, что $\|y\|_2^2 = \frac{\beta^2}{\alpha^2 k} = R^2$ с этими параметрами

$$
\min\limits_{1 \le i \le k} f(x_i) - f(x^*) \geq \frac{\beta^2}{2 \alpha k} =\frac{GR}{2(1 + \sqrt{k})}
$$
:::

. . .

::: {.column width="50%"}
### Сильно выпуклый случай
$$
\alpha = \frac{G}{2R} \quad \beta = \frac{G}{2}
$$
Заметим, в частности, что $\|y\|_2^2 = \frac{\beta^2}{\alpha^2 k} = \frac{G^2}{4\alpha^2 k} = R^2$ с этими параметрами

$$
\min\limits_{1 \le i \le k} f(x_i) - f(x^*) \geq \frac{G^2}{8\alpha k}
$$
:::

::::

## Ссылки

* [Subgradient Methods Stephen Boyd (with help from Jaehyun Park)](https://web.stanford.edu/class/ee364b/lectures/subgrad_method_notes.pdf)