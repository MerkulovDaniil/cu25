---
title: "Невыпуклая оптимизация. Нижние оценки. Субградиентный спуск."
author: Даня Меркулов
institute: Оптимизация для всех! ЦУ
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
 - \newcommand{\bgimage}{../files/back13.jpeg}
---

# Негладкие задачи

## Задача наименьших квадратов с $\ell_1$- регуляризацией

[![](l1_regularization.jpeg)](https://fmin.xyz/assets/Notebooks/Regularization_horizontal.mp4)

## Нормы не являются гладкими

$$
\min_{x \in \mathbb{R}^n} f(x),
$$

Рассмотрим классическую выпуклую задачу оптимизации. Мы предполагаем, что $f(x)$ является выпуклой функцией, но теперь мы не требуем гладкости. 

![Нормы конусов для разных $p$ — нормы не являются гладкими](norm_cones.pdf){width=90%}

## Пример Вульфа

![Пример Вульфа. [\faPython Открыть в Colab](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/subgrad.ipynb)](wolfe_3d.pdf){width=90%}

# Вычисление субградиента

## Линейная нижняя оценка выпуклых функций

:::: {.columns}

::: {.column width="60%"}
![Линейная аппроксимация Тейлора служит глобальной нижней оценкой для выпуклой функции](Subgrad.pdf)
:::

::: {.column width="40%"}
Важное свойство непрерывной выпуклой функции $f(x)$ заключается в том, что для любой выбранной точки $x_0$ для всех $x \in \text{dom } f$ выполняется неравенство:
$$
f(x) \geq f(x_0) + \langle g, x - x_0 \rangle
$$

. . .

для некоторого вектора $g$, т.е. касательная к графику функции является *глобальной* нижней оценкой для функции. 

* Если $f(x)$ дифференцируема, то $g = \nabla f(x_0)$.
* Не все непрерывные выпуклые функции дифференцируемы.

. . .

Мы не хотим потерять такое удобное свойство.
:::

::::

## Субградиент и субдифференциал

Вектор $g$ называется **субградиентом** функции $f(x): S \to \mathbb{R}$ в точке $x_0$, если $\forall x \in S$:
$$
f(x) \geq f(x_0) + \langle g, x - x_0 \rangle
$$

. . .

Множество всех субградиентов функции $f(x)$ в точке $x_0$ называется **субдифференциалом** функции $f$ в точке $x_0$ и обозначается $\partial f(x_0)$.

. . .

![Субдифференциал — это множество всех возможных субградиентов](Subdifferential.pdf)

## Субградиент и субдифференциал

Найдите $\partial f(x)$, если $f(x) = |x|$

. . .

![Субдифференциал $\vert x \vert$](subgradmod.pdf){width=85%}

## Свойства субдифференциала

:::: {.columns}
::: {.column width="50%"}
* Если $x_0 \in \mathbf{ri }(S)$, то $\partial f(x_0)$ является выпуклым компактным множеством.
* Выпуклая функция $f(x)$ дифференцируема в точке $x_0\Rightarrow \partial f(x_0) = \{\nabla f(x_0)\}$.
* Если $\partial f(x_0) \neq \emptyset \quad \forall x_0 \in S$, то $f(x)$ выпукла на $S$.

. . .


::: {.callout-theorem}

### Субдифференциал дифференцируемой функции

Пусть $f : S \to \mathbb{R}$ — функция, определенная на множестве $S$ в евклидовом пространстве $\mathbb{R}^n$. Если $x_0 \in \mathbf{ri }(S)$ и $f$ дифференцируема в точке $x_0$, то либо $\partial f(x_0) = \emptyset$ либо $\partial f(x_0) = \{\nabla f(x_0)\}$. Более того, если функция $f$ выпукла, то первая ситуация невозможна.
:::
:::

. . .

::: {.column width="50%"}

**Доказательство**

1. Пусть $s \in \partial f(x_0)$ для некоторого $s \in \mathbb{R}^n$ отличного от $\nabla f(x_0)$. Пусть $v \in  \mathbb{R}^n$ — единичный вектор. Поскольку $x_0$ является внутренней точкой множества $S$, существует $\delta > 0$ такое, что $x_0 + tv \in S$ для всех $0 < t < \delta$. По определению субградиента:
 $$
    f(x_0 + tv) \geq f(x_0) + t \langle s, v \rangle
 $$

. . .

что влечёт:
$$
\frac{f(x_0 + tv) - f(x_0)}{t} \geq \langle s, v \rangle
$$
для всех $0 < t < \delta$. Переходя к пределу при $t \to 0$ и используя определение градиента, получаем:
$$
\langle \nabla f(x_0), v \rangle = \lim_{{t \to 0; 0 < t < \delta}} \frac{f(x_0 + tv) - f(x_0)}{t} \geq \langle s, v \rangle
$$
:::
::::

## Свойства субдифференциала

2. Отсюда $\langle s - \nabla f(x_0), v \rangle \geq 0$. В силу произвольности $v$ можно выбрать 
 $$
    v = -\frac{s - \nabla f(x_0)}{\| s - \nabla f(x_0) \|},
    $$ 
 которое приводит к $s = \nabla f(x_0)$.
3. Более того, если функция $f$ выпукла, то согласно дифференциальному условию выпуклости $f(x) \geq f(x_0) + \langle \nabla f(x_0), x - x_0 \rangle$ для всех $x \in S$. Но по определению это означает, что $\nabla f(x_0) \in \partial f(x_0)$.


## Вычисление субдифференциалов

:::: {.columns}
::: {.column width="50%"}
:::{.callout-theorem}
### Теорема Моро — Роккафеллара (субдифференциал линейной комбинации)
Пусть $f_i(x)$ — выпуклые функции на выпуклых множествах $S_i, \; i = \overline{1,n}$. Тогда если $\bigcap\limits_{i=1}^n \mathbf{ri } (S_i) \neq \emptyset$, то функция $f(x) = \sum\limits_{i=1}^n a_i f_i(x), \; a_i > 0$ имеет субдифференциал $\partial_S f(x)$ на множестве $S = \bigcap\limits_{i=1}^n S_i$ и 
$$
\partial_S f(x) = \sum\limits_{i=1}^n a_i \partial_{S_i} f_i(x).
$$
:::
:::

. . .

::: {.column width="50%"}
::: {.callout-theorem}

### Теорема Дубовицкого — Милютина (субдифференциал поточечного максимума) 

Пусть $f_i(x)$ — выпуклые функции на открытом выпуклом множестве $S \subseteq \mathbb{R}^n, \; x_0 \in S$, и поточечный максимум определяется как $f(x) = \underset{i}{\operatorname{max}} f_i(x)$. Тогда:
$$
\partial_S f(x_0) = \mathbf{conv}\left\{  \bigcup\limits_{i \in I(x_0)} \partial_S f_i(x_0) \right\},
$$
$$
\quad I(x) = \{ i \in [1:m]: f_i(x) = f(x)\}
$$
:::
:::
::::

## Вычисление субдифференциала

* $\partial (\alpha f)(x) = \alpha \partial f(x)$, для $\alpha \geq 0$
* $\partial (\sum f_i)(x) = \sum \partial f_i (x)$, $f_i$ — выпуклые функции
* $\partial (f(Ax + b))(x) = A^T\partial f(Ax + b)$, $f$ — выпуклая функция
* $z \in \partial f(x)$ тогда и только тогда, когда $x \in \partial f^*(z)$.

# Субградиентный метод

## Алгоритм

Вектор $g$ называется **субградиентом** функции $f(x): S \to \mathbb{R}$ в точке $x_0$ если $\forall x \in S$:
$$
f(x) \geq f(x_0) + \langle g, x - x_0 \rangle
$$

. . .

Идея очень проста: заменим градиент $\nabla f(x_k)$ в методе градиентного спуска субградиентом $g_k$ в точке $x_k$:
$$
x_{k+1} = x_k - \alpha_k g_k,
$$
где $g_k$ — произвольный субградиент функции $f(x)$ в точке $x_k$, $g_k \in \partial f (x_k)$

. . .

Заметьте, что метод субградиента не гарантирует убывание: отрицательный субградиент может не быть направлением убывания, а выбор длины шага может привести к тому, что $f(x_{k+1}) > f(x_k)$. 

Поэтому мы обычно отслеживаем лучшее значение целевой функции 
$$
f_k^{\text{best}} = \min\limits_{i=1,\ldots,k} f(x_i).
$$

## Сходимость

:::: {.columns}

::: {.column width="60%"}

$$
\begin{split}
\uncover<+->{\| x_{k+1} - x^* \|^2 & = \|x_k - x^* - \alpha_k g_k\|^2 = \\ }
\uncover<+->{& =   \| x_k - x^* \|^2 + \alpha_k^2 \|g_k\|^2 - 2 \alpha_k \langle g_k, x_k - x^* \rangle \\ }
\uncover<+->{&\leq \| x_k - x^* \|^2 + \alpha_k^2 \|g_k\|^2 - 2 \alpha_k (f(x_k) - f(x^*)) \\ }
\uncover<+->{2 \alpha_k (f(x_k) - f(x^*)) &\leq \| x_k - x^* \|^2 - \| x_{k+1} - x^* \|^2 + \alpha_k^2 \|g_k\|^2 }
\end{split}
$$

. . .

Просуммируем полученное неравенство для $k = 0, \ldots, T-1$:
$$
\begin{split}
\uncover<+->{ \sum\limits_{k = 0}^{T-1}2\alpha_k (f(x_k) - f(x^*)) &\leq  \| x_0 - x^* \|^2 - \| x_{T} - x^* \|^2 + \sum\limits_{k=0}^{T-1}\alpha_k^2 \|g_k\|^2 \\ }
\uncover<+->{ &\leq \| x_0 - x^* \|^2 + \sum\limits_{k=0}^{T-1}\alpha_k^2 \|g_k\|^2 \\ }
\uncover<+->{&\leq R^2 + G^2\sum\limits_{k=0}^{T-1}\alpha_k^2}
\end{split}
$$

:::
::: {.column width="40%"}

* Запишем, насколько близко мы подошли к оптимуму $x^* = \text{arg}\min\limits_{x \in \mathbb{R}^n} f(x) = \text{arg} f^*$ на последней итерации:
* Для субградиента: $\langle g_k, x^* - x_k \rangle \leq f(x^*) - f(x_k)$.
* Дополнительно предположим, что $\|g_k\|^2 \leq G^2$
* Используем обозначение $R = \|x_0 - x^*\|_2$
:::
::::

## Сходимость

* Наконец, заметим:
 $$
    \sum\limits_{k = 0}^{T-1}2\alpha_k (f(x_k) - f(x^*)) \geq
    \sum\limits_{k = 0}^{T-1}2\alpha_k (f_k^{\text{best}} - f(x^*)) = 
    (f_k^{\text{best}} - f(x^*))\sum\limits_{k = 0}^{T-1}2\alpha_k
    $$
* Которое приводит к основному неравенству:
 $$
    \boxed{
    f_k^{\text{best}} - f(x^*) \leq \frac{R^2 + G^2\sum\limits_{k=0}^{T-1}\alpha_k^2}{2\sum\limits_{k = 0}^{T-1}\alpha_k}}
    $$
* Из этого мы можем видеть, что если стратегия шага такая, что
 $$
    \sum\limits_{k = 0}^{T-1}\alpha_k^2 < \infty, \quad \sum\limits_{k = 0}^{T-1}\alpha_k = \infty,
    $$
 то метод субградиента сходится (шаг должен быть убывающим, но не слишком быстрым).

## Разные стратегии шага

![](subgrad_step_sizes.pdf){fig-align="center" width=88%}

## Разные стратегии шага

![](subgrad_fraction.pdf){fig-align="center" width=88%}

## Оценки сходимости. Негладкий выпуклый случай. Постоянный шаг

:::{.callout-theorem}
Пусть $f$ — выпуклая $G$-липшицева функция и $R = \|x_0 - x^*\|_2$. Для постоянного шага $\alpha$, метод субградиента удовлетворяет
$$
f_k^{\text{best}} - f(x^*) \leq \frac{R^2}{2\alpha k} + \frac{\alpha}{2}G^2
$$
:::

* Заметим, что с любым постоянным шагом, первое слагаемое правой части убывает, но второе остается постоянным.
* Некоторые варианты метода субградиента (например, убывающие, но несуммируемые длины шагов) работают даже когда предположение $\|g_k\|_2 \leq G$ не выполняется; см. ^[B. Polyak. Introduction to Optimization. Optimization Software, Inc., 1987.] или ^[N. Shor. Minimization Methods for Non-diﬀerentiable Functions. Springer Series in Computational Mathematics. Springer, 1985.].
* Найдем оптимальный шаг $\alpha$ который минимизирует правую часть неравенства.

## Оценки сходимости. Негладкий выпуклый случай. Постоянный шаг

:::{.callout-theorem}
Пусть $f$ — выпуклая $G$-липшицева функция и $R = \|x_0 - x^*\|_2$. Для постоянного шага $\alpha = \frac{R}{G}\sqrt{\frac{1}{k}}$, метод субградиента удовлетворяет
$$
f_k^{\text{best}} - f(x^*) \leq \frac{G R}{\sqrt{k}}
$$
:::

* Эта версия требует знания числа итераций заранее, что не всегда практично.
* Интересно отметить, что если мы хотим найти оптимальные шаги для всей последовательности $\alpha_0, \alpha_1, \ldots, \alpha_{k-1}$, мы получим тот же результат.
* Почему? Потому что правая часть является выпуклой и **симметричной** функцией $\alpha_0, \alpha_1, \ldots, \alpha_{k-1}$.

## Оценки сходимости. Негладкий выпуклый случай. Постоянный шаг

:::{.callout-theorem}
Пусть $f$ — выпуклая $G$-липшицева функция и $R = \|x_0 - x^*\|_2$. Для постоянного шага $\gamma = \alpha_k \|g_k\|_2$, т.е. $\alpha_k = \frac{\gamma}{\|g_k\|_2}$, метод субградиента удовлетворяет
$$
f_k^{\text{best}} - f(x^*) \leq \frac{GR^2}{2\gamma k} + \frac{G \gamma}{2}
$$
:::

* Заметим, что для метода субградиента, мы обычно не можем использовать норму субградиента как критерий остановки (представьте $f(x) = |x|$). Существуют более продвинутые варианты критериев остановки, но из‑за очень медленной сходимости обычно просто задают максимальное число итераций.

## Оценки сходимости. Негладкий выпуклый случай. Практическая стратегия

:::{.callout-theorem}
Пусть $f$ — выпуклая $G$-липшицева функция и $R = \|x_0 - x^*\|_2$. Для убывающей стратегии шага $\alpha_k = \frac{R}{G\sqrt{k+1}}$, метод субградиента удовлетворяет
$$
f_k^{\mathrm{best}} - f(x^*) \le \frac{GR(2+\ln k)}{4\sqrt{k+1}}
$$
:::
1. Оценка сумм:
 $$
    \uncover<+->{\sum_{k=0}^{T-1}\alpha_k^2 = \frac{R^2}{G^2}\sum_{k=1}^{T}\frac{1}{k} \le \frac{R^2}{G^2}\bigl(1+\ln T\bigr);} \uncover<+->{\qquad \sum_{k=0}^{T-1}\alpha_k = \frac{R}{G}\sum_{k=1}^{T}\frac{1}{\sqrt{k}} \ge \frac{R}{G}\int_{1}^{T+1}\frac{1}{\sqrt{t}}\,dt = \frac{2R}{G}\bigl(\sqrt{T+1}-1\bigr).}
    $$
1. Мы отбрасываем последний $-1$ в верхней оценке выше и используем основное неравенство:
 $$
    \uncover<+->{f_T^{\text{best}} - f(x^*) \leq \frac{R^2 + G^2\sum\limits_{k=0}^{T-1}\alpha_k^2}{2\sum\limits_{k = 0}^{T-1}\alpha_k}}\uncover<+->{ \leq \frac{R^2 + R^2 (1 + \ln T)}{4\frac{R}{G}\bigl(\sqrt{T+1}\bigr)}}\uncover<+->{ = \frac{GR(2 + \ln T)}{4\sqrt{T+1}}}
    $$

## Негладкий сильно выпуклый случай

![](non_smooth_convex_strongly_convex_ru.pdf){fig-align="center"}

. . .

:::: {.columns}

::: {.column width="50%"}
$$
\mathcal{O}\left(\frac{1}{\sqrt{k}}\right)
$$
:::

::: {.column width="50%"}
$$
\mathcal{O}\left(\frac{1}{k}\right)
$$
:::

::::

## Негладкий сильно выпуклый случай

:::{.callout-theorem}
Пусть $f$ — $\mu$-сильно выпуклая на выпуклом множестве и $x, y$ — произвольные точки. Тогда для любого $g\in\partial f(x)$,
$$
\langle g,x-y\rangle \ge f(x)-f(y)+\frac{\mu}{2}\|x-y\|^2.
$$
:::

1. Для любого $\lambda\in[0,1)$, из $\mu$-сильной выпуклости,
 $$
    f(\lambda x+(1-\lambda)y) \le \lambda f(x)+(1-\lambda)f(y)-\frac{\mu}{2}\lambda(1-\lambda)\|x-y\|^2.
    $$
1. Из неравенства субградиента в $x$, мы получаем
 $$
    f(\lambda x+(1-\lambda)y) \ge f(x) + \langle g,\lambda x+(1-\lambda)y-x\rangle \quad \to \quad f(\lambda x+(1-\lambda)y) \ge f(x)-(1-\lambda)\langle g,x-y\rangle.
    $$
1. Таким образом,
 $$
    \begin{aligned}
    f(x)-(1-\lambda)\langle g,x-y\rangle &\le \lambda f(x)+(1-\lambda)f(y)-\frac{\mu}{2}\lambda(1-\lambda)\|x-y\|^2 \\
    (1 - \lambda) f(x) &\le (1 - \lambda) f(y) + (1 - \lambda) \langle g,x-y\rangle - \frac{\mu}{2}\lambda(1-\lambda)\|x-y\|^2 \\
    f(x) &\le f(y) + \langle g,x-y\rangle - \frac{\mu}{2}\lambda\|x-y\|^2 \\
    \end{aligned}
    $$
1. Переходя к пределу при $\lambda\to 1^-$ получаем $f(x) \le f(y) + \langle g,x-y\rangle - \frac{\mu}{2}\|x-y\|^2 \to \langle g,x-y\rangle \ge f(x)-f(y)+\frac{\mu}{2}\|x-y\|^2$.

## Оценки сходимости. Негладкий сильно выпуклый случай.

:::{.callout-theorem}
Пусть $f$ — $\mu$-сильно выпуклая функция (возможно негладкая) с минимумом $x^*$ и ограниченными субградиентами $\|g_k\| \le G$. Используя шаг $\alpha_k = \frac{2}{\mu (k+1)}$, метод субградиента гарантирует для $k > 0$ что:
$$
f_k^{\text{best}} - f(x^*) \leq \frac{2G^2}{\mu k}.
$$
:::

1. Начнем с формулировки метода как раньше:
 $$
    \begin{aligned}
    \uncover<+->{\| x_{k+1} - x^* \|^2 & = \|x_k - x^* - \alpha_k g_k\|^2 = \\ }
    \uncover<+->{& =   \| x_k - x^* \|^2 + \alpha_k^2 \|g_k\|^2 - 2 \alpha_k \langle g_k, x_k - x^* \rangle \\ }
    \uncover<+->{&\leq \| x_k - x^* \|^2 + \alpha_k^2 \|g_k\|^2 - 2 \alpha_k (f(x_k) - f(x^*)) - \alpha_k \mu  \|x_k - x^*\|^2 \\} 
    \uncover<+->{&= (1 - \mu \alpha_k)\| x_k - x^* \|^2 + \alpha_k^2 \|g_k\|^2 - 2 \alpha_k \left(f(x_k) - f(x^*)\right) \\}
    \uncover<+->{2\alpha_k\left(f(x_k)-f(x^*)\right) &\le (1-\mu\alpha_k)\|x_k-x^*\|^2-\|x_{k+1}-x^*\|^2+\alpha_k^2\|g_k\|^2 \\}
    \uncover<+->{f(x_k)-f(x^*) &\le \frac{1-\mu\alpha_k}{2\alpha_k}\|x_k-x^*\|^2-\frac{1}{2\alpha_k}\|x_{k+1}-x^*\|^2+\frac{\alpha_k}{2}\|g_k\|^2}
    \end{aligned}
    $$

## Оценки сходимости. Негладкий сильно выпуклый случай. Доказательство

2. Подставим шаг $\alpha_k = \frac{2}{\mu (k+1)}$ в неравенство:
 $$
    \begin{aligned}
    \uncover<+->{f(x_k)-f(x^*) &\le \frac{\mu (k-1)}{4}\|x_k-x^*\|^2-\frac{\mu(k+1)}{4}\|x_{k+1}-x^*\|^2+\frac{1}{\mu(k+1)}\|g_k\|^2 \\}
    \uncover<+->{f(x_k)-f(x^*) &\le \frac{\mu (k-1)}{4}\|x_k-x^*\|^2-\frac{\mu(k+1)}{4}\|x_{k+1}-x^*\|^2+\frac{1}{\mu k}\|g_k\|^2 \\}
    \uncover<+->{k \left(f(x_k)-f(x^*)\right) &\le \frac{\mu k(k-1)}{4}\|x_k-x^*\|^2-\frac{\mu k(k+1)}{4}\|x_{k+1}-x^*\|^2+\frac{1}{\mu}\|g_k\|^2}
    \end{aligned}
    $$

3. Суммируя неравенства для всех $k = 0, 1, \ldots, T-1$, мы получаем:
 $$
    \begin{aligned}
    \uncover<+->{\sum_{k=0}^{T-1} k \left(f(x_k)-f(x^*)\right) &\le 0 -\frac{\mu (T-1)T}{4}\|x_{T}-x^*\|^2+\frac{1}{\mu}\sum_{k=0}^{T-1}\|g_k\|^2}\uncover<+->{ \leq \frac{G^2 T}{\mu} \\}
    \uncover<+->{\left(f^{\text{best}}_{T-1}-f(x^*)\right) \sum_{k=0}^{T-1} k = \sum_{k=0}^{T-1} k \left(f^{\text{best}}_{T-1}-f(x^*)\right) }\uncover<+->{&\le \sum_{k=0}^{T-1} k \left(f(x_k)-f(x^*)\right)}\uncover<+->{ \leq \frac{G^2 T}{\mu} \\}
    \uncover<+->{f^{\text{best}}_{T-1}-f(x^*) &\leq \frac{G^2 T}{\mu \sum_{k=0}^{T-1} k }}\uncover<+->{ = \frac{2G^2 T}{\mu T (T-1) }}\uncover<+->{ \qquad  f_k^{\text{best}} - f(x^*) \leq \frac{2G^2}{\mu k}.}
    \end{aligned}
    $$

## Summary. Метод субградиента

| Тип задачи                          | Стратегия шага          | Скорость сходимости   | Сложность итераций |
|:---------------------------------------:|:----------------------:|:------------------:|:--------------------:|
| Выпуклые и липшицевые задачи          | $\alpha \sim \dfrac{1}{\sqrt{k}}$ | $\mathcal{O}\left(\dfrac{1}{\sqrt{k}}\right)$  | $\mathcal{O}\left(\dfrac{1}{\varepsilon^2}\right)$ |
| Сильно выпуклые и липшицевые задачи | $\alpha \sim \dfrac{1}{k}$        | $\mathcal{O}\left(\dfrac{1}{k}\right)$          | $\mathcal{O}\left(\dfrac{1}{\varepsilon}\right)$   |


## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Гладкий выпуклый случай. Сублинейная сходимость, не сходится в области определения](lasso_m_1000_n_100_mu_0_lambda_0.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий выпуклый случай. Маленькое значение $\lambda$ приводит к негладкости. Не сходится с постоянным шагом](lasso_m_1000_n_100_mu_0_lambda_0.1.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий выпуклый случай. При большем значении $\lambda$ проявляется немонотонность $f(x_k)$. Видно, что меньшая постоянная длина шага приводит к более низкому стационарному уровню функции.](lasso_m_1000_n_100_mu_0_lambda_1.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий выпуклый случай. Убывающая длина шага приводит к сходимости для $f^{\text{best}}_k$](lasso_m_100_n_100_mu_0_lambda_1_k.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий выпуклый случай. $\frac{\alpha_0}{\sqrt{k}}$ шаг приводит к сходимости для $f^{\text{best}}_k$](lasso_m_100_n_100_mu_0_lambda_1_sqrtk.pdf)


## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий выпуклый случай. $\frac{\alpha_0}{\sqrt{k}}$ шаг приводит к сходимости для $f^{\text{best}}_k$](lasso_m_100_n_100_mu_0_lambda_1_sqrtk_long.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий сильно выпуклый случай. $\frac{\alpha_0}{k}$ шаг приводит к сходимости для $f^{\text{best}}_k$](lasso_m_100_n_100_mu_1_lambda_1_1_k.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2m}\|Ax - b\|_2^2 + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A \in \mathbb{R}^{m \times n}, \quad \lambda\left(\tfrac{1}{m} A^TA\right) \in [\mu; L].
$$

![Негладкий сильно выпуклый случай. Шаг $\frac{\alpha_0}{\sqrt{k}}$ работает хуже](lasso_m_100_n_100_mu_1_lambda_1_1_sqrtk.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{m}\sum_{i=1}^{m} \log(1 + \exp(-b_i(A_i x))) + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A_i \in \mathbb{R}^n, \quad b_i \in \{-1,1\}
$$

![Логистическая регрессия с $\ell_1$ регуляризацией](logreg_lasso_small_short.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{m}\sum_{i=1}^{m} \log(1 + \exp(-b_i(A_i x))) + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A_i \in \mathbb{R}^n, \quad b_i \in \{-1,1\}
$$

![Логистическая регрессия с $\ell_1$ регуляризацией](logreg_lasso_small_long.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{m}\sum_{i=1}^{m} \log(1 + \exp(-b_i(A_i x))) + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A_i \in \mathbb{R}^n, \quad b_i \in \{-1,1\}
$$

![Логистическая регрессия с $\ell_1$ регуляризацией](logreg_lasso_medium_short.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{m}\sum_{i=1}^{m} \log(1 + \exp(-b_i(A_i x))) + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A_i \in \mathbb{R}^n, \quad b_i \in \{-1,1\}
$$

![Логистическая регрессия с $\ell_1$ регуляризацией](logreg_lasso_medium_long.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{m}\sum_{i=1}^{m} \log(1 + \exp(-b_i(A_i x))) + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A_i \in \mathbb{R}^n, \quad b_i \in \{-1,1\}
$$

![Логистическая регрессия с $\ell_1$ регуляризацией](logreg_lasso_high_short.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{m}\sum_{i=1}^{m} \log(1 + \exp(-b_i(A_i x))) + \lambda \|x\|_1 \to \min_{x \in \mathbb{R}^n}, \qquad A_i \in \mathbb{R}^n, \quad b_i \in \{-1,1\}
$$

![Логистическая регрессия с $\ell_1$ регуляризацией](logreg_lasso_high_long.pdf)


# Нижние оценки

## Нижние оценки

| выпуклые (негладкие) ^[[Nesterov, Lectures on Convex Optimization](https://fmin.xyz/assets/files/Nesterov_the_best.pdf)] | гладкие (невыпуклые)^[[Carmon, Duchi, Hinder, Sidford, 2017](https://arxiv.org/pdf/1710.11606.pdf)] | гладкие и выпуклые^[[Nemirovski, Yudin, 1979](https://fmin.xyz/assets/files/nemyud1979.pdf)] | гладкие и сильно выпуклые (или PL)$^1$ |
|:-----:|:-----:|:----:|:---------:|
| $\mathcal{O} \left( \dfrac{1}{\sqrt{k}} \right)$ | $\mathcal{O} \left( \dfrac{1}{k^2} \right)$ |  $\mathcal{O} \left( \dfrac{1}{k^2} \right)$ | $\mathcal{O} \left( \left(\dfrac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^k \right)$ |
| $k_\varepsilon \sim  \mathcal{O} \left( \dfrac{1}{\varepsilon^2} \right)$  | $k_\varepsilon \sim  \mathcal{O}  \left( \dfrac{1}{\sqrt{\varepsilon}} \right)$ | $k_\varepsilon \sim  \mathcal{O}  \left( \dfrac{1}{\sqrt{\varepsilon}} \right)$ | $k_\varepsilon  \sim \mathcal{O} \left( \sqrt{\kappa} \log \dfrac{1}{{\varepsilon}}\right)$ |

## Итерация «чёрного ящика»

Итерация градиентного спуска:
$$
\begin{aligned}
x^{k+1} &= x^k - \alpha^k \nabla f(x^k)\\
&= x^{k-1} - \alpha^{k-1} \nabla f(x^{k-1}) - \alpha^k \nabla f(x^k) \\
& \;\;\vdots \\
&= x^0 - \sum\limits_{i=0}^k \alpha^{k-i} \nabla f(x^{k-i})
\end{aligned}
$$

. . .

Рассмотрим семейство методов первого порядка, где
$$
\begin{aligned}
x^{k+1} &\in x^0 + \text{span} \left\{\nabla f(x^{0}), \nabla f(x^{1}), \ldots, \nabla f(x^{k})\right\} \; & f \text{ - smooth} \\
x^{k+1} &\in x^0 + \text{span} \left\{g_{0}, g_{1}, \ldots, g_{k}\right\} \text{, where }
g_{i} \in \partial f(x^{i}) \; & f \text{ - non-smooth}
\end{aligned}
$$ {#eq-fom}

. . .

Чтобы построить нижнюю оценку, нужно найти функцию $f$ из соответствующего класса так, чтобы любой метод из семейства @eq-fom сходился не быстрее этой нижней оценки.



## Негладкий выпуклый случай

:::{.callout-theorem}
Существует функция $f$, которая является $G$‑липшицевой и выпуклой, такая, что любой метод [@eq-fom] удовлетворяет
$$
\min_{i \in [1, k]} f(x^i) - \min_{x \in \mathbb{B}(R)} f(x) \geq \frac{GR}{2(1 + \sqrt{k})}
$$
для $R > 0$ и $k \leq n$, где $n$ — размерность задачи.
:::

. . .

**Идея доказательства:** построить такую функцию $f$, что для любого метода [@eq-fom] получаем
$$
\text{span} \left\{g_{0}, g_{1}, \ldots, g_{k}\right\} \subset \text{span} \left\{e_{1}, e_{2}, \ldots, e_{i}\right\}
$$
где $e_i$ — $i$-й стандартный базисный вектор. На итерации $k\leq n$, есть по крайней мере $n-k$ координат $x$, равных $0$. Это позволяет нам получить оценку на ошибку.

## Негладкий выпуклый случай (доказательство)

Рассмотрим функцию:
$$
f(x) = \beta \max_{i \in [1,k]} x[i] + \frac{\alpha}{2} \|x\|_2^2,
$$
где $\alpha, \beta \in \mathbb{R}$ — параметры, и $x[1:k]$ — первые $k$ компонент $x$.

. . .

**Основные свойства:**

* Функция $f(x)$ является $\alpha$‑сильно выпуклой из-за квадратичного члена $\frac{\alpha}{2} \|x\|_2^2$.
* Функция негладкая, поскольку первое слагаемое вносит недифференцируемость в точке, соответствующей максимальной координате $x$.

. . .

Рассмотрим субдифференциал $f(x)$ в $x$:

:::: {.columns}

::: {.column width="50%"}
$$
\begin{aligned}
\partial f(x) &=   \partial \left( \beta\max_{i \in [1,k]} x[i] \right) + \partial \left( \frac{\alpha}{2} \|x\|_2^2 \right) \\
&=\beta \partial \left(\max_{i \in [1,k]} x[i] \right) + \alpha x\\
&= \beta \text{conv}\left\{e_i \mid i: x[i] =  \max_{j} x[j] \right\} + \alpha x
\end{aligned}
$$

:::

. . .

::: {.column width="50%"}
Легко видеть, что если $g \in \partial f(x)$ и $\|x\|\leq R$, то
$$
\|g\| \leq \alpha R + \beta
$$

Таким образом, $f$ является $\alpha R + \beta$‑липшицевой на $B(R)$.
:::
::::

## Негладкий выпуклый случай (доказательство)

Далее опишем оракул первого порядка для этой функции. При запросе субградиента в точке $x$ оракул возвращает:
$$
\alpha x + \gamma e_{i},
$$
где $i$ — *первая* координата для которой $x[i] = \max_{1 \leq j \leq k} x[j]$. 

* Мы обеспечиваем $\|x^0\| \leq R$ начиная с $x^0 = 0$. 
* При запросе оракула в $x^0=0$, он возвращает $e_1$. Следовательно, $x^1$ должен лежать на прямой, порождённой $e_1$. 
* По индукции, показывается, что для всех $i$, итерация $x^i$ лежит в линейной оболочке $\{e_1,\dots, e_{i}\}$. В частности, для $i \leq k$, $k+1$-я координата $x_i$ равна нулю и вследствие структуры $f(x)$:
 $$
    f(x^i) \geq 0.
    $$

## Негладкий выпуклый случай (доказательство)

* Остаётся вычислить минимальное значение $f$. Определим точку $y\in\mathbb{R}^n$ как
 $$
    y[i] = - \frac{\beta}{\alpha k} \quad \text{for } 1 \leq i \leq k,\qquad y[i] = 0 \quad \text{for } k+1 \leq i \leq n.
    $$
* Заметим, что $0 \in \partial f(y)$:
 $$
    \begin{aligned}
    \partial f(y) &= \alpha y + \beta \text{conv}\left\{e_i \mid i: y[i] =  \max_{j} y[j] \right\} \\
    &= \alpha y + \beta \text{conv}\left\{e_i \mid i: y[i] =  0 \right\} \\
    0 &\in \partial f(y).
    \end{aligned}
    $$
* Следовательно, минимальное значение $f = f(y) = f(x^*)$ равно
 $$
    f(y) = - \frac{\beta^2}{\alpha k} + \frac{\alpha}{2} \cdot \frac{\beta^2}{\alpha^2 k} = - \frac{\beta^2}{2 \alpha k}.
    $$ 
* Теперь мы получаем:
 $$
    f(x^i) - f(x^*) \geq 0 - \left( - \frac{\beta^2}{2 \alpha k} \right) \geq \frac{\beta^2}{2 \alpha k}.
    $$

## Негладкий выпуклый случай (доказательство)

У нас есть: $f(x^i) - f(x^*) \geq \frac{\beta^2}{2 \alpha k}$, в то время как мы должны доказать, что $\min\limits_{i \in [1, k]} f(x^i) - f(x^*) \geq \frac{GR}{2(1 + \sqrt{k})}$.  

. . .

:::: {.columns}

::: {.column width="50%"}
### Выпуклый случай
$$
\alpha = \frac{G}{R}\frac{1}{1 + \sqrt{k}} \quad \beta = \frac{\sqrt{k}}{1 + \sqrt{k}}
$$

$$
\frac{\beta^2}{2\alpha} = \frac{GRk}{2(1 + \sqrt{k})}
$$
Заметим, в частности, что $\|y\|^2_2 = \frac{\beta^2}{\alpha^2 k} = R^2$ с этими параметрами

$$
\min\limits_{i \in [1, k]} f(x^i) - f(x^*) \geq \frac{\beta^2}{2 \alpha k} =\frac{GR}{2(1 + \sqrt{k})}
$$
:::

. . .

::: {.column width="50%"}
### Сильно выпуклый случай
$$
\alpha = \frac{G}{2R} \quad \beta = \frac{G}{2}
$$
Заметим, в частности, что $\|y\|_2^2 = \frac{\beta^2}{\alpha^2 k} = \frac{G^2}{4\alpha^2 k} = R^2$ с этими параметрами

$$
\min\limits_{i \in [1, k]} f(x^i) - f(x^*) \geq \frac{G^2}{8\alpha k}
$$
:::

::::

## Ссылки

* [Subgradient Methods Stephen Boyd (with help from Jaehyun Park)](https://web.stanford.edu/class/ee364b/lectures/subgrad_method_notes.pdf)