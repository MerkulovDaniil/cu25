---
title: "Gradient methods for conditional problems. Projected Gradient Descent. Frank-Wolfe method. Idea of Mirror Descent algorithm"
author: Даня Меркулов
institute: Методы Оптимизации в Машинном Обучении. ФКН ВШЭ
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
 - \newcommand{\bgimage}{../files/back12.jpeg}
---

# Conditional methods

## Constrained optimization

:::: {.columns}

::: {.column width="50%"}

### Unconstrained optimization

$$
\min_{x \in \mathbb{R}^n} f(x)
$$

* Any point $x_0 \in \mathbb{R}^n$ is feasible and could be a solution.

:::

. . .

::: {.column width="50%"}

### Constrained optimization

$$
\min_{x \in S} f(x)
$$

* Not all $x \in \mathbb{R}^n$ are feasible and could be a solution.
* The solution has to be inside the set $S$.
* Example: 
    $$
    \frac12\|Ax - b\|_2^2 \to \min_{\|x\|_2^2 \leq 1}
    $$

:::

::::

. . .

Gradient Descent is a great way to solve unconstrained problem 
$$
\tag{GD}
x_{k+1} = x_k - \alpha_k \nabla f(x_k)
$$
Is it possible to tune GD to fit constrained problem? 

. . .

**Yes**. We need to use projections to ensure feasibility on every iteration.

## Example: White-box Adversarial Attacks 

:::: {.columns}

::: {.column width="55%"}

![[Source](https://arxiv.org/abs/1811.07018)](adversarial.jpeg)

:::

::: {.column width="45%"}

* Mathematically, a neural network is a function $f(w; x)$
* Typically, input $x$ is given and network weights $w$ optimized
* Could also freeze weights $w$ and optimize $x$, adversarially!
$$ 
\min_{\delta} \text{size}(\delta) \quad \text{s.t.} \quad \text{pred}[f(w;x+\delta)] \neq y
$$
or 
$$
\max_{\delta} l(w; x+\delta, y) \; \text{s.t.} \; \text{size}(\delta) \leq \epsilon, \; 0 \leq x+\delta \leq 1
$$
:::
::::


## Idea of Projected Gradient Descent

![Suppose, we start from a point $x_k$.](PGD1.pdf)

## Idea of Projected Gradient Descent{.noframenumbering}

![And go in the direction of $-\nabla f(x_k)$.](PGD2.pdf)

## Idea of Projected Gradient Descent{.noframenumbering}

![Occasionally, we can end up outside the feasible set.](PGD3.pdf)

## Idea of Projected Gradient Descent{.noframenumbering}

![Solve this little problem with projection!](PGD4.pdf)

## Idea of Projected Gradient Descent

$$
x_{k+1} = \text{proj}_S\left(x_k - \alpha_k \nabla f(x_k) \right)  \qquad \Leftrightarrow \qquad \begin{aligned}
y_k &= x_k - \alpha_k \nabla f(x_k) \\
x_{k+1} &= \text{proj}_S\left( y_k\right)
\end{aligned}
$$

![Illustration of Projected Gradient Descent algorithm](PGD.pdf)

# Projection

## Projection

The distance $d$ from point $\mathbf{y} \in \mathbb{R}^n$ to closed set $S \subset \mathbb{R}^n$:
$$
d(\mathbf{y}, S, \| \cdot \|) = \inf\{\|x - y\| \mid x \in S \}
$$

. . .

We will focus on Euclidean projection (other options are possible) of a point $\mathbf{y} \in \mathbb{R}^n$ on set $S \subseteq \mathbb{R}^n$ is a point $\text{proj}_S(\mathbf{y}) \in S$: 
$$
\text{proj}_S(\mathbf{y}) =\underset{\mathbf{x} \in S}{\operatorname{argmin}}  \frac12 \|x - y\|_2^2
$$

. . .

* **Sufficient conditions of existence of a projection**. If $S \subseteq \mathbb{R}^n$ - closed set, then the projection on set $S$ exists for any point.
* **Sufficient conditions of uniqueness of a projection**. If $S \subseteq \mathbb{R}^n$ - closed convex set, then the projection on set $S$ is unique for any point.
* If a set is open, and a point is beyond this set, then its projection on this set may not exist.
* If a point is in set, then its projection is the point itself.

## Projection criterion (Bourbaki-Cheney-Goldstein inequality)

:::: {.columns}

::: {.column width="65%"}

:::{.callout-theorem}
\small
Let $S \subseteq \mathbb{R}^n$ be closed and convex, $\forall x \in S, y \in \mathbb{R}^n$. Then
$$
\langle y - \text{proj}_S(y), \mathbf{x} - \text{proj}_S(y)\rangle \leq 0
$$ {#eq-proj1}
$$
\|x - \text{proj}_S(y)\|^2 + \|y - \text{proj}_S(y)\|^2 \leq \|x-y\|^2
$$ {#eq-proj2}
:::

1. $\text{proj}_S(y)$ is minimizer of differentiable convex function $d(y, S, \| \cdot \|) = \|x - y\|^2$ over $S$. By first-order characterization of optimality.
    $$
    \begin{aligned}
    \uncover<+->{\nabla d(\text{proj}_S(y))^T(x - \text{proj}_S(y))&\geq 0 \\ }
    \uncover<+->{2\left(\text{proj}_S(y) - y \right)^T(x - \text{proj}_S(y))&\geq 0 \\ }
    \uncover<+->{\left(y - \text{proj}_S(y) \right)^T(x - \text{proj}_S(y))&\leq 0}
    \end{aligned}
    $$
2. Use cosine rule $2x^Ty = \|x\|^2 + \|y\|^2 - \|x-y\|^2$ with $x = x - \text{proj}_S(y)$ and $y = y - \text{proj}_S(y)$. By the first property of the theorem:
    $$
    \begin{aligned}
    \uncover<+->{ 0 \geq 2x^Ty = \|x - \text{proj}_S(y)\|^2 + \|y + \text{proj}_S(y)\|^2 - \|x-y\|^2 \\ }
    \uncover<+->{ \|x - \text{proj}_S(y)\|^2 + \|y + \text{proj}_S(y)\|^2 \leq \|x-y\|^2 }
    \end{aligned}
    $$
:::

::: {.column width="35%"}
![Obtuse or straight angle should be for any point $x \in S$](proj_crit.pdf)
:::
::::

## Projection operator is non-expansive

* A function $f$ is called non-expansive if $f$ is $L$-Lipschitz with $L \leq 1$ ^[Non-expansive becomes contractive if $L < 1$.]. That is, for any two points $x,y \in \text{dom} f$,
    $$
    \|f(x)-f(y)\| \leq L\|x-y\|, \text{ where } L \leq 1.
    $$
    It means the distance between the mapped points is possibly smaller than that of the unmapped points.

* Projection operator is non-expansive:
    $$
    \| \text{proj}(x) - \text{proj}(y) \|_2 \leq \| x - y \|_2.
    $$

* Next: variational characterization implies non-expansiveness. i.e.,
    $$
    \langle y - \text{proj}(y), x - \text{proj}(y) \rangle \leq 0 \quad \forall x \in S \qquad \Rightarrow \qquad \| \text{proj}(x) - \text{proj}(y) \|_2 \leq \| x - y \|_2.
    $$ 

## Projection operator is non-expansive

Shorthand notation: let $\pi = \text{proj}$ and $\pi(x)$ denotes $\text{proj}(x)$.

. . .

Begins with the variational characterization / obtuse angle inequality
$$
\langle y-\pi(y) , x-\pi(y) \rangle \leq 0 \quad \forall x \in S.
$$ {#eq-proj1}

. . .

:::: {.columns}

::: {.column width="50%"}
Replace $x$ by $\pi(x)$ in @eq-proj1
$$
\langle y-\pi(y), \pi(x)-\pi(y) \rangle \leq 0.
$$ {#eq-proj2}
:::

. . .

::: {.column width="50%"}
Replace $y$ by $x$ and $x$ by $\pi(y)$ in @eq-proj1

$$
\langle x-\pi(x), \pi(y)-\pi(x) \rangle \leq 0.
$$ {#eq-proj3}
:::

::::

. . .

(@eq-proj2)+(@eq-proj3) will cancel $\pi(y) - \pi(x)$, not good. So flip the sign of (@eq-proj3) gives
$$
\langle \pi(x)-x, \pi(x)-\pi(y) \rangle \leq 0.
$$ {#eq-proj4}

. . .

:::: {.columns}

::: {.column width="60%"}
$$
\begin{split}
\langle y-\pi(y)+\pi(x)-x , \pi(x)-\pi(y) \rangle & \leq 0 \\
\langle y - x, \pi(x) - \pi(y) \rangle & \leq -\langle \pi(x)-\pi(y), \pi(x)-\pi(y) \rangle \\
\langle y - x, \pi(y) - \pi(x) \rangle & \geq \lVert \pi(x) - \pi(y) \rVert^2_2 \\
\lVert (y - x)^\top (\pi(y) - \pi(x)) \rVert_2 & \geq \lVert \pi(x) - \pi(y) \rVert^2_2
\end{split}
$$
:::

. . .

::: {.column width="40%"}
By Cauchy-Schwarz inequality, the left-hand-side is upper bounded by $\lVert y - x \rVert_2 \lVert \pi(y) - \pi(x) \rVert_2$, we get $\lVert y - x \rVert_2 \lVert \pi(y) - \pi(x) \rVert_2 \geq \lVert \pi(x) - \pi(y) \rVert^2_2$. Cancels $\lVert \pi(x) - \pi(y) \rVert_2$ finishes the proof.
:::

::::

## Example: projection on the ball

Find $\pi_S (y) = \pi$, if $S = \{x \in \mathbb{R}^n \mid \|x - x_0\| \le R \}$, $y \notin S$ 

. . .

Build a hypothesis from the figure: $\pi = x_0 + R \cdot \frac{y - x_0}{\|y - x_0\|}$ 

. . .

:::: {.columns}

::: {.column width="60%"}
Check the inequality for a convex closed set: $(\pi - y)^T(x - \pi) \ge 0$ 

. . .

$$
\begin{split}
\left( x_0 - y + R \frac{y - x_0}{\|y - x_0\|} \right)^T\left( x - x_0 - R \frac{y - x_0}{\|y - x_0\|} \right) &= \\
\left( \frac{(y - x_0)(R - \|y - x_0\|)}{\|y - x_0\|} \right)^T\left( \frac{(x-x_0)\|y-x_0\|-R(y - x_0)}{\|y - x_0\|} \right) &= \\
\frac{R - \|y - x_0\|}{\|y - x_0\|^2} \left(y - x_0 \right)^T\left( \left(x-x_0\right)\|y-x_0\|-R\left(y - x_0\right) \right) &= \\
\frac{R - \|y - x_0\|}{\|y - x_0\|} \left( \left(y - x_0 \right)^T\left( x-x_0\right)-R\|y - x_0\| \right) &= \\
\left(R - \|y - x_0\| \right) \left( \frac{(y - x_0 )^T( x-x_0)}{\|y - x_0\|}-R \right) & \\
\end{split}
$$
:::

. . .

::: {.column width="40%"}
The first factor is negative for point selection $y$. The second factor is also negative, which follows from the Cauchy-Bunyakovsky inequality: 

. . .

$$
\begin{split}
(y - x_0 )^T( x-x_0) &\le \|y - x_0\|\|x-x_0\| \\
\frac{(y - x_0 )^T( x-x_0)}{\|y - x_0\|} - R &\le \frac{\|y - x_0\|\|x-x_0\|}{\|y - x_0\|} - R = \|x - x_0\| - R \le 0
\end{split}
$$

![Ball](proj_ball.pdf){width=60%}

:::
::::

## Example: projection on the halfspace

Find $\pi_S (y) = \pi$, if $S = \{x \in \mathbb{R}^n \mid c^T x = b \}$, $y \notin S$. Build a hypothesis from the figure: $\pi = y + \alpha c$. Coefficient $\alpha$ is chosen so that $\pi \in S$: $c^T \pi = b$, so:

. . .

:::: {.columns}

::: {.column width="50%"}
![Hyperplane](proj_half.pdf)
:::

. . .

::: {.column width="50%"}
$$
\begin{split}
c^T (y + \alpha c) &= b \\
c^Ty + \alpha c^T c &= b \\
c^Ty &= b - \alpha c^T c \\
\end{split}
$$
Check the inequality for a convex closed set: $(\pi - y)^T(x - \pi) \ge 0$ 
$$
\begin{split}
(y + \alpha c - y)^T(x - y - \alpha c) =& \\
\alpha c^T(x - y - \alpha c) =& \\
\alpha (c^Tx) - \alpha (c^T y) - \alpha^2 (c^Tc) =& \\
\alpha b - \alpha (b - \alpha c^T c) - \alpha^2 c^Tc =& \\
\alpha b - \alpha b + \alpha^2 c^T c - \alpha^2 c^Tc =& 0 \ge 0
\end{split}
$$
:::

::::
 
# Projected Gradient Descent (PGD)

## Idea

$$
x_{k+1} = \text{proj}_S\left(x_k - \alpha_k \nabla f(x_k) \right)  \qquad \Leftrightarrow \qquad \begin{aligned}
y_k &= x_k - \alpha_k \nabla f(x_k) \\
x_{k+1} &= \text{proj}_S\left( y_k\right)
\end{aligned}
$$

![Illustration of Projected Gradient Descent algorithm](PGD.pdf)

## Convergence tools \faGem \ \faGem[regular] \faGem[regular] \faGem[regular]

:::{.callout-theorem}
Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be an $L$-smooth convex function. Then, for any $x, y \in \mathbb{R}^n$, the following inequality holds:

$$
\begin{aligned}
f(x) + \langle \nabla f(x), y - x \rangle + \frac{1}{2L} & \|\nabla f(x) - \nabla f(y)\|^2_2 \leq f(y) \text{ or, equivalently, }\\
\|\nabla f(y)-\nabla f (x)\|_2^2 = & \|\nabla f(x)-\nabla f (y)\|_2^2 \leq 2L\left(f(x)-f(y)-\langle\nabla f (y),x -y\rangle \right)
\end{aligned}
$$
:::

**Proof**

1. To prove this, we'll consider another function $\varphi(y) = f(y) - \langle \nabla f(x), y\rangle$. It is obviously a convex function (as a sum of convex functions). And it is easy to verify, that it is an $L$-smooth function by definition, since $\nabla \varphi(y) = \nabla f(y) - \nabla f(x)$ and $\|\nabla \varphi(y_1) - \nabla \varphi(y_2)\| = \|\nabla f(y_1) - \nabla f(y_2)\| \leq L\|y_1 - y_2\|$.
2. Now let's consider the smoothness parabolic property for the $\varphi(y)$ function:
  $$
  \begin{aligned}
  \uncover<+->{ \varphi(y) & \leq  \varphi(x) + \langle \nabla \varphi(x), y-x \rangle + \frac{L}{2}\|y-x\|_2^2 \\ }
  \uncover<+->{ \stackrel{x := y, y := y - \frac1L \nabla\varphi(y)}{ }\;\;\varphi\left(y - \frac1L \nabla\varphi(y)\right) &  \leq \varphi(y) + \left\langle \nabla \varphi(y), - \frac1L \nabla\varphi(y)\right\rangle + \frac{1}{2L}\|\nabla\varphi(y)\|_2^2 \\ }
  \uncover<+->{ \varphi\left(y - \frac1L \nabla\varphi(y)\right) &  \leq \varphi(y) - \frac{1}{2L}\|\nabla\varphi(y)\|_2^2 }
  \end{aligned}
  $$

## Convergence tools \faGem \ \faGem \ \faGem[regular] \faGem[regular]

3. From the first order optimality conditions for the convex function $\nabla \varphi (y) =\nabla f(y) - \nabla f(x) = 0$. We can conclude, that for any $x$, the minimum of the function $\varphi(y)$ is at the point $y=x$. Therefore:
  $$
  \varphi(x) \leq \varphi\left(y - \frac1L \nabla\varphi(y)\right) \leq \varphi(y) - \frac{1}{2L}\|\nabla\varphi(y)\|_2^2
  $$
4. Now, substitute $\varphi(y) = f(y) - \langle \nabla f(x), y\rangle$:
  $$
  \begin{aligned}
  \uncover<+->{ & f(x) - \langle \nabla f(x), x\rangle \leq f(y) - \langle \nabla f(x), y\rangle - \frac{1}{2L}\|\nabla f(y) - \nabla f(x)\|_2^2 \\ }
  \uncover<+->{ & f(x) + \langle \nabla f(x), y - x \rangle + \frac{1}{2L} \|\nabla f(x) - \nabla f(y)\|^2_2 \leq f(y) \\ }
  \uncover<+->{ & \|\nabla f(y) - \nabla f(x)\|^2_2 \leq 2L \left( f(y) - f(x) - \langle \nabla f(x), y - x \rangle \right) \\ }
  \uncover<+->{ {\scriptsize \text{switch x and y}} \quad & \|\nabla f(x)-\nabla f (y)\|_2^2 \leq 2L\left(f(x)-f(y)-\langle\nabla f (y),x -y\rangle \right)}
  \end{aligned}
  $$

. . .

The lemma has been proved. From the first view it does not make a lot of geometrical sense, but we will use it as a convenient tool to bound the difference between gradients.

## Convergence tools \faGem \ \faGem \ \faGem \ \faGem[regular]

:::{.callout-theorem}
Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be continuously differentiable on $\mathbb{R}^n$. Then, the function $f$ is $\mu$-strongly convex if and only if for any $x, y \in \mathbb{R}^d$ the following holds:
$$
\begin{aligned}
\text{Strongly convex case } \mu >0 & &\langle \nabla f(x) - \nabla f(y), x - y \rangle &\geq \mu \|x - y\|^2 \\
\text{Convex case } \mu = 0 & &\langle \nabla f(x) - \nabla f(y), x - y \rangle &\geq 0
\end{aligned}
$$
:::

**Proof**

1. We will only give the proof for the strongly convex case, the convex one follows from it with setting $\mu=0$. We start from necessity. For the strongly convex function
  $$
  \begin{aligned}
  & f(y) \geq f(x) + \langle \nabla f(x), y-x\rangle + \frac{\mu}{2}\|x-y\|_2^2 \\
  & f(x) \geq f(y) + \langle \nabla f(y), x-y\rangle + \frac{\mu}{2}\|x-y\|_2^2 \\
  {\scriptsize \text{sum}} \;\; & \langle \nabla f(x) - \nabla f(y), x - y \rangle \geq \mu \|x - y\|^2
  \end{aligned}
  $$

## Convergence tools \faGem \ \faGem \ \faGem \ \faGem

2. For the sufficiency we assume, that $\langle \nabla f(x) - \nabla f(y), x - y \rangle \geq \mu \|x - y\|^2$. Using Newton-Leibniz theorem $f(x) = f(y) + \int_{0}^{1} \langle \nabla f(y + t(x - y)), x - y \rangle dt$:
  $$
  \begin{aligned}
  \uncover<+->{ f(x) - f(y) - \langle \nabla f(y), x - y \rangle &= \int_{0}^{1} \langle \nabla f(y + t(x - y)), x - y \rangle dt - \langle \nabla f(y), x - y \rangle \\ }
  \uncover<+->{ \stackrel{ \langle \nabla f(y), x - y \rangle = \int_{0}^{1}\langle \nabla f(y), x - y \rangle dt}{ }\qquad &= \int_{0}^{1} \langle \nabla f(y + t(x - y)) - \nabla f(y), (x - y) \rangle dt \\ }
  \uncover<+->{ \stackrel{ y + t(x - y) - y = t(x - y)}{ }\qquad&= \int_{0}^{1} t^{-1} \langle \nabla f(y + t(x - y)) - \nabla f(y), t(x - y) \rangle dt \\ }
  \uncover<+->{ & \geq \int_{0}^{1} t^{-1} \mu \| t(x - y) \|^2 dt } \uncover<+->{ = \mu \| x - y \|^2 \int_{0}^{1} t dt} \uncover<+->{= \frac{\mu}{2} \| x - y \|^2_2 }
  \end{aligned}
  $$

  . . .

  Thus, we have a strong convexity criterion satisfied
  $$
  \begin{aligned}
  \uncover<+->{ & f(x) \geq f(y) + \langle \nabla f(y), x - y \rangle + \frac{\mu}{2} \| x - y \|^2_2} \uncover<+->{ \text{ or, equivivalently: }\\ }
  \uncover<+->{ {\scriptsize \text{switch x and y}} \quad & - \langle \nabla f(x), x - y \rangle \leq - \left(f(x) - f(y) + \frac{\mu}{2} \| x - y \|^2_2 \right) }
  \end{aligned}
  $$

## Convergence rate for smooth and convex case \faGem \ \faGem[regular] \faGem[regular] \faGem[regular] \faGem[regular]

:::{.callout-theorem}
Let $f: \mathbb{R}^n \to \mathbb{R}$ be convex and differentiable. Let $S \subseteq  \mathbb{R}^n$d be a closed convex set, and assume that there is a minimizer $x^*$ of $f$ over $S$; furthermore, suppose that $f$ is smooth over $S$ with parameter $L$. The Projected Gradient Descent algorithm with stepsize $\frac1L$ achieves the following convergence after iteration $k > 0$:
$$
f(x_k) - f^* \leq \frac{L\|x_0 - x^*\|_2^2}{2k}
$$
:::

. . .

1. Let's prove sufficient decrease lemma, assuming, that $y_{k} = x_k - \frac1L\nabla f(x_k)$ and cosine rule $2x^Ty = \|x\|^2 + \|y\|^2 - \|x-y\|^2$:
    $$
    \begin{aligned}
    \uncover<+->{ &\text{Smoothness:} &f(x_{k+1})& \leq f(x_{k}) + \langle \nabla f(x_{k}), x_{k+1}-x_{k} \rangle +\frac{L}{2} \| x_{k+1}-x_{k}\|^2\\ }
    \uncover<+->{ &\text{Method:} & &= f(x_{k})-L\langle y_{k} - x_k , x_{k+1}-x_{k} \rangle +\frac{L}{2} \| x_{k+1}-x_{k}\|^2\\ }
    \uncover<+->{ &\text{Cosine rule:} & &= f(x_{k})-\frac{L}{2}\left( \|y_{k} - x_k\|^2 + \|x_{k+1}-x_{k}\|^2 - \|y_{k} - x_{k+1}\|^2\right) +\frac{L}{2} \| x_{k+1}-x_{k}\|^2\\ }
    \uncover<+->{ & & &= f(x_{k})-\frac{1}{2L}\|\nabla f(x_k)\|^2 + \frac{L}{2} \|y_{k} - x_{k+1}\|^2 \\ }
    \end{aligned}
    $$ {#eq-suff_dec}

## Convergence rate for smooth and convex case \faGem \ \faGem \ \faGem[regular] \faGem[regular] \faGem[regular]

2. Now we do not immediately have progress at each step. Let's use again cosine rule:
    $$
    \begin{aligned}
    \left\langle\frac1L \nabla f(x_k), x_k - x^* \right\rangle &=  \frac12\left(\frac{1}{L^2}\|\nabla f(x_k)\|^2 + \|x_k - x^*\|^2 -  \|x_k - x^* - \frac1L \nabla f(x_k)\|^2 \right) \\
    \langle \nabla f(x_k), x_k - x^* \rangle &=  \frac{L}{2}\left(\frac{1}{L^2}\|\nabla f(x_k)\|^2 + \|x_k - x^*\|^2 -  \|y_k - x^*\|^2 \right) \\
    \end{aligned}
    $$
3. We will use now projection property: $\|x - \text{proj}_S(y)\|^2 + \|y - \text{proj}_S(y)\|^2 \leq \|x-y\|^2$ with $x = x^*, y = y_k$:
    $$
    \begin{aligned}
    \|x^* - \text{proj}_S(y_k)\|^2 + \|y_k - \text{proj}_S(y_k)\|^2 \leq \|x^*-y_k\|^2 \\
    \|y_k - x^*\|^2 \geq \|x^* - x_{k+1}\|^2 + \|y_k - x_{k+1}\|^2
    \end{aligned}
    $$
4. Now, using convexity and previous part:
    $$
    \begin{aligned}
    \uncover<+->{ &\text{Convexity:} &f(x_k) - f^* &\leq  \langle \nabla f(x_k), x_k - x^* \rangle \\}
    \uncover<+->{ & & &\leq  \frac{L}{2}\left(\frac{1}{L^2}\|\nabla f(x_k)\|^2 + \|x_k - x^*\|^2 -  \|x_{k+1} - x^*\|^2 - \|y_k - x_{k+1}\|^2 \right) \\}
    \uncover<+->{&\text{Sum for } i=0,k-1 &\sum\limits_{i=0}^{k-1} \left[f(x_i) - f^*\right]&\leq\sum\limits_{i=0}^{k-1} \frac{1}{2L}\|\nabla f(x_i)\|^2 + \frac{L}{2}\|x_0 - x^*\|^2  - \frac{L}{2} \sum\limits_{i=0}^{i-1} \|y_i - x_{i+1}\|^2}
    \end{aligned}
    $$

## Convergence rate for smooth and convex case \faGem \ \faGem \ \faGem \ \faGem[regular] \faGem[regular]

5. Bound gradients with [sufficient decrease inequality @eq-suff_dec]:
    $$
    \begin{aligned}
    \uncover<+->{\sum\limits_{i=0}^{k-1} \left[f(x_i) - f^*\right]&\leq \sum\limits_{i=0}^{k-1}\left[ f(x_{i}) - f(x_{i+1}) + \frac{L}{2} \|y_{i} - x_{i+1}\|^2 \right] + \frac{L}{2}\|x_0 - x^*\|^2  - \frac{L}{2} \sum\limits_{i=0}^{i-1} \|y_i - x_{i+1}\|^2  \\}
    \uncover<+->{&\leq f(x_0) - f(x_k) + \frac{L}{2} \sum\limits_{i=0}^{i-1} \|y_i - x_{i+1}\|^2 + \frac{L}{2}\|x_0 - x^*\|^2  - \frac{L}{2} \sum\limits_{i=0}^{i-1} \|y_i - x_{i+1}\|^2 \\}
    \uncover<+->{&\leq f(x_0) - f(x_k) + \frac{L}{2}\|x_0 - x^*\|^2 \\}
    \uncover<+->{\sum\limits_{i=0}^{k-1} f(x_i) - k f^* &\leq f(x_0) - f(x_k) + \frac{L}{2}\|x_0 - x^*\|^2\\}
    \uncover<+->{\sum\limits_{i=1}^{k} \left[ f(x_i) - f^*\right] &\leq \frac{L}{2}\|x_0 - x^*\|^2\\}
    \end{aligned}
    $$

## Convergence rate for smooth and convex case \faGem \ \faGem \ \faGem \ \faGem \ \faGem[regular]

6. From the sufficient decrease inequality 
    $$
    f(x_{k+1}) \le f(x_k) - \frac{1}{2L}\|\nabla f(x_k)\|^2 + \frac{L}{2}\|y_k - x_{k+1}\|^2,
    $$

    . . .

    we use the fact that $x_{k+1} = \mathrm{proj}_S(y_k)$.  By definition of projection,
    $$
    \|y_k - x_{k+1}\| \le \|y_k - x_k\|,
    $$

    . . .

    and recall that $y_k = x_k - \tfrac{1}{L}\nabla f(x_k)$ implies
    $\|y_k - x_k\| = \tfrac{1}{L}\|\nabla f(x_k)\|$.
    Hence
    $$
    \frac{L}{2}\,\|y_k - x_{k+1}\|^2 \le \frac{L}{2}\,\|y_k - x_k\|^2 = \frac{L}{2}\,\frac{1}{L^2}\,\|\nabla f(x_k)\|^2 = \frac{1}{2L}\,\|\nabla f(x_k)\|^2.
    $$

    . . .

    Substitute back into $(*)$:
    $$
    f(x_{k+1}) \le f(x_k) - \frac{1}{2L}\|\nabla f(x_k)\|^2 + \frac{1}{2L}\|\nabla f(x_k)\|^2 = f(x_k).
    $$

    . . .

    Hence 
    $$
    f(x_{k+1}) \le f(x_k)\quad\text{for each }k,
    $$
    so $\{f(x_k)\}$ is a monotonically nonincreasing sequence.


## Convergence rate for smooth and convex case \faGem \ \faGem \ \faGem \ \faGem \ \faGem

7. Final convergence bound
    From step 5, we have already established
    $$
    \sum_{i=0}^{k-1}\bigl[f(x_i) - f^*\bigr] \le \frac{L}{2}\|x_0 - x^*\|_2^2.
    $$

    . . .

    Since $f(x_i)$ decreases in $i$, in particular $f(x_k) \le f(x_i)$ for all $i \le k$.  Therefore
    $$
    k\,\bigl[f(x_k) - f^*\bigr] \le \sum_{i=0}^{k-1}\bigl[f(x_i) - f^*\bigr] \le \frac{L}{2}\|x_0 - x^*\|_2^2,
    $$

    . . .

    which immediately gives
    $$
    f(x_k) - f^* \le \frac{L\|x_0 - x^*\|_2^2}{2k}.
    $$
    This completes the proof of the $\mathcal{O}(\tfrac1k)$ convergence rate for convex and $L$‐smooth $f$ under projection constraints.

## Convergence rate for smooth strongly convex case \faGem \ \faGem[regular] \faGem[regular]

:::{.callout-theorem}

Let $f: \mathbb{R}^n \to \mathbb{R}$ be $\mu$-strongly convex. Let $S \subseteq  \mathbb{R}^n$d be a closed convex set, and assume that there is a minimizer $x^*$ of $f$ over $S$; furthermore, suppose that $f$ is smooth over $S$ with parameter $L$. The Projected Gradient Descent algorithm with stepsize $\alpha \leq \frac1L$ achieves the following convergence after iteration $k > 0$:

$$
\|x_{k} - x^*\|_2^2 \leq \left(1 - \alpha \mu\right)^k \|x_{0} - x^*\|_2^2
$$
:::

**Proof**

1. We first prove the stationary point property: $\text{proj}_S(x^* - \alpha \nabla f(x^*)) = x^*$.
   
   This follows from the projection criterion and the first-order optimality condition for $x^*$.
   Let $y = x^* - \alpha \nabla f(x^*)$. We need to show $\langle y - x^*, x - x^* \rangle \le 0$ for all $x \in S$.
   $$
   \langle (x^* - \alpha \nabla f(x^*)) - x^*, x - x^* \rangle = -\alpha \langle \nabla f(x^*), x - x^* \rangle \le 0
   $$
   The inequality holds because $\alpha > 0$ and $\langle \nabla f(x^*), x-x^* \rangle \geq 0$ is the optimality condition for $x^*$.



## Convergence rate for smooth strongly convex case \faGem \ \faGem \ \faGem[regular]

1. Considering the distance to the solution and using the stationary point property:
  $$
  \begin{aligned}
  \uncover<+->{ \|x_{k+1} - x^*\|^2_2 &= \|\text{proj}_S (x_k - \alpha \nabla f (x_k)) - x^*\|^2_2 \\ }
  \uncover<+->{ {\scriptsize \text{stationary point property}}  & = \|\text{proj}_S (x_k - \alpha \nabla f (x_k)) - \text{proj}_S (x^* - \alpha \nabla f (x^*)) \|^2_2 \\ }
  \uncover<+->{ {\scriptsize \text{nonexpansiveness}}   & \leq \|x_k - \alpha \nabla f (x_k) - (x^* - \alpha \nabla f (x^*)) \|^2_2 \\ }
  \uncover<+->{ & =  \|x_k - x^*\|^2 - 2\alpha \langle \nabla f(x_k) - \nabla f(x^*), x_k - x^* \rangle + \alpha^2 \|\nabla f(x_k) - \nabla f(x^*)\|^2_2 }
  \end{aligned}
  $$

2. Now we use smoothness from the convergence tools and strong convexity: 
  $$
  \begin{aligned}
  \uncover<+->{ \text{smoothness} \;\; &\|\nabla f(x_k)-\nabla f (x^*)\|_2^2 \leq 2L\left(f(x_k)-f(x^*)-\langle\nabla f (x^*),x_k -x^*\rangle \right) \\ }
  \uncover<+->{ \text{strong convexity} \;\; & - \langle \nabla f(x_k) -  \nabla f(x^*), x_k - x^* \rangle \leq - \left(f(x_k) - f(x^*) + \frac{\mu}{2} \| x_k - x^* \|^2_2 \right) - \langle \nabla f(x^*), x_k - x^* \rangle }
  \end{aligned}
  $$

## Convergence rate for smooth strongly convex case \faGem \ \faGem \ \faGem

3. Substitute it:
  $$
  \begin{aligned}
  \uncover<+->{ \|x_{k+1} - x^*\|^2_2 &\leq \|x_k - x^*\|^2 - 2\alpha \left(f(x_k) - f(x^*) + \frac{\mu}{2} \| x_k - x^* \|^2_2 \right) - 2\alpha \langle \nabla f(x^*), x_k - x^* \rangle + \\ 
  & + \alpha^2 2L\left(f(x_k)-f(x^*)-\langle\nabla f (x^*),x_k -x^*\rangle \right)  \\ }
  \uncover<+->{ &\leq (1 - \alpha \mu)\|x_k - x^*\|^2 + 2\alpha (\alpha L - 1) \left( f(x_k) - f(x^*) - \langle \nabla f(x^*), x_k - x^* \rangle \right)}
  \end{aligned}
  $$

4. Due to convexity of $f$: $f(x_k) - f(x^*) - \langle \nabla f(x^*), x_k - x^* \rangle \geq 0$. Therefore, if we use $\alpha \leq \frac1L$:
  $$
  \|x_{k+1} - x^*\|^2_2 \leq (1 - \alpha \mu)\|x_k - x^*\|^2,
  $$
  which is exactly linear convergence of the method with up to $1 - \frac{\mu}{L}$ convergence rate.



# Frank-Wolfe Method

---

::::{.columns}

:::{.column width="50%"}
![Marguerite Straus Frank (1927-2024)](frank.jpg){height=88%}
:::

:::{.column width="50%"}
![Philip Wolfe (1927-2016)](wolfe.jpg){height=88%}
:::

::::

## Idea

![Illustration of Frank-Wolfe (conditional gradient) algorithm](FW1.pdf)

## Idea {.noframenumbering}

![Illustration of Frank-Wolfe (conditional gradient) algorithm](FW2.pdf)

## Idea {.noframenumbering}

![Illustration of Frank-Wolfe (conditional gradient) algorithm](FW3.pdf)

## Idea {.noframenumbering}

![Illustration of Frank-Wolfe (conditional gradient) algorithm](FW4.pdf)

## Idea {.noframenumbering}

![Illustration of Frank-Wolfe (conditional gradient) algorithm](FW5.pdf)

## Idea {.noframenumbering}

![Illustration of Frank-Wolfe (conditional gradient) algorithm](FW6.pdf)

## Idea {.noframenumbering}

![Illustration of Frank-Wolfe (conditional gradient) algorithm](FW7.pdf)

## Idea

$$
\begin{split}
y_k &= \text{arg}\min_{x \in S} f^I_{x_k}(x) = \text{arg}\min_{x \in S} \langle\nabla f(x_k), x \rangle \\
x_{k+1} &= \gamma_k x_k + (1-\gamma_k)y_k
\end{split}
$$

![Illustration of Frank-Wolfe (conditional gradient) algorithm](FW.pdf)

## Convergence rate for smooth and convex case \faGem \ \faGem[regular] \faGem[regular]

:::{.callout-theorem}
Let $f: \mathbb{R}^n \to \mathbb{R}$ be convex and differentiable. Let $S \subseteq \mathbb{R}^n$ be a closed convex set, and assume that there is a minimizer $x^*$ of $f$ over $S$; furthermore, suppose that $f$ is smooth over $S$ with parameter $L$. The Frank-Wolfe algorithm with step size $\gamma_k = \frac{k-1}{k+1}$ achieves the following convergence after iteration $k > 0$:
$$
f(x_k) - f^* \leq \frac{2LR^2}{k+1}
$$
where $R = \max\limits_{x, y \in S} \|x - y\|$ is the diameter of the set $S$.
:::

. . .

1. By $L$-smoothness of $f$, we have:
    $$
    \begin{aligned}
    f\left(x_{k+1}\right) - f\left(x_k\right) &\leq \left\langle \nabla f\left(x_k\right), x_{k+1} - x_k \right\rangle + \frac{L}{2} \left\|x_{k+1} - x_k\right\|^2 \\
    &= (1 - \gamma_k) \left\langle \nabla f\left(x_k\right), y_k - x_k \right\rangle + \frac{L (1 - \gamma_k)^2}{2} \left\|y_k - x_k\right\|^2 
    \end{aligned}
    $$

## Convergence rate for smooth and convex case \faGem \ \faGem \ \faGem[regular]

2. By convexity of $f$, for any $x \in S$, including $x^*$:
    $$
    \langle \nabla f(x_k), x - x_k \rangle \leq f(x) - f(x_k)
    $$
    In particular, for $x = x^*$:
    $$
    \langle \nabla f(x_k), x^* - x_k \rangle \leq f(x^*) - f(x_k)
    $$

3. By definition of $y_k$, we have $\langle \nabla f(x_k), y_k \rangle \leq \langle \nabla f(x_k), x^* \rangle$, thus:
    $$
    \langle \nabla f(x_k), y_k - x_k \rangle \leq \langle \nabla f(x_k), x^* - x_k \rangle \leq f(x^*) - f(x_k)
    $$

4. Combining the above inequalities:
    $$
    \begin{aligned}
    f\left(x_{k+1}\right) - f\left(x_k\right) &\leq (1 - \gamma_k) \left\langle \nabla f\left(x_k\right), y_k - x_k \right\rangle + \frac{L (1 - \gamma_k)^2}{2} \left\|y_k - x_k\right\|^2 \\
    &\leq (1 - \gamma_k) \left( f(x^*) - f(x_k) \right) + \frac{L (1 - \gamma_k)^2}{2} R^2
    \end{aligned}
    $$

5. Rearranging terms:
    $$
    \begin{aligned}
    f\left(x_{k+1}\right) - f(x^*) &\leq \gamma_k \left( f(x_k) - f(x^*) \right) + (1 - \gamma_k)^2 \frac{L R^2}{2}
    \end{aligned}
    $$

## Convergence rate for smooth and convex case \faGem \ \faGem \ \faGem

6. Denoting $\delta_k = \frac{f\left(x_k\right) - f\left(x^*\right)}{L R^2}$, we get:
    $$
    \delta_{k+1} \leq \gamma_k \delta_k + \frac{(1 - \gamma_k)^2}{2} = \frac{k - 1}{k + 1} \delta_k + \frac{2}{(k + 1)^2}
    $$

7. We will prove that $\delta_k \leq \frac{2}{k+1}$ by induction.

    * Base: $\delta_2 \leq \frac{1}{2} < \frac23$
    * Assume $\delta_k \leq \frac{2}{k+1}$
    * Then $\delta_{k+1} \leq \frac{k-1}{k+1} \cdot \frac{2}{k+1} + \frac{2}{(k+1)^2} = \frac{2k}{k^2 + 2k + 1} < \frac{2}{k+2}$ \faGraduationCap

    which gives us the desired result:
    $$
    f(x_k) - f^* \leq \frac{2LR^2}{k+1}
    $$

## Lower bound for Frank-Wolfe method ^[[\faFilePdf The Complexity of Large-scale Convex Programming under a Linear Optimization Oracle](https://arxiv.org/abs/1309.5550)] \faGem  \faGem[regular]

:::{.callout-theorem}
Consider any algorithm that accesses the feasible set $S \subseteq \mathbb{R}^n$ only via a linear minimization oracle (LMO). Let the diameter of the set $S$ be $R$. There exists an $L$-smooth strongly convex function $f : \mathbb{R}^n \to \mathbb{R}$ such that this algorithm requires at least
$$
\min \left( \frac{n}{2}, \frac{LR^2}{16 \varepsilon} \right)
$$
iterations (i.e., calls to the LMO) to construct a point $\hat{x} \in S$ with $f(\hat{x}) - \min\limits_{x \in S} f(x) \leq \varepsilon$. The lower bound applies both for convex and strongly convex functions.
:::

. . .

::::{.columns}

:::{.column width="50%"}

**Sketch of the proof.** Consider the following optimization problem:
$$
\begin{aligned}
\min_{x \in S} f(x) &= \min_{x \in S} \frac{1}{2} \|x\|_2^2 \\
S &= \left\{ x \in \mathbb{R}^n \mid x \geq 0,\ \sum_{i=1}^n x_i = 1 \right\}
\end{aligned}
$$  
:::
:::{.column width="50%" .nonincremental}
Note, that:

- $f$ is $1$-smooth;
- the diameter of $S$ is $R = 2$;
- $f$ is strongly convex.
:::
::::

## Lower bound for Frank-Wolfe method ^[[\faFilePdf The Complexity of Large-scale Convex Programming under a Linear Optimization Oracle](https://arxiv.org/abs/1309.5550)] \faGem \ \faGem

1. The optimal solution is
    $$
    x^* = \frac{1}{n} \mathbf{1} = \frac{1}{n} \sum_{i=1}^n e_i, \quad \text{and} \quad f(x^*) = \frac{1}{2n},
    $$
where $e_i = (0, \dots, 0, \underset{\text{position } i}{1}, 0, \dots, 0)^\top$ is the $i$-th standard basis vector.

1. A linear minimization oracle (LMO) over $S$ returns a vertex $e_i$. After $k$ iterations, the method will have discovered at most $k$ different basis vectors $e_{i_1}, \dots, e_{i_k}$. The best convex combination one can form is
    $$
    \hat{x} = \frac{1}{k} \sum_{j=1}^k e_{i_j}.
    $$

1. Evaluating the function at $\hat{x}$, we obtain:
    $$
    f(\hat{x}) - f(x^*) \geq \frac{1}{2} \left( \frac{1}{\min\{k, n\}} - \frac{1}{n} \right).
    $$

1. To ensure that $f(\hat{x}) - f(x^*) \leq \varepsilon$, it is necessary that (full proof is in the paper):
    $$
    k \geq \min\left\{ \frac{n}{2}, \frac{1}{4\varepsilon} \right\} = \min\left\{ \frac{n}{2}, \frac{L R^2}{16\varepsilon} \right\}.
    $$

## Frank-Wolfe method summary

* Method does not require projections, in some special cases allows to compute iterations in closed form
* Global convergence rate is $O\left(\frac{1}{k}\right)$ for smooth and convex functions. Strong convexity does not improve the rate. This is the lower bound for LMO
* In comparison with projected gradient descent, the rate is worse, but iteration could be cheaper and more sparse
* Recently, it was shown that for strongly convex sets, the rate can be improved to $O\left(\frac{1}{k^2}\right)$ ([\faFilePdf \ paper](https://arxiv.org/abs/1406.1305))
* If we allow away steps, the convergence becomes linear ([\faFilePdf \ paper](https://arxiv.org/abs/1511.05932)) in strongly convex case
* Recent work showed the extension to non-smooth case ([\faFilePdf \ paper](https://arxiv.org/abs/2010.01848)) with convergence rate $O\left(\frac{1}{\sqrt{k}}\right)$
    

# Numerical experiments

## 2d example. Frank-Wolfe method

\includegraphics<1>[center]{fw_2d_0.pdf}
\includegraphics<2>[center]{fw_2d_1.pdf}
\includegraphics<3>[center]{fw_2d_2.pdf}
\includegraphics<4>[center]{fw_2d_3.pdf}
\includegraphics<5>[center]{fw_2d_4.pdf}
\includegraphics<6>[center]{fw_2d_5.pdf}
\includegraphics<7>[center]{fw_2d_6.pdf}
\includegraphics<8>[center]{fw_2d_7.pdf}

## 2d example. Projected gradient descent

\includegraphics<1>[center]{pgd_2d_0.pdf}
\includegraphics<2>[center]{pgd_2d_1.pdf}
\includegraphics<3>[center]{pgd_2d_2.pdf}

## Quadratic function. Box constraints

::::{.columns}

:::{.column width="40%"}

$$
\min_{\substack{x \in \mathbb{R}^n \\ -\mathbf{1} \preceq x \preceq \mathbf{1}}} \frac{1}{2} x^\top A x - b^\top x,
$$

$$
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [\mu; L].
$$

The projection is simple:
$$
\pi_S(x) = \text{clip}(x, -\mathbf{1}, \mathbf{1}).
$$
or
$$
\pi_S(x) = \max\left(-\mathbf{1}, \min(\mathbf{1}, x)\right).
$$

The linear minimization oracle (LMO) for a given gradient $g$ is given by $y = \operatorname*{argmin}\limits_{z \in S} \langle g, z \rangle$.

Since the feasible set is separable across coordinates, the solution is computed coordinate–wise as  
$$
y_i = \begin{cases}
-1, & \text{if } g_i > 0, \\
1,  & \text{if } g_i \le 0.
\end{cases}
$$  
:::
:::{.column width="60%"}

![](conditional_quadratic_80_0_10.pdf)

:::
::::

## Quadratic function. Box constraints

::::{.columns}

:::{.column width="40%"}

$$
\min_{\substack{x \in \mathbb{R}^n \\ -\mathbf{1} \preceq x \preceq \mathbf{1}}} \frac{1}{2} x^\top A x - b^\top x,
$$

$$
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [\mu; L].
$$

The projection is simple:
$$
\pi_S(x) = \text{clip}(x, -\mathbf{1}, \mathbf{1}).
$$
or
$$
\pi_S(x) = \max\left(-\mathbf{1}, \min(\mathbf{1}, x)\right).
$$

The linear minimization oracle (LMO) for a given gradient $g$ is given by $y = \operatorname*{argmin}\limits_{z \in S} \langle g, z \rangle$.

Since the feasible set is separable across coordinates, the solution is computed coordinate–wise as  
$$
y_i = \begin{cases}
-1, & \text{if } g_i > 0, \\
1,  & \text{if } g_i \le 0.
\end{cases}
$$  
:::
:::{.column width="60%"}

![](conditional_quadratic_80_1_10.pdf)

:::
::::

## Quadratic function. Simplex constraints (Lucky problem with diagonal matrix)

::::{.columns}
:::{.column width="40%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ x \ge 0, \mathbf{1}^T x = 1}} \frac{1}{2} x^T A x, \\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [0; 100].
\end{aligned}
$$

| Method | Update time, ms | LMO/Projection time, ms |
|:------:|:---------------:|:---------------:|
| PGD    | 0.0069 | 0.0167 |
| FW     | 0.0070 | 0.0066 |

The projection onto the unit simplex $\pi_S(x)$ can be done in $\mathcal{O}(n \log n)$ or expected $\mathcal{O}(n)$ time. ^[[\faFilePdf \ Efficient Projections onto the $\ell_1$-Ball for Learning in High Dimensions](https://stanford.edu/~jduchi/projects/DuchiShSiCh08.pdf)]

The LMO for a given gradient $g$ is given by $y = \operatorname*{argmin}\limits_{z \in S} \langle g, z \rangle$. The solution corresponds to a vertex of the simplex:
$$
y = e_j \quad \text{where} \quad j = \operatorname*{argmin}_i g_i.
$$

:::
:::{.column width="60%"}
![](fw_simplex_200_mu_0_L_100.0_diag.pdf)

:::
::::

## Quadratic function. Simplex constraints

::::{.columns}
:::{.column width="40%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ x \ge 0, \mathbf{1}^T x = 1}} \frac{1}{2} x^T A x, \\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [0; 100].
\end{aligned}
$$

| Method | Update time, ms | LMO/Projection time, ms |
|:------:|:---------------:|:---------------:|
| PGD    | 0.0069 | 0.0420 |
| FW     | 0.0069 | 0.0066 |

:::
:::{.column width="60%"}
![](fw_simplex_200_mu_0_L_100.0_rotated.pdf)

:::
::::

## Quadratic function. Simplex constraints

::::{.columns}
:::{.column width="40%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ x \ge 0, \mathbf{1}^T x = 1}} \frac{1}{2} x^T A x, \\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [0; 100].
\end{aligned}
$$

| Method | Update time, ms | LMO/Projection time, ms |
|:------:|:---------------:|:---------------:|
| PGD    | 0.0068 | 0.0761 |
| FW     | 0.0069 | 0.0070 |

:::
:::{.column width="60%"}
![](fw_simplex_300_mu_0_L_100.0_rotated.pdf)

:::
::::

## Quadratic function. Simplex constraints

::::{.columns}
:::{.column width="40%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ x \ge 0, \mathbf{1}^T x = 1}} \frac{1}{2} x^T A x, \\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [1; 100].
\end{aligned}
$$

| Method | Update time, ms | LMO/Projection time, ms |
|:------:|:---------------:|:---------------:|
| PGD    | 0.0068 | 0.0752 |
| FW     | 0.0067 | 0.0068 |

:::
:::{.column width="60%"}
![](fw_simplex_300_mu_1_L_100.0_rotated.pdf)

:::
::::

## PGD vs Frank-Wolfe

The key difference between PGD and FW is that PGD requires projection, while FW needs only linear minimization oracle (LMO).

In a recent [book](https://arxiv.org/pdf/2211.14103) authors presented the following comparison table with complexities of linear minimizations and projections on some convex sets up to an additive error $\epsilon$ in the Euclidean norm. 

| **Set**   | **Linear minimization**  | **Projection**    |
|---------------------|-----------------|---------|
| $n$-dimensional $\ell_p$-ball, $p \neq 1,2,\infty$ | $\mathcal{O}(n)$  | $\tilde{\mathcal{O}}\!\bigl(\tfrac{n}{\epsilon^2}\bigr)$|
| Nuclear norm ball of $n\times m$ matrices | $\mathcal{O}\!\Bigl(\nu\,\ln(m + n)\,\tfrac{\sqrt{\sigma_1}}{\sqrt{\epsilon}}\Bigr)$    | $\mathcal{O}\!\bigl(m\,n\,\min\{m,n\}\bigr)$   |
| Flow polytope on a graph with $m$ vertices and $n$ edges (capacity bound on edges) | $\mathcal{O}\!\Bigl((n \log m)\bigl(n + m\,\log m\bigr)\Bigr)$ | $\tilde{\mathcal{O}}\!\bigl(\tfrac{n}{\epsilon^2}\bigr)\ \text{or}\ \mathcal{O}(n^4\,\log n)$    |
| Birkhoff polytope ($n \times n$ doubly stochastic matrices)   | $\mathcal{O}(n^3)$| $\tilde{\mathcal{O}}\!\bigl(\tfrac{n^2}{\epsilon^2}\bigr)$   |

When $\epsilon$ is missing, there is no additive error. The $\tilde{\mathcal{O}}$ hides polylogarithmic factors in the dimensions and polynomial factors in constants related to thedistancetothe optimum. For the nuclear norm ball, i.e., the spectrahedron, $\nu$ denotes the number of non-zero entries and $\sigma_1$ denotes the top singular value of the projected matrix.