---
title: Вспоминаем линейную алгебру. Скорости сходимости.
author: Даня Меркулов
institute: Оптимизация для всех! ЦУ
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back1.jpeg}
---

# Вспоминаем линейную алгебру

## Векторы и матрицы

Мы будем считать, что все векторы являются столбцами по умолчанию. Пространство векторов длины $n$ обозначается $\mathbb{R}^n$, а пространство матриц размера $m \times n$ с вещественными элементами обозначается $\mathbb{R}^{m \times n}$. То есть [^1]:

[^1]: Подробный вводный курс по прикладной линейной алгебре можно найти в книге [Introduction to Applied Linear Algebra -- Vectors, Matrices, and Least Squares](https://web.stanford.edu/~boyd/vmls/) - книга от Stephen Boyd & Lieven Vandenberghe, которая указана в источнике. Также полезен материал по линейной алгебре приведенный в приложении А книги Numerical Optimization by Jorge Nocedal Stephen J. Wright.

$$
x = \begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix} \quad x^T = \begin{bmatrix}
x_1 & x_2 & \dots & x_n
\end{bmatrix} \quad x \in \mathbb{R}^n, x_i \in \mathbb{R}
$$ {#eq-vector}

. . .

Аналогично, если $A \in \mathbb{R}^{m \times n}$ мы обозначаем транспонирование как $A^T \in \mathbb{R}^{n \times m}$:
$$
A = \begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n} \\
a_{21} & a_{22} & \dots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \dots & a_{mn}
\end{bmatrix} \quad A^T = \begin{bmatrix}
a_{11} & a_{21} & \dots & a_{m1} \\
a_{12} & a_{22} & \dots & a_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} & a_{2n} & \dots & a_{mn}
\end{bmatrix} \quad A \in \mathbb{R}^{m \times n}, a_{ij} \in \mathbb{R}
$$
Мы будем писать $x \geq 0$ и $x \neq 0$ для обозначения покомпонентных неравенств

---

![Эквивалентные представления вектора](vector.pdf){#fig-vector}

---

Матрица $A$ называется симметричной, если $A = A^T$. Обозначается как $A \in \mathbb{S}^n$ (множество квадратных симметричных матриц размерности $n$). Заметим, что только квадратная матрица может быть симметричной по определению.

. . .

Матрица $A \in \mathbb{S}^n$ называется **положительно (отрицательно) определенной**, если для всех $x \neq 0 : x^T Ax > (<) 0$. Обозначается как $A \succ (\prec) 0$. Множество таких матриц обозначается как $\mathbb{S}^n_{++} (\mathbb{S}^n_{- -})$

. . .

Матрица $A \in \mathbb{S}^n$ называется **положительно (отрицательно) полуопределенной**, если для всех $x : x^T Ax \geq (\leq) 0$. Обозначается как $A \succeq (\preceq) 0$. Множество таких матриц обозначается как $\mathbb{S}^n_{+} (\mathbb{S}^n_{-})$

:::{.callout-question}
Верно ли, что положительно определенная матрица имеет все положительные элементы?
:::

. . .

:::{.callout-question}
Верно ли, что если матрица симметрична, то она должна быть положительно определенной?
:::

. . .

:::{.callout-question}
Верно ли, что если матрица положительно определена, то она должна быть симметричной?
:::


---

## Матричное умножение (matmul)

Пусть $A$ - матрица размера $m \times n$, а $B$ - матрица размера $n \times p$, тогда их произведение $AB$ равно:
$$
C = AB
$$
Тогда $C$ - матрица размера $m \times p$, элемент $(i, j)$ которой равен:
$$
c_{ij} = \sum_{k=1}^n a_{ik}b_{kj}.
$$

Эта операция в наивной форме требует $\mathcal{O}(n^3)$ арифметических операций, где $n$ обычно считается наибольшей размерностью матриц.

. . .

:::{.callout-question}
Возможно ли умножить две матрицы быстрее, чем за $\mathcal{O}(n^3)$? Как насчет $\mathcal{O}(n^2)$, $\mathcal{O}(n)$?
:::

---

## Умножение матрицы на вектор (matvec)

Пусть $A$ - матрица размера $m \times n$, а $x$ - вектор длины $n$, тогда $i$-й элемент произведения $Ax$ равен:
$$
z = Ax
$$
равен:
$$
z_i = \sum_{k=1}^n a_{ik}x_k
$$

Эта операция в наивной форме требует $\mathcal{O}(n^2)$ арифметических операций, где $n$ обычно считается наибольшей размерностью входов.

Отметим, что:

* $C = AB \quad C^T = B^T A^T$
* $AB \neq BA$
* $e^{A} =\sum\limits_{k=0}^{\infty }{1 \over k!}A^{k}$
* $e^{A+B} \neq e^{A} e^{B}$ (но если $A$ и $B$ коммутируют, то есть $AB = BA$, то $e^{A+B} = e^{A} e^{B}$)
* $\langle x, Ay\rangle = \langle A^T x, y\rangle$

## Нормы

Норма - это **количественная мера малости вектора** и обычно обозначается как $\Vert x \Vert$.

Норма должна удовлетворять определенным свойствам:

1.  $\Vert \alpha x \Vert = \vert \alpha\vert \Vert x \Vert$, $\alpha \in \mathbb{R}$
2.  $\Vert x + y \Vert \leq \Vert x \Vert + \Vert y \Vert$ (неравенство треугольника)
3.  Если $\Vert x \Vert = 0$, то $x = 0$

. . .

Расстояние между двумя векторами определяется как
$$ 
d(x, y) = \Vert x - y \Vert. 
$$
Наиболее широко используемой нормой является **Евклидова норма**:
$$
\Vert x \Vert_2 = \sqrt{\sum_{i=1}^n |x_i|^2},
$$
которая соответствует расстоянию в нашей реальной жизни. Если векторы имеют комплексные элементы, мы используем их модуль. Евклидова норма, или $2$-норма, является подклассом важного класса $p$-норм:

$$
\Vert x \Vert_p = \Big(\sum_{i=1}^n |x_i|^p\Big)^{1/p}. 
$$

---

## $p$-норма вектора

Существуют два очень важных частных случая. Бесконечность-норма, или норма Чебышева, определяется как максимальное абсолютное значение элемента вектора:
$$
\Vert x \Vert_{\infty} = \max_i | x_i| 
$$

. . .

$l_1$ норма (или **манхэттенское расстояние**) определяется как сумма модулей элементов вектора $x$:

$$
\Vert x \Vert_1 = \sum_i |x_i| 
$$

. . .

$l_1$ норма играет очень важную роль: она все связана с методами **compressed sensing**, которые появились в середине 00-х как одна из популярных тем исследований. Код для изображения ниже доступен [*здесь:*](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Balls_p_norm.ipynb). Также посмотрите [*это*](https://fmin.xyz/docs/theory/balls_norm.mp4) видео.

![Шары в разных нормах на плоскости](p_balls.pdf)

## Матричные нормы

В некотором смысле между матрицами и векторами нет большой разницы (вы можете векторизовать матрицу), и здесь появляется самая простая матричная норма **Фробениуса**:
$$
\Vert A \Vert_F = \left(\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2\right)^{1/2}
$$

. . .

Спектральная норма, $\Vert A \Vert_2$ является одной из наиболее широко используемых матричных норм (наряду с нормой Фробениуса).

$$
\Vert A \Vert_2 = \sup_{x \ne 0} \frac{\Vert A x \Vert_2}{\Vert x \Vert_{2}},
$$

Она не может быть вычислена непосредственно из элементов с помощью простой формулы, как в случае нормы Фробениуса, однако, существуют эффективные алгоритмы для ее вычисления. Она напрямую связана с **сингулярным разложением** (SVD) матрицы. Для неё справедливо:

$$
\Vert A \Vert_2 = \sigma_1(A) = \sqrt{\lambda_{\max}(A^TA)}
$$

где $\sigma_1(A)$ - наибольшее сингулярное значение матрицы $A$.

## Скалярное произведение

Стандартное **скалярное произведение** между векторами $x$ и $y$ из $\mathbb{R}^n$ равно:
$$
\langle x, y \rangle = x^T y = \sum\limits_{i=1}^n x_i y_i = y^T x =  \langle y, x \rangle
$$

Здесь $x_i$ и $y_i$ - $i$-ые компоненты соответствующих векторов.

::: {.callout-example}
Докажите, что вы можете переставить матрицу внутри скалярного произведения с транспонированием: $\langle x, Ay\rangle = \langle A^Tx, y\rangle$ и $\langle x, yB\rangle = \langle xB^T, y\rangle$
:::

## Скалярное произведение матриц

Стандартное **скалярное произведение** между матрицами $X$ и $Y$ из $\mathbb{R}^{m \times n}$ равно:

$$
\langle X, Y \rangle = \text{tr}(X^T Y) = \sum\limits_{i=1}^m\sum\limits_{j=1}^n X_{ij} Y_{ij} =  \text{tr}(Y^T X) =  \langle Y, X \rangle
$$

::: {.callout-question} 
Существует ли связь между нормой Фробениуса $\Vert \cdot \Vert_F$ и скалярным произведением между матрицами $\langle \cdot, \cdot \rangle$?
:::


## Собственные вектора и собственные значения

Число $\lambda$ является собственным значением квадратной матрицы $A$ размера $n \times n$, если существует ненулевой вектор $q$ такой, что
$$ 
Aq = \lambda q. 
$$

Вектор $q$ называется собственным вектором матрицы $A$. Матрица $A$ невырожденная, если ни одно из её собственных значений не равно нулю. Собственные значения симметричных матриц являются вещественными числами, в то время как несимметричные матрицы могут иметь комплексные собственные значения. Если матрица положительно определена и симметрична, то все её собственные значения являются положительными вещественными числами.

## Собственные вектора и собственные значения

:::{.callout-theorem}
$$
A \succeq (\succ) 0 \Leftrightarrow \text{все собственные значения } A \text{ } \geq (>) 0 
$$

:::{.callout-proof collapse="true"}
1. $\rightarrow$ Предположим, что некоторое собственное значение $\lambda$ отрицательно, и пусть $x$ обозначает соответствующий собственный вектор. Тогда
$$
Ax = \lambda x \rightarrow x^T Ax = \lambda x^T x < 0
$$
что противоречит условию $A \succeq 0$.
2. $\leftarrow$ Для любой симметричной матрицы мы можем выбрать набор собственных векторов $v_1, \dots, v_n$, которые образуют ортонормированный базис в $\mathbb{R}^n$. Возьмем любой вектор $x \in \mathbb{R}^n$.
$$
\begin{split}
x^T A x &= (\alpha_1 v_1 + \ldots + \alpha_n v_n)^T A (\alpha_1 v_1 + \ldots + \alpha_n v_n)\\
&= \sum \alpha_i^2 v_i^T A v_i = \sum \alpha_i^2 \lambda_i v_i^T v_i \geq 0
\end{split}
$$
Здесь мы использовали тот факт, что $v_i^T v_j = 0$, для $i \neq j$.
:::
:::

## Спектральное разложение (eigendecomposition)

Пусть $A \in S_n$, т.е. $A$ - вещественная симметричная матрица размера $n \times n$. Тогда $A$ может быть разложена как

$$ 
A = Q\Lambda Q^T,
$$

. . .

где $Q \in \mathbb{R}^{n \times n}$ ортогональная, т.е. удовлетворяет $Q^T Q = I$, и $\Lambda = \text{diag}(\lambda_1, \ldots , \lambda_n)$. Вещественные числа $\lambda_i$ являются собственными значениями $A$ и являются корнями характеристического полинома $\text{det}(A - \lambda I)$. Столбцы $Q$ образуют ортонормированный набор собственных векторов $A$. Такое разложение называется спектральным. [^2]

[^2]: Хорошая шпаргалка с разложением матриц доступна на сайте курса по линейной алгебре [website](https://nla.skoltech.ru/_files/decompositions.pdf).

. . .

Мы обычно упорядочиваем вещественные собственные значения как $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n$. Мы используем обозначение $\lambda_i(A)$ для обозначения $i$-го наибольшего собственного значения $A \in S$. Мы обычно пишем наибольшее или максимальное собственное значение как $\lambda_1(A) = \lambda_{\text{max}}(A)$, и наименьшее или минимальное собственное значение как $\lambda_n(A) = \lambda_{\text{min}}(A)$.

## Собственные значения

Наибольшее и наименьшее вещественныесобственные значения удовлетворяют

$$
\lambda_{\text{min}} (A) = \inf_{x \neq 0} \dfrac{x^T Ax}{x^T x}, \qquad \lambda_{\text{max}} (A) = \sup_{x \neq 0} \dfrac{x^T Ax}{x^T x}
$$

. . .

и, следовательно, $\forall x \in \mathbb{R}^n$ (соотношение Рэлея):

$$
\lambda_{\text{min}} (A) x^T x \leq x^T Ax \leq \lambda_{\text{max}} (A) x^T x
$$

. . .

**Число обусловленности** невырожденной матрицы определяется как

$$
\kappa(A) = \|A\|\|A^{-1}\|
$$

. . .

Если мы используем спектральную матричную норму, мы можем получить:

$$
\kappa(A) = \dfrac{\sigma_{\text{max}}(A)}{\sigma _{\text{min}}(A)}
$$

Если, кроме того, $A \in \mathbb{S}^n_{++}$: $\kappa(A) = \dfrac{\lambda_{\text{max}}(A)}{\lambda_{\text{min}}(A)}$

## Число обусловленности

![](conditions.pdf)

## Сингулярное разложение (SVD)

Пусть $A \in \mathbb{R}^{m \times n}$ с рангом $A = r$. Тогда $A$ может быть разложена как

$$
A = U \Sigma V^T 
$$

. . .

где $U \in \mathbb{R}^{m \times r}$ удовлетворяет $U^T U = I$, $V \in \mathbb{R}^{n \times r}$ удовлетворяет $V^T V = I$, и $\Sigma$ является диагональной матрицей с $\Sigma = \text{diag}(\sigma_1, ..., \sigma_r)$, такой что

. . .

$$
\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r > 0. 
$$

. . .

Это разложение называется **сингулярным разложением (SVD)** матрицы $A$. Столбцы $U$ называются левыми сингулярными векторами $A$, столбцы $V$ называются правыми сингулярными векторами, и числа $\sigma_i$ являются сингулярными значениями. Сингулярное разложение может быть записано как

$$
A = \sum_{i=1}^{r} \sigma_i u_i v_i^T,
$$

где $u_i \in \mathbb{R}^m$ являются левыми сингулярными векторами, и $v_i \in \mathbb{R}^n$ являются правыми сингулярными векторами.

## Сингулярное разложение

::: {.callout-question}
Пусть $A \in \mathbb{S}^n_{++}$. Что мы можем сказать о связи между его собственными значениями и сингулярными значениями?
:::

. . .

::: {.callout-question}
Как сингулярные значения матрицы связаны с её собственными значениями, особенно для симметричной матрицы?
:::

## Ранговое разложение (Skeleton decomposition)

:::: {.columns}

::: {.column width="70%"}
Простое, но очень интересное разложение - это ранговое разложение, которое может быть записано в двух формах:

$$
A = U V^T \quad A = \hat{C}\hat{A}^{-1}\hat{R}
$$

. . .

Последнее выражение относится к забавному факту: вы можете случайным образом выбрать $r$ линейно независимых столбцов матрицы и любые $r$ линейно независимых строк матрицы и хранить только их с возможностью точно (!) восстановить всю матрицу.

. . .

Применения для рангового разложения:

* Сжатие модели, сжатие данных и ускорение вычислений в численном анализе: для матрицы ранга $r$ с $r \ll n, m$ необходимо хранить $\mathcal{O}((n + m)r) \ll nm$ элементов.
* Извлечение признаков в машинном обучении
* Все приложения, где применяется SVD, так как ранговое разложение может быть преобразовано в форму усеченного SVD.
:::

::: {.column width="30%"}
![Иллюстрация рангового разложения](skeleton.pdf){#fig-skeleton}
:::

::::

## Каноническое тензорное разложение

Можно рассмотреть обобщение рангового разложения на структуры данных более высокого порядка, такие как тензоры, что означает представление тензора в виде суммы $r$ простых тензоров.

![Иллюстрация канонического тензорного разложения](cp.pdf){width=40%}

::: {.callout-example} 
Заметьте, что существует множество тензорных разложений: каноническое, Таккера, тензорный поезд (TT), тензорное кольцо (TR) и другие. В случае тензоров мы не имеем прямого определения *ранга* для всех типов разложений. Например, для разложения Тензорного поезда ранг является не скаляром, а вектором.
:::

## Определитель и след матрицы

Определитель и след матрицы могут быть выражены через собственные значения
$$
\text{det} A = \prod\limits_{i=1}^n \lambda_i, \qquad \text{tr} A = \sum\limits_{i=1}^n \lambda_i
$$
Определитель имеет несколько интересныхсвойств. Например,  

* $\text{det} A = 0$ тогда и только тогда, когда $A$ является вырожденной; 
* $\text{det}  AB = (\text{det} A)(\text{det}  B)$; 
* $\text{det}  A^{-1} = \frac{1}{\text{det} \ A}$.

. . .

Не забывайте о циклическом свойстве следа для произвольных матриц $A, B, C, D$ (предполагая, что все размерности согласованы):

$$
\text{tr} (ABCD) = \text{tr} (DABC) = \text{tr} (CDAB) = \text{tr} (BCDA)
$$

. . .

::: {.callout-question} 
Как определитель матрицы связан с её обратимостью?
:::

# Скорости сходимости

## Скорость сходимости

![Разница в скоростях сходимости](convergence.pdf)

## Линейная сходимость

Чтобы сравнить производительность алгоритмов, мы должны определить термины для различных типов сходимости. Пусть $r_k$ - последовательность неотрицательных вещественных чисел, которая сходится к нулю. Обычно мы имеем итерационный метод, который производит последовательность итераций $x_k$, приближающихся к оптимальному решению $x^*$, и $r_k = \|x_k - x^*\|_2$.

**Линейная сходимость** последовательности $r_k$ определяется следующим образом:

Последовательность $\{r_k\}_{k=m}^\infty$ сходится линейно с параметром $0 < q < 1$, если существует константа $C > 0$ такая, что:
$$
r_k \leq C q^k, \quad \text{for all } k \geq m.
$$
Если такое $q$ существует, то последовательность называется линейно сходящейся. **Точная нижняя граница** всех $q$, удовлетворяющих неравенству, называется **скоростью линейной сходимости** последовательности.

:::{.callout-question}
Предположим, у вас есть две последовательности с линейными скоростями сходимости $q_1 = 0.1$ и $q_2 = 0.7$, какая из них быстрее?
:::

## Линейная сходимость

:::{.callout-example}
Предположим, у нас есть следующая последовательность:

$$
r_k = \dfrac{1}{2^k}
$$

Можно сразу заключить, что мы имеем линейную сходимость с параметрами $q = \dfrac{1}{2}$ и $C = 0$.
:::

:::{.callout-question}
Определите сходимость следующей последовательности 
$$
r_k = \dfrac{3}{2^k}
$$

:::

## Сублинейная сходимость

Если последовательность $r_k$ сходится к нулю, но не имеет линейной сходимости, то сходимость называется сублинейной. Иногда мы можем рассмотреть следующий частный случай сублинейной сходимости:
$$
\| x_{k+1} - x^* \|_2 \leq C k^{q},
$$
где $q < 0$ и $0 < C < \infty$. Интуитивно, сублинейная сходимость означает, что последовательность сходится медленнее любой геометрической прогрессии.

## Сверхлинейная сходимость

Сходимость последовательности $\{r_k\}_{k=m}^\infty$ называется **сверхлинейной**, если она сходится к нулю быстрее любой линейно сходящейся последовательности. Проверьте, что последовательность $\{r_k\}_{k=m}^\infty$ является сверхлинейной, если она сходится линейно с параметром $q = 0$. 

Для $p > 1$, последовательность имеет **сверхлинейную сходимость порядка $p$**, если существует $C > 0$ и $0 < q < 1$ такая, что:
$$
r_k \leq C q^{p^k}, \quad \text{for all } k \geq m.
$$
Когда $p = 2$, это называется **квадратичной сходимостью**.

. . .

::: {.callout-example}

### **Важный пример**
Предположим, что $x^* = 1.23456789$ (истинное решение), и итерационная последовательность начинается с ошибки $r_k = 10^{-3}$, соответствующей 3 правильным значащим цифрам ($1.234$).

1. После первой итерации:
   $$
   r_{k+1} \approx r_k^2 = (10^{-3})^2 = 10^{-6}.
   $$
   Теперь ошибка равна $10^{-6}$, и мы имеем 6 правильных значащих цифр ($1.23456$).

2. После второй итерации:
   $$
   r_{k+2} \approx r_{k+1}^2 = (10^{-6})^2 = 10^{-12}.
   $$
   Теперь ошибка равна $10^{-12}$, и мы имеем 12 правильных значащих цифр ($1.234567890123$).

:::






## Практические наблюдения о скоростях сходимости

* $\|x_{k+1} - x^*\|_2 \leq \frac{1}{k^{\frac1p}} \|x_0 - x^*\|_2$ означает сублинейную скорость сходимости
* $\|x_{k+1} - x^*\|_2 \leq q \|x_k - x^*\|_2$ означает линейную скорость сходимости, где $q<1$
* $\|x_{k+1} - x^*\|_2 \leq q \|x_k - x^*\|_2^2$ означает квадратичную скорость сходимости, где $q\|x_0 - x^*\|<1$

## Тест корней

:::{.callout-theorem}
Пусть $(r_k)_{k=m}^\infty$ - последовательность неотрицательных чисел, сходящаяся к нулю, и пусть $\alpha := \limsup_{k \to \infty} r_k^{1/k}$. (Заметим, что $\alpha \geq 0$.)

(a) Если $0 \leq \alpha < 1$, то $(r_k)_{k=m}^\infty$ сходится линейно с константой $\alpha$.

(b) В частности, если $\alpha = 0$, то $(r_k)_{k=m}^\infty$ сходится сверхлинейно.

(c) Если $\alpha = 1$, то $(r_k)_{k=m}^\infty$ сходится сублинейно.

(d) Случай $\alpha > 1$ невозможен.

**Доказательство**. 

1. Покажем, что если $(r_k)_{k=m}^\infty$ сходится линейно с константой $0 \leq \beta < 1$, то $\alpha \leq \beta$. Действительно, по определению константы линейной сходимости, для любого $\varepsilon > 0$ такого, что $\beta + \varepsilon < 1$, существует $C > 0$ такое, что $r_k \leq C(\beta + \varepsilon)^k$ для всех $k \geq m$. Отсюда, $r_k^{1/k} \leq C^{1/k}(\beta + \varepsilon)$ для всех $k \geq m$. Переходя к пределу при $k \to \infty$ и используя $C^{1/k} \to 1$, мы получаем $\alpha \leq \beta + \varepsilon$. Учитывая произвольность $\varepsilon$, получаем $\alpha \leq \beta$.

1. Таким образом, в случае $\alpha = 1$ последовательность $(r_k)_{k=m}^\infty$ не может иметь линейной сходимости в соответствии с приведенным выше результатом (доказано от противного). Тем не менее, $(r_k)_{k=m}^\infty$ сходится к нулю, поэтому она должна сходиться сублинейно.
:::

## Тест корней

:::{.callout-theorem}
1. Теперь рассмотрим случай $0 \leq \alpha < 1$. Пусть $\varepsilon > 0$ - произвольное число такое, что $\alpha + \varepsilon < 1$. Согласно свойствам limsup, существует $N \geq m$ такое, что $r_k^{1/k} \leq \alpha + \varepsilon$ для всех $k \geq N$. Отсюда, $r_k \leq (\alpha + \varepsilon)^k$ для всех $k \geq N$. Следовательно, $(r_k)_{k=m}^\infty$ сходится линейно с параметром $\alpha + \varepsilon$ (не имеет значения, что неравенство выполняется только для числа $N$). Учитывая произвольность $\varepsilon$, это означает, что константа линейной сходимости $(r_k)_{k=m}^\infty$ не превышает $\alpha$. Поскольку, как показано выше, константа линейной сходимости не может быть меньше $\alpha$, это означает, что константа линейной сходимости $(r_k)_{k=m}^\infty$ точно равна $\alpha$.

1. Наконец, покажем, что случай $\alpha > 1$ невозможен. Действительно, предположим, что $\alpha > 1$. Тогда из определения limsup следует, что для любого $N \geq m$ существует $k \geq N$ такое, что $r_k^{1/k} \geq 1$, и, в частности, $r_k \geq 1$. Но это означает, что $r_k$ имеет подпоследовательность, которая не ограничена от нуля. Следовательно, $(r_k)_{k=m}^\infty$ не может сходиться к нулю, что противоречит условию.
:::

## Тест отношений

Пусть $\{r_k\}_{k=m}^\infty$ - последовательность строго положительных чисел, сходящаяся к нулю. Пусть

$$
q = \lim_{k \to \infty} \dfrac{r_{k+1}}{r_k}
$$

* Если существует $q$ и $0 \leq q <  1$, то $\{r_k\}_{k=m}^\infty$ имеет линейную сходимость с константой $q$.
* В частности, если $q = 0$, то $\{r_k\}_{k=m}^\infty$ имеет сверхлинейную сходимость.
* Если $q$ не существует, но $q = \lim\limits_{k \to \infty} \sup_k \dfrac{r_{k+1}}{r_k} <  1$, то $\{r_k\}_{k=m}^\infty$ имеет линейную сходимость с константой, не превышающей $q$. 
* Если $\lim\limits_{k \to \infty} \inf_k \dfrac{r_{k+1}}{r_k} =1$, то $\{r_k\}_{k=m}^\infty$ имеет сублинейную сходимость. 
* Случай $\lim\limits_{k \to \infty} \inf_k \dfrac{r_{k+1}}{r_k} > 1$ невозможен. 
* В остальных случаях (т.е., когда $\lim\limits_{k \to \infty} \inf_k \dfrac{r_{k+1}}{r_k} <  1 \leq  \lim\limits_{k \to \infty} \sup_k \dfrac{r_{k+1}}{r_k}$) мы не можем сделать никаких конкретных утверждений о скорости сходимости $\{r_k\}_{k=m}^\infty$.

## Лемма о тесте отношений

:::{.callout-theorem}
Пусть $(r_k)_{k=m}^\infty$ - последовательность строго положительных чисел. (Строгая положительность необходима для того, чтобы отношения $\frac{r_{k+1}}{r_k}$, которые появляются ниже, были определены.) Тогда

$$
\liminf_{k \to \infty} \frac{r_{k+1}}{r_k} \leq \liminf_{k \to \infty} r_k^{1/k} \leq \limsup_{k \to \infty} r_k^{1/k} \leq \limsup_{k \to \infty} \frac{r_{k+1}}{r_k}.
$$

**Доказательство**. 

1. Среднее неравенство следует из того, что liminf любой последовательности всегда меньше или равен её limsup. Докажем последнее неравенство; первое доказывается аналогично.

1. Обозначим $L := \limsup_{k \to \infty} \frac{r_{k+1}}{r_k}$. Если $L = +\infty$, то неравенство очевидно, поэтому предположим, что $L$ конечно. Заметим, что $L \geq 0$, поскольку отношение $\frac{r_{k+1}}{r_k}$ положительно для всех $k \geq m$. Пусть $\varepsilon > 0$ - произвольное число. Согласно свойствам limsup, существует $N \geq m$ такое, что $\frac{r_{k+1}}{r_k} \leq L + \varepsilon$ для всех $k \geq N$. Отсюда, $r_{k+1} \leq (L + \varepsilon)r_k$ для всех $k \geq N$. Применяя индукцию, получаем $r_k \leq (L + \varepsilon)^{k-N}r_N$ для всех $k \geq N$. Пусть $C := (L + \varepsilon)^{-N}r_N$. Тогда $r_k \leq C(L + \varepsilon)^k$ для всех $k \geq N$, откуда $r_k^{1/k} \leq C^{1/k}(L + \varepsilon)$. Переходя к limsup при $k \to \infty$ и используя $C^{1/k} \to 1$, получаем $\limsup_{k \to \infty} r_k^{1/k} \leq L + \varepsilon$. Учитывая произвольность $\varepsilon$, получаем $\limsup_{k \to \infty} r_k^{1/k} \leq L$.
:::

# Итоги

## Итоги

:::: {.columns .nonincremental}

::: {.column width="50%"}

### Определения

1. Положительно определённая матрица.
1. Евклидова норма вектора.
1. Неравенство треугольника для нормы.
1. $p$-норма вектора.
1. Как выглядит единичный шар в $p$ - норме на плоскости для $p=1,2,\infty$?
1. Норма Фробениуса для матрицы.
1. Спектральная норма матрицы.
1. Скалярное произведение двух векторов.
1. Скалярное произведение двух матриц, согласованное с нормой Фробениуса.
1. Собственные значения матрицы. Спектр матрицы.
1. Связь спектра матрицы и её определенности.
1. Спектральное разложение матрицы.
1. Сингулярное разложение матрицы.
1. Связь определителя и собственных чисел для квадратной матрицы.
1. Связь следа и собственных чисел для квадратной матрицы.

:::

::: {.column width="50%"}

16. Линейная сходимость последовательности. 
1. Сублинейная сходимость последовательности. 
1. Сверхлинейная сходимость последовательности. 
1. Квадратичная сходимость последовательности.
1. Тест корней для определения скорости сходимости последовательности.
1. Тест отношений для определения скорости сходимости последовательности.

### Теоремы

1. Критерий положительной определенности матрицы через знаки собственных значений матрицы.
1. Тест корней
1. Тест отношений

:::

::::