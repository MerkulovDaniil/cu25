---
title: "Градиентный спуск. Теоремы сходимости в гладком случае (выпуклые, сильно выпуклые, PL). Верхние и нижние оценки сходимости."
author: Даня Меркулов, Петр Остроухов
institute: Оптимизация для всех! ЦУ
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
    beamer-cu:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader_cu.tex
      header-includes:
        - \newcommand{\cover}{../files/БАК_Методы вып_оптимизации_презентация_7.pdf}
    beamer-cu-maga:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader_cu.tex
      header-includes:
        - \newcommand{\cover}{../files/Методы вып_оптимизации_презентация_7.pdf}
header-includes:
 - \newcommand{\bgimage}{../files/back8.jpeg}
---

# Градиентный спуск

## Направление локального наискорейшего спуска

:::: {.columns}

::: {.column width="40%"}
Рассмотрим линейное приближение дифференцируемой функции $f$ вдоль направления $h$, где $\|h\|_2 = 1$:

. . .

$$
f(x + \alpha h) = f(x) + \alpha \langle \nabla f(x), h \rangle + o(\alpha)
$$

. . .

Хотим, чтобы $h$ было направлением убывания:
$$
f(x + \alpha h) - f(x) < 0
$$
$$
\alpha \langle \nabla f(x), h \rangle + o(\alpha) < 0
$$

. . .

Переходя к пределу при $\alpha \to 0$:
$$
\langle \nabla f(x), h \rangle < 0
$$
:::

. . .

::: {.column width="60%"}
Более того, мы хотим, чтобы разница $f(x) - f(x + \alpha h)$ была максимальна:
$$
h = \arg \max_h \left( - \langle \nabla f(x), h \rangle \right) = \arg \min_h \langle \nabla f(x), h \rangle.
$$

Также из неравенства Коши–Буняковского получаем:
$$
\begin{split}
|\langle \nabla f(x), h \rangle | &\leq \| \nabla f(x) \|_2 \| h \|_2 \\
\langle \nabla f(x), h \rangle &\geq -\| \nabla f(x) \|_2 \| h \|_2 = -\| \nabla f(x) \|_2
\end{split}
$$

. . .

Таким образом, направление антиградиента
$$
h = \arg \min_h \langle \nabla f(x), h \rangle = -\dfrac{\nabla f(x)}{\|\nabla f(x)\|_2}
$$
представляет собой направление **наискорейшего локального** убывания функции $f$.


. . .

Итерация метода имеет вид:
$$
x^{k+1} = x^k - \alpha \, \nabla f(x^k)
$$

:::
::::

## Дифференциальное уравнение градиентного потока

:::: {.columns}
::: {.column width="78%"}

Рассмотрим дифференциальное уравнение градиентного потока:
$$
\tag{GF}
\frac{dx}{dt} = -\nabla f(x(t)).
$$

. . .

Дискретизируем его на равномерной сетке с шагом $\alpha$:
$$
\frac{x^{k+1} - x^k}{\alpha} = -\nabla f(x^k),
$$

. . .

где $x^k \equiv x(t_k)$ и $\alpha = t_{k+1} - t_k$ — шаг сетки.

Отсюда получаем выражение для $x^{k+1}$:
$$
x^{k+1} = x^k - \alpha \, \nabla f(x^k),
$$
являющееся точной формулой обновления градиентного спуска.

[Открыть в Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/GD_vs_GF.ipynb)
:::

. . .

::: {.column width="22%"}
![Траектория градиентного потока](GD_vs_GF.pdf)
:::
::::

## Сходимость алгоритма градиентного спуска

[\faPython Код](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/GD_2d_visualization.ipynb) для построения анимации ниже. Сходимость существенно зависит от выбора шага $\alpha$:

[![](gd_2d.png)](https://fmin.xyz/docs/visualizations/gd_lls.mp4)


## Точный линейный поиск (метод наискорейшего спуска)

:::: {.columns}
::: {.column width="80%"}
$$
\alpha_k = \operatorname*{arg\,min}_{\alpha \in \mathbb{R}^+} f\bigl(x^k - \alpha \, \nabla f(x^k)\bigr)
$$
Подход скорее теоретический, чем практический: он удобен для анализа сходимости, но точный линейный поиск часто затруднён, если вычисление функции занимает слишком много времени или стоит слишком дорого.

Интересное теоретическое свойство этого метода заключается в том, что градиенты на соседних итерациях ортогональны. Условие оптимальности по $\alpha_k$ даёт
$$
\left.\dfrac{d}{d\alpha} \, f\bigl(x^k - \alpha \, \nabla f(x^k)\bigr)\right|_{\alpha = \alpha_k} = 0.
$$

. . .

Условия оптимальности:

. . .

$$
\nabla f(x^{k+1})^\top \nabla f(x^k) = 0
$$

:::
::: {.column width="20%"}

![Наискорейший спуск](GD_vs_Steepest.pdf)

[Открыть в Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Steepest_descent.ipynb)
:::
::::

# Сильно выпуклые квадратичные функции

## Сдвиг координат

:::: {.columns}

::: {.column width="70%"}

Рассмотрим следующую задачу квадратичной оптимизации:
$$
\label{problem}
\min\limits_{x \in \mathbb{R}^d} f(x) =  \min\limits_{x \in \mathbb{R}^d} \dfrac{1}{2} x^\top  A x - b^\top  x + c, \text{ где } A \in \mathbb{S}^d_{++}.
$$

. . .

* Во-первых, без ограничения общности мы можем установить $c = 0$, что не повлияет на процесс оптимизации.
* Во-вторых, у нас есть спектральное разложение матрицы $A = Q \Lambda Q^T$.
* Покажем, что мы можем сделать сдвиг координат, чтобы сделать анализ немного проще. Пусть $\hat{x} = Q^T(x - x^*)$, где $x^*$ — точка минимума исходной функции, определяемая как $Ax^* = b$. При этом $x = Q\hat{x} + x^*$.
    $$
    \begin{split}
    \uncover<+->{ f(\hat{x}) &= \frac12  (Q\hat{x} + x^*)^\top  A (Q\hat{x} + x^*) - b^\top  (Q\hat{x} + x^*) \\}
    \uncover<+->{ &= \frac12 \hat{x}^T Q^TAQ\hat{x} + \frac12 (x^*)^T A (x^*) + (x^*)^TAQ\hat{x} - b^T Q\hat{x} - b^T x^*\\}
    \uncover<+->{ &= \frac12 \hat{x}^T \Lambda \hat{x} + \frac12 (x^*)^T A (x^*) + (x^*)^TAQ\hat{x} - (x^*)^T A^TQ\hat{x} - (x^*)^T A x^*\\}
    \uncover<+->{ &= \frac12 \hat{x}^T \Lambda \hat{x} - \frac12 (x^*)^T A x^*} \uncover<+->{\simeq \frac12 \hat{x}^T \Lambda \hat{x} }
    \end{split}
    $$

:::
::: {.column width="30%"}
![](coordinate_shift.pdf)
:::
::::

## Анализ сходимости

Теперь мы можем работать с функцией $f(x) = \frac12 x^T \Lambda x$ с $x^* = 0$ без ограничения общности (убрав крышку из $\hat{x}$)

:::: {.columns}
::: {.column width="50%"}
$$
\begin{split}
\uncover<+->{x^{k+1} &= x^k - \alpha^k \nabla f(x^k)} 
\uncover<+->{= x^k - \alpha^k \Lambda x^k \\ } 
\uncover<+->{&= (I - \alpha^k \Lambda) x^k \\ }
\uncover<+->{ x^{k+1}_{(i)} &= (1 - \alpha^k \lambda_{(i)}) \, x^k_{(i)} \quad \text{для $i$-й координаты} \\ }
\uncover<+->{  x^{k}_{(i)} &= (1 - \alpha \, \lambda_{(i)})^k \, x^0_{(i)} \quad \text{при постоянном шаге } \alpha^k = \alpha}
\end{split}
$$
\uncover<+->{
Используем постоянный шаг $\alpha^k = \alpha$. Условие сходимости:
$$
\rho(\alpha) = \max_{i} |1 - \alpha \lambda_{(i)}| < 1
$$

. . .

Помним, что $\lambda_{\min} = \mu > 0$, $\lambda_{\max} = L \geq \mu$.}

:::: {.columns}

::: {.column width="50%"}
$$
\begin{split}
\uncover<+->{ |1 - \alpha \mu| &< 1 \\ }
\uncover<+->{ -1 < 1 &- \alpha \mu < 1 \\ }
\uncover<+->{ \alpha < \tfrac{2}{\mu} \quad & \quad \alpha\mu > 0}
\end{split}
$$
:::

::: {.column width="50%"}

$$
\begin{split}
\uncover<+->{ |1 - \alpha L| &< 1 \\ }
\uncover<+->{ -1 < 1 &- \alpha L < 1 \\ }
\uncover<+->{ \alpha < \tfrac{2}{L} \quad & \quad \alpha L > 0}
\end{split}
$$
:::
::::

:::

. . .

::: {.column width="50%"}
Выберем $\alpha$, минимизирующий худший знаменатель прогрессии
$$
\begin{split}
\uncover<+->{ \rho^* &=  \min_{\alpha} \rho(\alpha) } \uncover<+->{  = \min_{\alpha} \max_{i} |1 - \alpha \lambda_{(i)}| \\ }
\uncover<+->{ &=  \min_{\alpha} \max \left\{|1 - \alpha \mu|, |1 - \alpha L| \right\} \\ }
\uncover<+->{ \alpha^* &: \quad  1 - \alpha^* \mu = \alpha^* L - 1 \\ }
\uncover<+->{ & \alpha^* = \frac{2}{\mu + L} } \uncover<+->{ \quad \rho^* = \frac{L - \mu}{L + \mu} \\ }
\uncover<+->{ |x^{k}_{(i)}| &\leq \left( \frac{L - \mu}{L + \mu} \right)^k |x^0_{(i)}| \\}
\uncover<+->{ \|x^{k}\|_2 &\leq \left( \frac{L - \mu}{L + \mu} \right)^k \|x^0\|_2 } \uncover<+->{ \quad f(x^{k}) \leq \left( \frac{L - \mu}{L + \mu} \right)^{2k} f(x^0)}
\end{split}
$$
:::
::::

## Анализ сходимости

Таким образом, имеем линейную сходимость по аргументу со скоростью $\frac{\varkappa - 1}{\varkappa + 1} = 1 - \frac{2}{\varkappa + 1}$, где $\varkappa = \frac{L}{\mu}$ — *число обусловленности* квадратичной задачи.

| $\varkappa$ | $\rho$ | Итераций до уменьшения ошибки по аргументу в $10$ раз | Итераций до уменьшения ошибки по функции в $10$ раз |
|:-:|:-:|:-----------:|:-----------:|
| $1.1$ | $0.05$ | $1$ | $1$ |
| $2$ | $0.33$ | $3$ | $2$ |
| $5$ | $0.67$ | $6$ | $3$ |
| $10$ | $0.82$ | $12$ | $6$ |
| $50$ | $0.96$ | $58$ | $29$ |
| $100$ | $0.98$ | $116$ | $58$ |
| $500$ | $0.996$ | $576$ | $288$ |
| $1000$ | $0.998$ | $1152$ | $576$ |

## Число обусловленности $\varkappa$

[![](condition_number_gd.pdf)](https://fmin.xyz/docs/visualizations/condition_number_gd.mp4)

# Случай PL-функций

## PL-функции. Линейная сходимость градиентного спуска без выпуклости

Говорят, что $f$ удовлетворяет условию Поляка-Лоясиевича (PL), если для некоторого $\mu > 0$ выполняется
$$
\Vert \nabla f(x) \Vert^2 \geq 2 \mu (f(x) - f^*) \quad \forall x
$$
Интересно, что градиентный спуск может сходиться линейно даже без выпуклости.

Следующие функции удовлетворяют условию PL, но не являются выпуклыми. [\faPython Код](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/PL_function.ipynb)

:::: {.columns}

::: {.column width="50%"}

$$
f(x) = x^2 + 3\sin^2(x)
$$

![PL-функция](pl_2d.pdf){width=67%}

:::

. . .

::: {.column width="50%"}

$$
f(x,y) = \dfrac{(y - \sin x)^2}{2}
$$

![PL-функция](pl_3d.pdf){width=84%}

:::
::::

## Анализ сходимости

:::{.callout-theorem}
Рассмотрим задачу
$$
f(x) \to \min_{x \in \mathbb{R}^d}
$$
и предположим, что $f$ является PL-функцией с константой $\mu$ и $L$-гладкой, для некоторых $L\geq \mu > 0$.

Рассмотрим последовательность $(x^k)_{k \in \mathbb{N}}$, сгенерированную методом градиентного спуска из точки $x^0$ с постоянным шагом $\alpha$, удовлетворяющим $0<\alpha \leq \frac{1}{L}$. Пусть $f^* = \min\limits_{x \in \mathbb{R}^d} f(x)$. Тогда:
$$
f(x^{k})-f^* \leq (1-\alpha \mu)^k (f(x^0)-f^*).
$$
:::

## Анализ сходимости

Используем $L$-гладкость вместе с правилом обновления, чтобы записать:
$$
\begin{split}
\uncover<+->{ f(x^{k+1})& \leq f(x^{k}) + \langle \nabla f(x^{k}), x^{k+1}-x^{k} \rangle +\frac{L}{2} \| x^{k+1}-x^{k}\|^2\\ }
\uncover<+->{ &= f(x^{k})-\alpha\Vert \nabla f(x^{k}) \Vert^2 +\frac{L \alpha^2}{2} \| \nabla f(x^{k})\|^2 \\ }
\uncover<+->{ &= f(x^{k}) - \frac{\alpha}{2} \left(2 - L \alpha \right)\Vert \nabla f(x^{k}) \Vert^2 \\ }
\uncover<+->{ & \leq f(x^{k}) - \frac{\alpha}{2}\Vert \nabla f(x^{k})\Vert^2,}
\end{split}
$$

. . .

где в последнем неравенстве использована гипотеза о шаге $\alpha L \leq 1$.

. . .

Теперь используем свойство PL-функции и получаем:
$$
f(x^{k+1}) \leq f(x^{k}) - \alpha \mu (f(x^{k}) - f^*).
$$
Вычтя $f^*$ из обеих частей этого неравенства и применив рекурсию, мы получим искомый результат.

## Любая $\mu$-сильно выпуклая дифференцируемая функция является PL-функцией

:::{.callout-theorem}
Если функция $f(x)$ дифференцируема и $\mu$-сильно выпукла, то она является PL-функцией.
:::

**Доказательство**

:::: {.columns}

::: {.column width="60%"}

По критерию сильной выпуклости первого порядка:
$$
f(y) \geq f(x) + \nabla f(x)^T(y-x) + \tfrac{\mu}{2}\|y-x\|_2^2
$$
Положим $y = x^*$:
$$
\begin{split}
\uncover<+->{ f(x^*) &\geq f(x) + \nabla f(x)^T(x^*-x) + \tfrac{\mu}{2}\|x^*-x\|_2^2 \\ }
\uncover<+->{ f(x) - f(x^*) &\leq \nabla f(x)^T(x-x^*) - \tfrac{\mu}{2}\|x^*-x\|_2^2 = \\ }
\uncover<+->{ &= \left(\nabla f(x)^T - \tfrac{\mu}{2}(x^*-x)\right)^T (x-x^*) = \\ }
\uncover<+->{ &= \frac12 \left(\tfrac{2}{\sqrt{\mu}}\nabla f(x)^T - \sqrt{\mu}(x^*-x)\right)^T \sqrt{\mu}(x-x^*)\\ }
\end{split}
$$
:::

. . .

::: {.column width="40%"}

Пусть $a = \frac{1}{\sqrt{\mu}}\nabla f(x)$ и $b =\sqrt{\mu}(x-x^*) -\frac{1}{\sqrt{\mu}}\nabla f(x)$ 

. . .

Тогда $a+b = \sqrt{\mu}(x-x^*)$ и $a-b=\frac{2}{\sqrt{\mu}}\nabla f(x)-\sqrt{\mu}(x-x^*)$
:::
::::

## Любая $\mu$-сильно выпуклая дифференцируемая функция является PL-функцией
$$
\begin{split}
\uncover<+->{ f(x) - f(x^*) &\leq \frac12 \left(\frac{1}{\mu}\|\nabla f(x)\|^2_2 - \left\|\sqrt{\mu}(x-x^*) -\frac{1}{\sqrt{\mu}}\nabla f(x)\right\|_2^2\right) \\ }
\uncover<+->{ f(x) - f(x^*) &\leq \frac{1}{2\mu}\|\nabla f(x)\|^2_2, \\ }
\end{split}
$$

. . .

которое является точным условием PL. Это означает, что мы уже имеем доказательство линейной сходимости для любой сильно выпуклой функции.

# Выпуклый гладкий случай

## Выпуклый гладкий случай

:::{.callout-theorem}
Рассмотрим задачу
$$
f(x) \to \min_{x \in \mathbb{R}^d}
$$
и предположим, что $f$ является выпуклой и $L$-гладкой функцией, для некоторого $L>0$.

Пусть $(x^{k})_{k \in \mathbb{N}}$ — последовательность итераций, сгенерированная методом градиентного спуска из точки $x^0$ с постоянным шагом $\alpha$, удовлетворяющим $0 < \alpha\leq \frac{1}{L}$. Пусть $f^* = \min\limits_{x \in \mathbb{R}^d} f(x)$. Тогда для всех $x^* \in {\rm argmin}\, f$ и всех $k \in \mathbb{N}$ справедливо:
$$
f(x^{k})-f^* \leq \frac{\|x^0-x^*\| ^2}{2 \alpha k}.
$$
:::

## Анализ сходимости

* Как и раньше, сначала используем гладкость:
    $$
    \begin{split}
    \uncover<+->{ f(x^{k+1})& \leq f(x^{k}) + \langle \nabla f(x^{k}), x^{k+1}-x^{k} \rangle +\frac{L}{2} \| x^{k+1}-x^{k}\|^2\\ }
    \uncover<+->{ &= f(x^{k})-\alpha\Vert \nabla f(x^{k}) \Vert^2 +\frac{L \alpha^2}{2} \| \nabla f(x^{k})\|^2 \\ }
    \uncover<+->{ &= f(x^{k}) - \frac{\alpha}{2} \left(2 - L \alpha \right)\Vert \nabla f(x^{k}) \Vert^2 \\ }
    \uncover<+->{ & \leq f(x^{k}) - \frac{\alpha}{2}\Vert \nabla f(x^{k})\Vert^2, \\ }
    \uncover<+->{ f(x^{k}) - f(x^{k+1}) & \geq \dfrac{1}{2L} \Vert \nabla f(x^{k})\Vert^2 \quad \text{если } \alpha = \tfrac1L }
    \end{split}
    $$ {#eq-gd-cs-smoothness}

    . . .

    Обычно для сходящегося градиентного спуска чем больше допустимый шаг, тем быстрее сходимость, поэтому часто берут $\alpha = \tfrac1L$.
 * После этого используем выпуклость:
    $$
    \begin{split}
    \uncover<+->{ f(y) &\geq f(x) + \langle \nabla f(x), y-x\rangle} \uncover<+->{ \text{ где } y = x^*, x = x^k} \\
    \uncover<+->{f(x^k) - f^* &\leq \langle \nabla f(x^k), x^k-x^*\rangle }
    \end{split}
    $$ {#eq-gd-cs-convexity}

## Анализ сходимости

* Теперь подставляем (-@eq-gd-cs-convexity) в (-@eq-gd-cs-smoothness):
    $$
    \begin{split}
    \uncover<+->{ f(x^{k+1}) &\leq f(x^{k}) -\frac{\alpha}{2} \Vert \nabla f(x^{k})\Vert^2 \leq f^* + \langle \nabla f(x^k), x^k-x^*\rangle - \frac{\alpha}{2} \Vert \nabla f(x^{k})\Vert^2 \\ }
    \uncover<+->{ &= f^* + \langle \nabla f(x^k), x^k-x^* - \frac{\alpha}{2} \nabla f(x^{k})\rangle \\ }
    \uncover<+->{ &= f^* + \frac{1}{2 \alpha}\left\langle \alpha \nabla f(x^k), 2\left(x^k-x^* - \frac{\alpha}{2} \nabla f(x^{k})\right)\right\rangle }
    \end{split}
    $$
    \uncover<+->{ Пусть $a = x^k-x^*$ и $b = x^k-x^* - \alpha\nabla f(x^k)$.} \uncover<+->{Тогда $a+b = \alpha \nabla f(x^k)$ и $a-b=2\left(x^k-x^* - \frac{\alpha}{2} \nabla f(x^{k})\right)$.}
    $$
    \begin{split}
    \uncover<+->{ f(x^{k+1}) &\leq f^* + \frac{1}{2 \alpha}\left[ \|x^k-x^*\|_2^2 - \|x^k-x^* - \alpha\nabla f(x^k)\|_2^2\right] \\ }
    \uncover<+->{ &\leq f^* + \frac{1}{2 \alpha}\left[ \|x^k-x^*\|_2^2 - \|x^{k+1}-x^*\|_2^2\right] \\ }
    \uncover<+->{ 2\alpha \left(f(x^{k+1}) - f^*\right) &\leq \|x^k-x^*\|_2^2 - \|x^{k+1}-x^*\|_2^2 }
    \end{split}
    $$
* Просуммируем по $i = 0,\dots, k-1$. Большинство слагаемых обнуляется из-за телескопической суммы:
    $$
    \begin{split}
    \uncover<+->{2\alpha \sum\limits_{i=0}^{k-1} \left(f(x^{i+1}) - f^*\right) &\leq \|x^0-x^*\|_2^2 - \|x^{k}-x^*\|_2^2} \uncover<+->{ \leq \|x^0-x^*\|_2^2 }
    \end{split}
    $$ {#eq-gd-sc-telescopic}

## Анализ сходимости

* Поскольку на каждой итерации $f(x^{i+1}) \leq f(x^i)$, то
    $$
    kf(x^k) \leq \sum\limits_{i=0}^{k-1}f(x^{i+1})
    $$
* Теперь подставим это в (-@eq-gd-sc-telescopic):
    $$
    \begin{split}
    \uncover<+->{ 2\alpha kf(x^k) - 2\alpha kf^* &\leq 2\alpha \sum\limits_{i=0}^{k-1} \left(f(x^{i+1}) - f^*\right)  \leq \|x^0-x^*\|_2^2 \\ }
    \uncover<+->{ f(x^k) - f^* &\leq \frac{\|x^0-x^*\|_2^2}{2 \alpha k} } \uncover<+->{ \leq  \frac{L \|x^0-x^*\|_2^2}{2 k} }
    \end{split}
    $$

## Итог

$$
\text{Градиентный спуск:} \qquad \qquad \min_{x \in \mathbb{R}^n} f(x) \qquad \qquad x^{k+1} = x^k - \alpha^k \nabla f(x^k)
$$



| гладкий (не выпуклый) | гладкий и выпуклый | гладкий и сильно выпуклый (или PL) |
|:-----:|:-----:|:--------:|
| $\|\nabla f(x^k)\|^2 \sim \mathcal{O} \left( \dfrac{1}{k} \right)$ | $f(x^k) - f^* \sim  \mathcal{O} \left( \dfrac{1}{k} \right)$ | $\|x^k - x^*\|^2 \sim \mathcal{O} \left( \left(1 - \dfrac{\mu}{L}\right)^k \right)$ |
| $k_\varepsilon \sim \mathcal{O} \left( \dfrac{1}{\varepsilon} \right)$ | $k_\varepsilon \sim  \mathcal{O}  \left( \dfrac{1}{\varepsilon} \right)$ | $k_\varepsilon  \sim \mathcal{O} \left( \varkappa \log \dfrac{1}{\varepsilon}\right)$ |

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](gd_random_0.001_100_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](gd_random_10_100_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](gd_random_10_1000_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](gd_clustered_10_1000_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](gd_clustered_10_1000_600.pdf)

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![]("gd_uniform spectrum_1_100_60.pdf")

## Численные эксперименты

$$
f(x) = \frac{1}{2} x^T A x - b^T x \to \min_{x \in \mathbb{R}^n}
$$

![](gd_Hilbert_1_10_60.pdf)

## Численные эксперименты

$$
f(x) = \frac{\mu}{2} \|x\|_2^2 + \frac1m \sum_{i=1}^m \log (1 + \exp(- y_i \langle a_i, x \rangle)) \to \min_{x \in \mathbb{R}^n}
$$

![]("GD 0.07GD 0.9GD 10.0_0.pdf"){fig-align="center" width=95%}

## Численные эксперименты

$$
f(x) = \frac{\mu}{2} \|x\|_2^2 + \frac1m \sum_{i=1}^m \log (1 + \exp(- y_i \langle a_i, x \rangle)) \to \min_{x \in \mathbb{R}^n}
$$

![]("GD 0.12GD 0.14GD 0.15_0.1.pdf"){fig-align="center" width=95%}


## Численные эксперименты

$$
f(x) = \frac{\mu}{2} \|x\|_2^2 + \frac1m \sum_{i=1}^m \log (1 + \exp(- y_i \langle a_i, x \rangle)) \to \min_{x \in \mathbb{R}^n}
$$

![](gd_non_linear_1000_300_0_None.pdf)

## Численные эксперименты

$$
f(x) = \frac{\mu}{2} \|x\|_2^2 + \frac1m \sum_{i=1}^m \log (1 + \exp(- y_i \langle a_i, x \rangle)) \to \min_{x \in \mathbb{R}^n}
$$

![](gd_non_linear_1000_300_1_None.pdf)
