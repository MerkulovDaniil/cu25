---
title: "Градиентный спуск. Теоремы сходимости в гладком случае (выпуклые, сильно выпуклые, PL). Верхние и нижние оценки сходимости."
author: Даня Меркулов, Петр Остроухов
institute: Оптимизация для всех! ЦУ
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
    beamer-cu:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader_cu.tex
      header-includes:
        - \newcommand{\cover}{../files/БАК_Методы вып_оптимизации_презентация_7.pdf}
    beamer-cu-maga:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader_cu.tex
      header-includes:
        - \newcommand{\cover}{../files/Методы вып_оптимизации_презентация_7.pdf}
header-includes:
 - \newcommand{\bgimage}{../files/back8.jpeg}
---

# Повторение

## Виды выпуклости

![Примеры выпуклых функций](convex_function_ru.pdf)

## Гладкость

:::: {.columns}

::: {.column width="40%"}

![Иллюстрация Липшицевых парабол, между которыми зажата гладкая функция. Чаще нас интересует мажорирующая из них.](lipschitz_parabola.pdf)

:::

::: {.column width="60%"}

**Определение**: Будем говорить, что функция $f: \mathbb{R}^n \rightarrow \mathbb{R}$ является $L$ -гладкой, если $\forall x, y \in \mathbb{R}^n$ выполнено:
$$
\| \nabla f(y) - \nabla f(x) \| \leqslant L \| y - x \|
$$
\pause
Обратим внимание, что значение константы гладкости (Липшицевости градиента) зависит от выбора нормы. <!-- Так же отметим, что обычно нас интересует минимальное значение константы гладкости. То есть, если функция $L$-гладкая, то она также будет, например, $2L$-гладкой. -->

Если $f: \mathbb{R}^n \rightarrow \mathbb{R}$ - непрерывно дифференцируема и градиент Липшицев с константой $L$, то $\forall x, y \in \mathbb{R}^n$:
$$
\| f(y) - f(x) - \langle \nabla f(x), y - x \rangle \| \leqslant \frac{L}{2}\| y - x \|^2
$$
\pause
Если зафиксируем $x_0 \in \mathbb{R}^n$, то:
$$
\varphi_1(x) = f(x_0) + \langle f(x_0), x - x_0 \rangle - \frac{L}{2}\| x - x_0 \|^2
$$
$$
\varphi_2(x) = f(x_0) + \langle f(x_0), x - x_0 \rangle + \frac{L}{2}\| x - x_0 \|^2
$$
\pause
Это две параболы, и для них верно, что
$$
\varphi_1(x) \leqslant f(x) \leqslant \varphi_2(x)\;\forall x
$$

:::

::::

## Гладкость и сильная выпуклость

![](mu_L_parabola.pdf)

## Гладкость и сильная выпуклость

![](non_smooth_funs.pdf)

# Градиентный спуск

## Направление локального наискорейшего спуска

:::: {.columns}

::: {.column width="40%"}
Рассмотрим линейное приближение дифференцируемой функции $f$ вдоль направления $h$, где $\|h\|_2 = 1$:

. . .

$$
f(x + \alpha h) = f(x) + \alpha \langle \nabla f(x), h \rangle + o(\alpha)
$$

. . .

Хотим, чтобы $h$ было направлением убывания:
$$
f(x + \alpha h) - f(x) < 0
$$
$$
\alpha \langle \nabla f(x), h \rangle + o(\alpha) < 0
$$

. . .

Переходя к пределу при $\alpha \to 0$:
$$
\langle \nabla f(x), h \rangle < 0
$$
:::

. . .

::: {.column width="60%"}
Более того, мы хотим, чтобы разница $f(x) - f(x + \alpha h)$ была максимальна:
$$
h = \arg \max_h \left( - \langle \nabla f(x), h \rangle \right) = \arg \min_h \langle \nabla f(x), h \rangle.
$$

Также из неравенства Коши–Буняковского получаем:
$$
\begin{split}
|\langle \nabla f(x), h \rangle | &\leq \| \nabla f(x) \|_2 \| h \|_2 \\
\langle \nabla f(x), h \rangle &\geq -\| \nabla f(x) \|_2 \| h \|_2 = -\| \nabla f(x) \|_2
\end{split}
$$

. . .

Таким образом, направление антиградиента
$$
h = \arg \min_h \langle \nabla f(x), h \rangle = -\dfrac{\nabla f(x)}{\|\nabla f(x)\|_2}
$$
представляет собой направление **наискорейшего локального** убывания функции $f$.


. . .

Итерация метода имеет вид:
$$
x^{k+1} = x^k - \alpha \, \nabla f(x^k)
$$

:::
::::

## Сходимость алгоритма градиентного спуска

[\faPython Код](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/GD_2d_visualization.ipynb) для построения анимации ниже. Сходимость существенно зависит от выбора шага $\alpha$:

[![](gd_2d.png)](https://fmin.xyz/docs/visualizations/gd_lls.mp4)


## Точный линейный поиск (метод наискорейшего спуска)

:::: {.columns}
::: {.column width="80%"}
$$
\alpha_k = \operatorname*{arg\,min}_{\alpha \in \mathbb{R}^+} f\bigl(x^k - \alpha \, \nabla f(x^k)\bigr)
$$
Подход скорее теоретический, чем практический: он удобен для анализа сходимости, но точный линейный поиск часто затруднён, если вычисление функции занимает слишком много времени или стоит слишком дорого.

Интересное теоретическое свойство этого метода заключается в том, что градиенты на соседних итерациях ортогональны. Условие оптимальности по $\alpha_k$ даёт
$$
\left.\dfrac{d}{d\alpha} \, f\bigl(x^k - \alpha \, \nabla f(x^k)\bigr)\right|_{\alpha = \alpha_k} = 0.
$$

. . .

Условия оптимальности:

. . .

$$
\nabla f(x^{k+1})^\top \nabla f(x^k) = 0
$$

:::
::: {.column width="20%"}

![Наискорейший спуск](GD_vs_Steepest.pdf)

[Открыть в Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Steepest_descent.ipynb)
:::
::::

## Экстра: Дифференциальное уравнение градиентного потока

:::: {.columns}
::: {.column width="78%"}

Рассмотрим дифференциальное уравнение градиентного потока:
$$
\tag{GF}
\frac{dx}{dt} = -\nabla f(x(t)).
$$

. . .

Дискретизируем его на равномерной сетке с шагом $\alpha$:
$$
\frac{x^{k+1} - x^k}{\alpha} = -\nabla f(x^k),
$$

. . .

где $x^k \equiv x(t_k)$ и $\alpha = t_{k+1} - t_k$ — шаг сетки.

Отсюда получаем выражение для $x^{k+1}$:
$$
x^{k+1} = x^k - \alpha \, \nabla f(x^k),
$$
являющееся точной формулой обновления градиентного спуска.

[Открыть в Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/GD_vs_GF.ipynb)
:::

. . .

::: {.column width="22%"}
![Траектория градиентного потока](GD_vs_GF.pdf)
:::
::::



