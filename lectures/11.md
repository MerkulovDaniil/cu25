---
title: "Градиентные методы в условных задачах оптимизации - метод проекции градиента. Метод Франк - Вульфа. Идея метода зеркального спуска"
author: Даня Меркулов
institute: Оптимизация для всех! ЦУ
format: 
   beamer:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
#    beamer-cu:
#       pdf-engine: xelatex
#       aspectratio: 169
#       fontsize: 9pt
#       section-titles: true
#       incremental: true
#       include-in-header: ../files/xeheader_cu.tex
header-includes:
 - \newcommand{\bgimage}{../files/back12.jpeg}
---

# Методы оптимизации в условных задачах 

## Условная оптимизация

:::: {.columns}

::: {.column width="50%"}

### Безусловная оптимизация

$$
\min_{x \in \mathbb{R}^n} f(x)
$$

* Любая точка $x_0 \in \mathbb{R}^n$ является допустимой и может быть решением.

:::

. . .

::: {.column width="50%"}

### Условная оптимизация

$$
\min_{x \in S} f(x)
$$

* Не все точки $x \in \mathbb{R}^n$ являются допустимыми и могут быть решением.
* Решение должно быть внутри множества $S$.
* Пример: 
    $$
    \frac12\|Ax - b\|_2^2 \to \min_{\|x\|_2^2 \leq 1}
    $$

:::

::::

. . .

Градиентный спуск - хороший способ решать безусловные задачи 
$$
\tag{GD}
x_{k+1} = x_k - \alpha_k \nabla f(x_k)
$$
Можно ли настроить ГС для решения условных задач? 

. . .

**Да**. Нужно использовать проекции, чтобы обеспечить допустимость на каждой итерации.

## Пример: White-box Adversarial Attacks 

:::: {.columns}

::: {.column width="55%"}

![[Источник](https://arxiv.org/abs/1811.07018)](adversarial.jpeg)

:::

::: {.column width="45%"}

* Математически, нейронная сеть - это функция $f(w; x)$
* Обычно, вход $x$ задается и оптимизируются веса сети $w$
* Можно также зафиксировать веса $w$ и оптимизировать $x$, агрессивно!
$$ 
\min_{\delta} \text{size}(\delta) \quad \text{s.t.} \quad \text{pred}[f(w;x+\delta)] \neq y
$$
или 
$$
\max_{\delta} l(w; x+\delta, y) \; \text{s.t.} \; \text{size}(\delta) \leq \epsilon, \; 0 \leq x+\delta \leq 1
$$
:::
::::


## Идея метода проекции градиента

![Предположим, что мы начинаем с точки $x_k$.](PGD1.pdf)

## Идея метода проекции градиента{.noframenumbering}

![И идем в направлении $-\nabla f(x_k)$.](PGD2.pdf)

## Идея метода проекции градиента{.noframenumbering}

![Иногда мы можем оказаться вне допустимого множества.](PGD3.pdf)

## Идея метода проекции градиента{.noframenumbering}

![Решим эту маленькую задачу с помощью проекции!](PGD4.pdf)

## Идея метода проекции градиента

$$
x_{k+1} = \text{proj}_S\left(x_k - \alpha_k \nabla f(x_k) \right)  \qquad \Leftrightarrow \qquad \begin{aligned}
y_k &= x_k - \alpha_k \nabla f(x_k) \\
x_{k+1} &= \text{proj}_S\left( y_k\right)
\end{aligned}
$$

![Иллюстрация алгоритма проекции градиента](PGD.pdf)

# Проекция

## Проекция

Расстояние $d$ от точки $\mathbf{y} \in \mathbb{R}^n$ до замкнутого множества $S \subset \mathbb{R}^n$:
$$
d(\mathbf{y}, S, \| \cdot \|) = \inf\{\|x - y\| \mid x \in S \}
$$

. . .

Мы будем фокусироваться на евклидовой проекции (возможны другие варианты) точки $\mathbf{y} \in \mathbb{R}^n$ на множество $S \subseteq \mathbb{R}^n$ - это точка $\text{proj}_S(\mathbf{y}) \in S$: 
$$
\text{proj}_S(\mathbf{y}) =\underset{\mathbf{x} \in S}{\operatorname{argmin}}  \frac12 \|x - y\|_2^2
$$

. . .

* **Достаточные условия существования проекции**. Если $S \subseteq \mathbb{R}^n$ - замкнутое множество, то проекция на множество $S$ существует для любой точки.
* **Достаточные условия единственности проекции**. Если $S \subseteq \mathbb{R}^n$ - замкнутое выпуклое множество, то проекция на множество $S$ единственна для любой точки.
* Если множество открыто, и точка вне этого множества, то ее проекция на это множество может не существовать.
* Если точка в множестве, то ее проекция - это сама точка.

## Критерий проекции (неравенство Бурбаки-Чейни-Гольдштейна)

:::: {.columns}

::: {.column width="65%"}

:::{.callout-theorem}
\small
Пусть $S \subseteq \mathbb{R}^n$ - замкнутое и выпуклое, $\forall x \in S, y \in \mathbb{R}^n$. Тогда
$$
\langle y - \text{proj}_S(y), \mathbf{x} - \text{proj}_S(y)\rangle \leq 0
$$ {#eq-proj1}
$$
\|x - \text{proj}_S(y)\|^2 + \|y - \text{proj}_S(y)\|^2 \leq \|x-y\|^2
$$ {#eq-proj2}
:::

1. $\text{proj}_S(y)$ - минимизатор дифференцируемой выпуклой функции $d(y, S, \| \cdot \|) = \|x - y\|^2$ над $S$. По условию оптимальности первого порядка:
    $$
    \begin{aligned}
    \uncover<+->{\nabla d(\text{proj}_S(y))^T(x - \text{proj}_S(y))&\geq 0 \\ }
    \uncover<+->{2\left(\text{proj}_S(y) - y \right)^T(x - \text{proj}_S(y))&\geq 0 \\ }
    \uncover<+->{\left(y - \text{proj}_S(y) \right)^T(x - \text{proj}_S(y))&\leq 0}
    \end{aligned}
    $$
2. Используем правило косинусов $2x^Ty = \|x\|^2 + \|y\|^2 - \|x-y\|^2$ с $x = x - \text{proj}_S(y)$ и $y = y - \text{proj}_S(y)$. По первому свойству теоремы:
    $$
    \begin{aligned}
    \uncover<+->{ 0 \geq 2x^Ty = \|x - \text{proj}_S(y)\|^2 + \|y + \text{proj}_S(y)\|^2 - \|x-y\|^2 \\ }
    \uncover<+->{ \|x - \text{proj}_S(y)\|^2 + \|y + \text{proj}_S(y)\|^2 \leq \|x-y\|^2 }
    \end{aligned}
    $$
:::

::: {.column width="35%"}
![для любой точки $x \in S$ должен быть тупой или прямой угол](proj_crit.pdf)
:::
::::

## Оператор проекции не является растягивающим

* Функция $f$ называется нерастягивающей, если она $L$-Липшицева с $L \leq 1$ ^[Нерастягивающая становится сжимающей, если $L < 1$.]. То есть, для любых двух точек $x,y \in \text{dom} f$,
    $$
    \|f(x)-f(y)\| \leq L\|x-y\|, \text{ where } L \leq 1.
    $$
    Это означает, что расстояние между отображенными точками может быть меньше, чем расстояние между исходными точками.

* Оператор проекции не является растягивающим:
    $$
    \| \text{proj}(x) - \text{proj}(y) \|_2 \leq \| x - y \|_2.
    $$

* Вариационное свойство означает, что оператор не является растягивающим. То есть,
    $$
    \langle y - \text{proj}(y), x - \text{proj}(y) \rangle \leq 0 \quad \forall x \in S \qquad \Rightarrow \qquad \| \text{proj}(x) - \text{proj}(y) \|_2 \leq \| x - y \|_2.
    $$ 

## Оператор проекции не является растягивающим

Сокращенная запись: пусть $\pi = \text{proj}$ и $\pi(x)$ обозначает $\text{proj}(x)$.

. . .

Начинается с вариационного свойства / неравенства острых углов
$$
\langle y-\pi(y) , x-\pi(y) \rangle \leq 0 \quad \forall x \in S.
$$ {#eq-proj1}

. . .

:::: {.columns}

::: {.column width="50%"}
Заменим $x$ на $\pi(x)$ в @eq-proj1
$$
\langle y-\pi(y), \pi(x)-\pi(y) \rangle \leq 0.
$$ {#eq-proj2}
:::

. . .

::: {.column width="50%"}
Заменим $y$ на $x$ и $x$ на $\pi(y)$ в @eq-proj1

$$
\langle x-\pi(x), \pi(y)-\pi(x) \rangle \leq 0.
$$ {#eq-proj3}
:::

::::

. . .

(@eq-proj2)+(@eq-proj3) отменит $\pi(y) - \pi(x)$, это не хорошо. Поэтому перевернем знак в (@eq-proj3), что  даст нам
$$
\langle \pi(x)-x, \pi(x)-\pi(y) \rangle \leq 0.
$$ {#eq-proj4}

. . .

:::: {.columns}

::: {.column width="60%"}
$$
\begin{split}
\langle y-\pi(y)+\pi(x)-x , \pi(x)-\pi(y) \rangle & \leq 0 \\
\langle y - x, \pi(x) - \pi(y) \rangle & \leq -\langle \pi(x)-\pi(y), \pi(x)-\pi(y) \rangle \\
\langle y - x, \pi(y) - \pi(x) \rangle & \geq \lVert \pi(x) - \pi(y) \rVert^2_2 \\
\lVert (y - x)^\top (\pi(y) - \pi(x)) \rVert_2 & \geq \lVert \pi(x) - \pi(y) \rVert^2_2
\end{split}
$$
:::

. . .

::: {.column width="40%"}
По неравенству КБШ, левая часть ограничена сверху $\lVert y - x \rVert_2 \lVert \pi(y) - \pi(x) \rVert_2$, получаем $\lVert y - x \rVert_2 \lVert \pi(y) - \pi(x) \rVert_2 \geq \lVert \pi(x) - \pi(y) \rVert^2_2$. Отменяет $\lVert \pi(x) - \pi(y) \rVert_2$ завершает доказательство.
:::

::::

## Пример: проекция на шар

Найти $\pi_S (y) = \pi$, если $S = \{x \in \mathbb{R}^n \mid \|x - x_0\| \le R \}$, $y \notin S$ 

. . .

Построим гипотезу из рисунка: $\pi = x_0 + R \cdot \frac{y - x_0}{\|y - x_0\|}$ 

. . .

:::: {.columns}

::: {.column width="60%"}
Проверим неравенство для выпуклого замкнутого множества: $(\pi - y)^T(x - \pi) \ge 0$ 

. . .

$$
\begin{split}
\left( x_0 - y + R \frac{y - x_0}{\|y - x_0\|} \right)^T\left( x - x_0 - R \frac{y - x_0}{\|y - x_0\|} \right) &= \\
\left( \frac{(y - x_0)(R - \|y - x_0\|)}{\|y - x_0\|} \right)^T\left( \frac{(x-x_0)\|y-x_0\|-R(y - x_0)}{\|y - x_0\|} \right) &= \\
\frac{R - \|y - x_0\|}{\|y - x_0\|^2} \left(y - x_0 \right)^T\left( \left(x-x_0\right)\|y-x_0\|-R\left(y - x_0\right) \right) &= \\
\frac{R - \|y - x_0\|}{\|y - x_0\|} \left( \left(y - x_0 \right)^T\left( x-x_0\right)-R\|y - x_0\| \right) &= \\
\left(R - \|y - x_0\| \right) \left( \frac{(y - x_0 )^T( x-x_0)}{\|y - x_0\|}-R \right) & \\
\end{split}
$$
:::

. . .

::: {.column width="40%"}
Первый множитель отрицателен для выбора точки $y$. Второй множитель тоже отрицателен, что следует из неравенства КБШ: 

. . .

$$
\begin{split}
(y - x_0 )^T( x-x_0) &\le \|y - x_0\|\|x-x_0\| \\
\frac{(y - x_0 )^T( x-x_0)}{\|y - x_0\|} - R &\le \frac{\|y - x_0\|\|x-x_0\|}{\|y - x_0\|} - R = \|x - x_0\| - R \le 0
\end{split}
$$

![Шар](proj_ball.pdf){width=60%}

:::
::::

## Пример: проекция на гиперплоскость

Найти $\pi_S (y) = \pi$, если $S = \{x \in \mathbb{R}^n \mid c^T x = b \}$, $y \notin S$. Построим гипотезу из рисунка: $\pi = y + \alpha c$. Коэффициент $\alpha$ выбирается так, чтобы $\pi \in S$: $c^T \pi = b$, поэтому:

. . .

:::: {.columns}

::: {.column width="50%"}
![Гиперплоскость](proj_half.pdf)
:::

. . .

::: {.column width="50%"}
$$
\begin{split}
c^T (y + \alpha c) &= b \\
c^Ty + \alpha c^T c &= b \\
c^Ty &= b - \alpha c^T c \\
\end{split}
$$
Проверим неравенство для выпуклого замкнутого множества: $(\pi - y)^T(x - \pi) \ge 0$ 
$$
\begin{split}
(y + \alpha c - y)^T(x - y - \alpha c) =& \\
\alpha c^T(x - y - \alpha c) =& \\
\alpha (c^Tx) - \alpha (c^T y) - \alpha^2 (c^Tc) =& \\
\alpha b - \alpha (b - \alpha c^T c) - \alpha^2 c^Tc =& \\
\alpha b - \alpha b + \alpha^2 c^T c - \alpha^2 c^Tc =& 0 \ge 0
\end{split}
$$
:::

::::
 
# Метод проекции градиента (PGD)

## Идея

$$
x_{k+1} = \text{proj}_S\left(x_k - \alpha_k \nabla f(x_k) \right)  \qquad \Leftrightarrow \qquad \begin{aligned}
y_k &= x_k - \alpha_k \nabla f(x_k) \\
x_{k+1} &= \text{proj}_S\left( y_k\right)
\end{aligned}
$$

![Иллюстрация алгоритма метода проекции градиента](PGD.pdf)

## Инструменты сходимости \faGem \ \faGem[regular] \faGem[regular] \faGem[regular]

:::{.callout-theorem}
Пусть $f: \mathbb{R}^n \rightarrow \mathbb{R}$ - $L$-гладкая выпуклая функция. Тогда, для любых $x, y \in \mathbb{R}^n$, выполняется следующее неравенство:

$$
\begin{aligned}
f(x) + \langle \nabla f(x), y - x \rangle + \frac{1}{2L} & \|\nabla f(x) - \nabla f(y)\|^2_2 \leq f(y) \text{ or, equivalently, }\\
\|\nabla f(y)-\nabla f (x)\|_2^2 = & \|\nabla f(x)-\nabla f (y)\|_2^2 \leq 2L\left(f(x)-f(y)-\langle\nabla f (y),x -y\rangle \right)
\end{aligned}
$$
:::

## Инструменты сходимости \faGem \ \faGem \ \faGem[regular] \faGem[regular]

**Доказательство**

1. Для доказательства этого рассмотрим другую функцию $\varphi(y) = f(y) - \langle \nabla f(x), y\rangle$. Она очевидно выпукла (как сумма выпуклых функций). И легко проверить, что она $L$-гладкая по определению, так как $\nabla \varphi(y) = \nabla f(y) - \nabla f(x)$ и $\|\nabla \varphi(y_1) - \nabla \varphi(y_2)\| = \|\nabla f(y_1) - \nabla f(y_2)\| \leq L\|y_1 - y_2\|$.
2. Теперь рассмотрим свойство гладкости параболы для функции $\varphi(y)$:
  $$
  \begin{aligned}
  \uncover<+->{ \varphi(y) & \leq  \varphi(x) + \langle \nabla \varphi(x), y-x \rangle + \frac{L}{2}\|y-x\|_2^2 \\ }
  \uncover<+->{ \stackrel{x := y, y := y - \frac1L \nabla\varphi(y)}{ }\;\;\varphi\left(y - \frac1L \nabla\varphi(y)\right) &  \leq \varphi(y) + \left\langle \nabla \varphi(y), - \frac1L \nabla\varphi(y)\right\rangle + \frac{1}{2L}\|\nabla\varphi(y)\|_2^2 \\ }
  \uncover<+->{ \varphi\left(y - \frac1L \nabla\varphi(y)\right) &  \leq \varphi(y) - \frac{1}{2L}\|\nabla\varphi(y)\|_2^2 }
  \end{aligned}
  $$

## Инструменты сходимости \faGem \ \faGem \ \faGem[regular] \faGem[regular]

3. Из условий оптимальности первого порядка для выпуклой функции $\nabla \varphi (y) =\nabla f(y) - \nabla f(x) = 0$. Мы можем заключить, что для любого $x$, минимум функции $\varphi(y)$ находится в точке $y=x$. Следовательно:
  $$
  \varphi(x) \leq \varphi\left(y - \frac1L \nabla\varphi(y)\right) \leq \varphi(y) - \frac{1}{2L}\|\nabla\varphi(y)\|_2^2
  $$
4. Теперь заменим $\varphi(y) = f(y) - \langle \nabla f(x), y\rangle$:
  $$
  \begin{aligned}
  \uncover<+->{ & f(x) - \langle \nabla f(x), x\rangle \leq f(y) - \langle \nabla f(x), y\rangle - \frac{1}{2L}\|\nabla f(y) - \nabla f(x)\|_2^2 \\ }
  \uncover<+->{ & f(x) + \langle \nabla f(x), y - x \rangle + \frac{1}{2L} \|\nabla f(x) - \nabla f(y)\|^2_2 \leq f(y) \\ }
  \uncover<+->{ & \|\nabla f(y) - \nabla f(x)\|^2_2 \leq 2L \left( f(y) - f(x) - \langle \nabla f(x), y - x \rangle \right) \\ }
  \uncover<+->{ {\scriptsize \text{поменять x и y}} \quad & \|\nabla f(x)-\nabla f (y)\|_2^2 \leq 2L\left(f(x)-f(y)-\langle\nabla f (y),x -y\rangle \right)}
  \end{aligned}
  $$

. . .

Лемма доказана. С первого взгляда она не имеет много геометрического смысла, но мы будем использовать ее как удобный инструмент для оценки разницы между градиентами.

## Инструменты сходимости \faGem \ \faGem \ \faGem \ \faGem[regular]

:::{.callout-theorem}
Пусть $f: \mathbb{R}^n \rightarrow \mathbb{R}$ непрерывно дифференцируема на $\mathbb{R}^n$. Тогда функция $f$ является $\mu$-сильно выпуклой тогда и только тогда, когда для любых $x, y \in \mathbb{R}^d$ выполняется следующее:
$$
\begin{aligned}
\text{Strongly convex case } \mu >0 & &\langle \nabla f(x) - \nabla f(y), x - y \rangle &\geq \mu \|x - y\|^2 \\
\text{Convex case } \mu = 0 & &\langle \nabla f(x) - \nabla f(y), x - y \rangle &\geq 0
\end{aligned}
$$
:::

**Доказательство**

1. Мы дадим доказательство только для сильно выпуклого случая, выпуклый случай следует из него с установкой $\mu=0$. Начнем с необходимости. Для сильно выпуклой функции
  $$
  \begin{aligned}
  & f(y) \geq f(x) + \langle \nabla f(x), y-x\rangle + \frac{\mu}{2}\|x-y\|_2^2 \\
  & f(x) \geq f(y) + \langle \nabla f(y), x-y\rangle + \frac{\mu}{2}\|x-y\|_2^2 \\
  {\scriptsize \text{сумма}} \;\; & \langle \nabla f(x) - \nabla f(y), x - y \rangle \geq \mu \|x - y\|^2
  \end{aligned}
  $$

## Инструменты сходимости \faGem \ \faGem \ \faGem \ \faGem

2. Для достаточности мы предполагаем, что $\langle \nabla f(x) - \nabla f(y), x - y \rangle \geq \mu \|x - y\|^2$. Используя теорему Ньютона-Лейбница $f(x) = f(y) + \int_{0}^{1} \langle \nabla f(y + t(x - y)), x - y \rangle dt$:
  $$
  \begin{aligned}
  \uncover<+->{ f(x) - f(y) - \langle \nabla f(y), x - y \rangle &= \int_{0}^{1} \langle \nabla f(y + t(x - y)), x - y \rangle dt - \langle \nabla f(y), x - y \rangle \\ }
  \uncover<+->{ \stackrel{ \langle \nabla f(y), x - y \rangle = \int_{0}^{1}\langle \nabla f(y), x - y \rangle dt}{ }\qquad &= \int_{0}^{1} \langle \nabla f(y + t(x - y)) - \nabla f(y), (x - y) \rangle dt \\ }
  \uncover<+->{ \stackrel{ y + t(x - y) - y = t(x - y)}{ }\qquad&= \int_{0}^{1} t^{-1} \langle \nabla f(y + t(x - y)) - \nabla f(y), t(x - y) \rangle dt \\ }
  \uncover<+->{ & \geq \int_{0}^{1} t^{-1} \mu \| t(x - y) \|^2 dt } \uncover<+->{ = \mu \| x - y \|^2 \int_{0}^{1} t dt} \uncover<+->{= \frac{\mu}{2} \| x - y \|^2_2 }
  \end{aligned}
  $$

  . . .

  Таким образом, мы полчаем выполнение критерия сильной выпуклости
  $$
  \begin{aligned}
  \uncover<+->{ & f(x) \geq f(y) + \langle \nabla f(y), x - y \rangle + \frac{\mu}{2} \| x - y \|^2_2} \uncover<+->{ \text{ или, эквивалентно: }\\ }
  \uncover<+->{ {\scriptsize \text{поменять x и y}} \quad & - \langle \nabla f(x), x - y \rangle \leq - \left(f(x) - f(y) + \frac{\mu}{2} \| x - y \|^2_2 \right) }
  \end{aligned}
  $$

## Скорость сходимости для гладкой и выпуклой функции \faGem \ \faGem[regular] \faGem[regular] \faGem[regular] \faGem[regular]

:::{.callout-theorem}
Пусть $f: \mathbb{R}^n \to \mathbb{R}$ выпуклая и дифференцируемая. Пусть $S \subseteq  \mathbb{R}^n$ замкнутое выпуклое множество, и пусть есть минимизатор $x^*$ функции $f$ над $S$; кроме того, пусть $f$ гладкая над $S$ с параметром $L$. Алгоритм проекции градиента с шагом $\frac1L$ достигает следующей сходимости после итерации $k > 0$:
$$
f(x_k) - f^* \leq \frac{L\|x_0 - x^*\|_2^2}{2k}
$$
:::

. . .

1. Докажем лемму о достаточном убывании, предполагая, что $y_{k} = x_k - \frac1L\nabla f(x_k)$ и правило косинусов $2x^Ty = \|x\|^2 + \|y\|^2 - \|x-y\|^2$:
    $$
    \begin{aligned}
    \uncover<+->{ &\text{Гладкость:} &f(x_{k+1})& \leq f(x_{k}) + \langle \nabla f(x_{k}), x_{k+1}-x_{k} \rangle +\frac{L}{2} \| x_{k+1}-x_{k}\|^2\\ }
    \uncover<+->{ &\text{Метод:} & &= f(x_{k})-L\langle y_{k} - x_k , x_{k+1}-x_{k} \rangle +\frac{L}{2} \| x_{k+1}-x_{k}\|^2\\ }
    \uncover<+->{ &\text{Правило косинусов:} & &= f(x_{k})-\frac{L}{2}\left( \|y_{k} - x_k\|^2 + \|x_{k+1}-x_{k}\|^2 - \|y_{k} - x_{k+1}\|^2\right) +\frac{L}{2} \| x_{k+1}-x_{k}\|^2\\ }
    \uncover<+->{ & & &= f(x_{k})-\frac{1}{2L}\|\nabla f(x_k)\|^2 + \frac{L}{2} \|y_{k} - x_{k+1}\|^2 \\ }
    \end{aligned}
    $$ {#eq-suff_dec}

## Скорость сходимости для гладкой и выпуклой функции \faGem \ \faGem \ \faGem[regular] \faGem[regular] \faGem[regular]

2. Теперь на каждом шаге прогресс не гарантирован. Снова используем формулу косинусов:
    $$
    \begin{aligned}
    \left\langle\frac1L \nabla f(x_k), x_k - x^* \right\rangle &=  \frac12\left(\frac{1}{L^2}\|\nabla f(x_k)\|^2 + \|x_k - x^*\|^2 -  \|x_k - x^* - \frac1L \nabla f(x_k)\|^2 \right) \\
    \langle \nabla f(x_k), x_k - x^* \rangle &=  \frac{L}{2}\left(\frac{1}{L^2}\|\nabla f(x_k)\|^2 + \|x_k - x^*\|^2 -  \|y_k - x^*\|^2 \right) \\
    \end{aligned}
    $$
3. Теперь используем свойство проекции: $\|x - \text{proj}_S(y)\|^2 + \|y - \text{proj}_S(y)\|^2 \leq \|x-y\|^2$ с $x = x^*, y = y_k$:
    $$
    \begin{aligned}
    \|x^* - \text{proj}_S(y_k)\|^2 + \|y_k - \text{proj}_S(y_k)\|^2 \leq \|x^*-y_k\|^2 \\
    \|y_k - x^*\|^2 \geq \|x^* - x_{k+1}\|^2 + \|y_k - x_{k+1}\|^2
    \end{aligned}
    $$
4. Теперь, используя выпуклость и предыдущую часть:
    $$
    \begin{aligned}
    \uncover<+->{ &\text{Выпуклость:} &f(x_k) - f^* &\leq  \langle \nabla f(x_k), x_k - x^* \rangle \\}
    \uncover<+->{ & & &\leq  \frac{L}{2}\left(\frac{1}{L^2}\|\nabla f(x_k)\|^2 + \|x_k - x^*\|^2 -  \|x_{k+1} - x^*\|^2 - \|y_k - x_{k+1}\|^2 \right) \\}
    \uncover<+->{&\text{Просуммируем для } i=0,k-1 &\sum\limits_{i=0}^{k-1} \left[f(x_i) - f^*\right]&\leq\sum\limits_{i=0}^{k-1} \frac{1}{2L}\|\nabla f(x_i)\|^2 + \frac{L}{2}\|x_0 - x^*\|^2  - \frac{L}{2} \sum\limits_{i=0}^{i-1} \|y_i - x_{i+1}\|^2}
    \end{aligned}
    $$

## Скорость сходимости для гладкой и выпуклой функции \faGem \ \faGem \ \faGem \ \faGem[regular] \faGem[regular]

5. Оценим градиенты с помощью неравенства о достаточном убывании [@eq-suff_dec]:
    $$
    \begin{aligned}
    \uncover<+->{\sum\limits_{i=0}^{k-1} \left[f(x_i) - f^*\right]&\leq \sum\limits_{i=0}^{k-1}\left[ f(x_{i}) - f(x_{i+1}) + \frac{L}{2} \|y_{i} - x_{i+1}\|^2 \right] + \frac{L}{2}\|x_0 - x^*\|^2  - \frac{L}{2} \sum\limits_{i=0}^{i-1} \|y_i - x_{i+1}\|^2  \\}
    \uncover<+->{&\leq f(x_0) - f(x_k) + \frac{L}{2} \sum\limits_{i=0}^{i-1} \|y_i - x_{i+1}\|^2 + \frac{L}{2}\|x_0 - x^*\|^2  - \frac{L}{2} \sum\limits_{i=0}^{i-1} \|y_i - x_{i+1}\|^2 \\}
    \uncover<+->{&\leq f(x_0) - f(x_k) + \frac{L}{2}\|x_0 - x^*\|^2 \\}
    \uncover<+->{\sum\limits_{i=0}^{k-1} f(x_i) - k f^* &\leq f(x_0) - f(x_k) + \frac{L}{2}\|x_0 - x^*\|^2\\}
    \uncover<+->{\sum\limits_{i=1}^{k} \left[ f(x_i) - f^*\right] &\leq \frac{L}{2}\|x_0 - x^*\|^2\\}
    \end{aligned}
    $$

## Скорость сходимости для гладкой и выпуклой функции \faGem \ \faGem \ \faGem \ \faGem \ \faGem[regular]

6. Из неравенства о достаточном убывании 
    $$
    f(x_{k+1}) \le f(x_k) - \frac{1}{2L}\|\nabla f(x_k)\|^2 + \frac{L}{2}\|y_k - x_{k+1}\|^2,
    $$

    . . .

    используем факт, что $x_{k+1} = \mathrm{proj}_S(y_k)$.  По определению проекции,
    $$
    \|y_k - x_{k+1}\| \le \|y_k - x_k\|,
    $$

    . . .

    и вспомним, что $y_k = x_k - \tfrac{1}{L}\nabla f(x_k)$ указывает на
    $\|y_k - x_k\| = \tfrac{1}{L}\|\nabla f(x_k)\|$.
    Следовательно,
    $$
    \frac{L}{2}\,\|y_k - x_{k+1}\|^2 \le \frac{L}{2}\,\|y_k - x_k\|^2 = \frac{L}{2}\,\frac{1}{L^2}\,\|\nabla f(x_k)\|^2 = \frac{1}{2L}\,\|\nabla f(x_k)\|^2.
    $$

    . . .

    Заменим обратно в $(*)$:
    $$
    f(x_{k+1}) \le f(x_k) - \frac{1}{2L}\|\nabla f(x_k)\|^2 + \frac{1}{2L}\|\nabla f(x_k)\|^2 = f(x_k).
    $$

    . . .

    Следовательно,
    $$
    f(x_{k+1}) \le f(x_k)\quad\text{для каждого }k,
    $$
    $\{f(x_k)\}$ является монотонно неубывающей последовательностью.


## Скорость сходимости для гладкой и выпуклой функции \faGem \ \faGem \ \faGem \ \faGem \ \faGem

7. Финальная оценка сходимости
    Из шага 5, мы уже установили
    $$
    \sum_{i=0}^{k-1}\bigl[f(x_i) - f^*\bigr] \le \frac{L}{2}\|x_0 - x^*\|_2^2.
    $$

    . . .

    Поскольку $f(x_i)$ убывает в $i$, в частности $f(x_k) \le f(x_i)$ для всех $i \le k$.  Следовательно,
    $$
    k\,\bigl[f(x_k) - f^*\bigr] \le \sum_{i=0}^{k-1}\bigl[f(x_i) - f^*\bigr] \le \frac{L}{2}\|x_0 - x^*\|_2^2,
    $$

    . . .

    которое сразу дает
    $$
    f(x_k) - f^* \le \frac{L\|x_0 - x^*\|_2^2}{2k}.
    $$
    Это завершает доказательство скорости сходимости $\mathcal{O}(\tfrac1k)$ для выпуклой и $L$‐гладкой $f$ с ограничениями на проекцию.

## Скорость сходимости для гладкой сильно выпуклой функции \faGem \ \faGem[regular] \faGem[regular]

:::{.callout-theorem}

Пусть $f: \mathbb{R}^n \to \mathbb{R}$ является $\mu$-сильно выпуклой. Пусть $S \subseteq  \mathbb{R}^n$ замкнутое выпуклое множество, и пусть есть минимизатор $x^*$ функции $f$ над $S$; кроме того, пусть $f$ гладкая над $S$ с параметром $L$. Алгоритм проекции градиента с шагом $\alpha \leq \frac1L$ достигает следующей сходимости после  $k > 0$:

$$
\|x_{k} - x^*\|_2^2 \leq \left(1 - \alpha \mu\right)^k \|x_{0} - x^*\|_2^2
$$
:::

**Доказательство**

1. Сначала докажем свойство стационарной точки: $\text{proj}_S(x^* - \alpha \nabla f(x^*)) = x^*$.
   
   Это следует из критерия проекции и условия оптимальности первого порядка для $x^*$.
   Пусть $y = x^* - \alpha \nabla f(x^*)$. Мы должны показать, что $\langle y - x^*, x - x^* \rangle \le 0$ для всех $x \in S$.
   $$
   \langle (x^* - \alpha \nabla f(x^*)) - x^*, x - x^* \rangle = -\alpha \langle \nabla f(x^*), x - x^* \rangle \le 0
   $$
   Неравенство выполняется, потому что $\alpha > 0$ и $\langle \nabla f(x^*), x-x^* \rangle \geq 0$ является условием оптимальности для $x^*$.



## Скорость сходимости для гладкой сильно выпуклой функции \faGem \ \faGem \ \faGem[regular]

1. Учитывая расстояние до решения и используя свойство стационарной точки:
  $$
  \begin{aligned}
  \uncover<+->{ \|x_{k+1} - x^*\|^2_2 &= \|\text{proj}_S (x_k - \alpha \nabla f (x_k)) - x^*\|^2_2 \\ }
  \uncover<+->{ {\scriptsize \text{Свойство стационарной точки}}  & = \|\text{proj}_S (x_k - \alpha \nabla f (x_k)) - \text{proj}_S (x^* - \alpha \nabla f (x^*)) \|^2_2 \\ }
  \uncover<+->{ {\scriptsize \text{нерастяжимость}}   & \leq \|x_k - \alpha \nabla f (x_k) - (x^* - \alpha \nabla f (x^*)) \|^2_2 \\ }
  \uncover<+->{ & =  \|x_k - x^*\|^2 - 2\alpha \langle \nabla f(x_k) - \nabla f(x^*), x_k - x^* \rangle + \alpha^2 \|\nabla f(x_k) - \nabla f(x^*)\|^2_2 }
  \end{aligned}
  $$

2. Теперь используем гладкость из инструментов сходимости и сильную выпуклость: 
  $$
  \begin{aligned}
  \uncover<+->{ \text{гладкость} \;\; &\|\nabla f(x_k)-\nabla f (x^*)\|_2^2 \leq 2L\left(f(x_k)-f(x^*)-\langle\nabla f (x^*),x_k -x^*\rangle \right) \\ }
  \uncover<+->{ \text{сильная выпуклость} \;\; & - \langle \nabla f(x_k) -  \nabla f(x^*), x_k - x^* \rangle \leq - \left(f(x_k) - f(x^*) + \frac{\mu}{2} \| x_k - x^* \|^2_2 \right) - \langle \nabla f(x^*), x_k - x^* \rangle }
  \end{aligned}
  $$

## Скорость сходимости для гладкой сильно выпуклой функции \faGem \ \faGem \ \faGem

3. Заменим:
  $$
  \begin{aligned}
  \uncover<+->{ \|x_{k+1} - x^*\|^2_2 &\leq \|x_k - x^*\|^2 - 2\alpha \left(f(x_k) - f(x^*) + \frac{\mu}{2} \| x_k - x^* \|^2_2 \right) - 2\alpha \langle \nabla f(x^*), x_k - x^* \rangle + \\ 
  & + \alpha^2 2L\left(f(x_k)-f(x^*)-\langle\nabla f (x^*),x_k -x^*\rangle \right)  \\ }
  \uncover<+->{ &\leq (1 - \alpha \mu)\|x_k - x^*\|^2 + 2\alpha (\alpha L - 1) \left( f(x_k) - f(x^*) - \langle \nabla f(x^*), x_k - x^* \rangle \right)}
  \end{aligned}
  $$

4. В силу выпуклости $f$: $f(x_k) - f(x^*) - \langle \nabla f(x^*), x_k - x^* \rangle \geq 0$. Следовательно, при выборе $\alpha \leq \frac1L$:
  $$
  \|x_{k+1} - x^*\|^2_2 \leq (1 - \alpha \mu)\|x_k - x^*\|^2,
  $$
  что и дает линейную сходимость метода со скоростью не хуже $1 - \frac{\mu}{L}$.



# Метод Франк-Вульфа

---

::::{.columns}

:::{.column width="50%"}
![Маргарет Штраус Франк (1927-2024)](frank.jpg){height=88%}
:::

:::{.column width="50%"}
![Филипп Вульф (1927-2016)](wolfe.jpg){height=88%}
:::

::::

## Идея

![Иллюстрация метода Франк-Вульфа (условный градиент)](FW1.pdf)

## Идея {.noframenumbering}

![Иллюстрация метода Франк-Вульфа (условный градиент)](FW2.pdf)

## Идея {.noframenumbering}

![Иллюстрация метода Франк-Вульфа (условный градиент)](FW3.pdf)

## Идея {.noframenumbering}

![Иллюстрация метода Франк-Вульфа (условный градиент)](FW4.pdf)

## Идея {.noframenumbering}

![Иллюстрация метода Франк-Вульфа (условный градиент)](FW5.pdf)

## Идея {.noframenumbering}

![Иллюстрация метода Франк-Вульфа (условный градиент)](FW6.pdf)

## Идея {.noframenumbering}

![Иллюстрация метода Франк-Вульфа (условный градиент)](FW7.pdf)

## Идея

$$
\begin{split}
y_k &= \text{arg}\min_{x \in S} f^I_{x_k}(x) = \text{arg}\min_{x \in S} \langle\nabla f(x_k), x \rangle \\
x_{k+1} &= \gamma_k x_k + (1-\gamma_k)y_k
\end{split}
$$

![Иллюстрация метода Франк-Вульфа (условный градиент)](FW.pdf)

## Скорость сходимости для гладкой и выпуклой функции \faGem \ \faGem[regular] \faGem[regular]

:::{.callout-theorem}
Пусть $f: \mathbb{R}^n \to \mathbb{R}$ является выпуклой и дифференцируемой. Пусть $S \subseteq \mathbb{R}^n$ замкнутое выпуклое множество, и пусть существует минимизатор $x^*$ функции $f$ над $S$; кроме того, пусть $f$ гладкая над $S$ с параметром $L$. Метод Франк-Вульфа с шагом $\gamma_k = \frac{k-1}{k+1}$ достигает следующей сходимости после $k > 0$ итераций:
$$
f(x_k) - f^* \leq \frac{2LR^2}{k+1}
$$
где $R = \max\limits_{x, y \in S} \|x - y\|$ является диаметром множества $S$.
:::

. . .

1. Благодаря $L$-гладкости функции $f$, имеем:
    $$
    \begin{aligned}
    f\left(x_{k+1}\right) - f\left(x_k\right) &\leq \left\langle \nabla f\left(x_k\right), x_{k+1} - x_k \right\rangle + \frac{L}{2} \left\|x_{k+1} - x_k\right\|^2 \\
    &= (1 - \gamma_k) \left\langle \nabla f\left(x_k\right), y_k - x_k \right\rangle + \frac{L (1 - \gamma_k)^2}{2} \left\|y_k - x_k\right\|^2 
    \end{aligned}
    $$

## Скорость сходимости для гладкой и выпуклой функции \faGem \ \faGem \ \faGem[regular]

2. Благодаря выпуклости функции $f$, для любой точки $x \in S$, включая $x^*$:
    $$
    \langle \nabla f(x_k), x - x_k \rangle \leq f(x) - f(x_k)
    $$
    В частности, для $x = x^*$:
    $$
    \langle \nabla f(x_k), x^* - x_k \rangle \leq f(x^*) - f(x_k)
    $$

3. По определению $y_k$, имеем $\langle \nabla f(x_k), y_k \rangle \leq \langle \nabla f(x_k), x^* \rangle$, следовательно:
    $$
    \langle \nabla f(x_k), y_k - x_k \rangle \leq \langle \nabla f(x_k), x^* - x_k \rangle \leq f(x^*) - f(x_k)
    $$

4. Объединяя неравенства:
    $$
    \begin{aligned}
    f\left(x_{k+1}\right) - f\left(x_k\right) &\leq (1 - \gamma_k) \left\langle \nabla f\left(x_k\right), y_k - x_k \right\rangle + \frac{L (1 - \gamma_k)^2}{2} \left\|y_k - x_k\right\|^2 \\
    &\leq (1 - \gamma_k) \left( f(x^*) - f(x_k) \right) + \frac{L (1 - \gamma_k)^2}{2} R^2
    \end{aligned}
    $$

5. Перегруппируем слагаемые:
    $$
    \begin{aligned}
    f\left(x_{k+1}\right) - f(x^*) &\leq \gamma_k \left( f(x_k) - f(x^*) \right) + (1 - \gamma_k)^2 \frac{L R^2}{2}
    \end{aligned}
    $$

## Скорость сходимости для гладкой и выпуклой функции \faGem \ \faGem \ \faGem

6. Обозначим $\delta_k = \frac{f\left(x_k\right) - f\left(x^*\right)}{L R^2}$, получим:
    $$
    \delta_{k+1} \leq \gamma_k \delta_k + \frac{(1 - \gamma_k)^2}{2} = \frac{k - 1}{k + 1} \delta_k + \frac{2}{(k + 1)^2}
    $$

7. Докажем, что $\delta_k \leq \frac{2}{k+1}$ индукцией.

    * База: $\delta_2 \leq \frac{1}{2} < \frac23$
    * Предположение: $\delta_k \leq \frac{2}{k+1}$
    * Тогда $\delta_{k+1} \leq \frac{k-1}{k+1} \cdot \frac{2}{k+1} + \frac{2}{(k+1)^2} = \frac{2k}{k^2 + 2k + 1} < \frac{2}{k+2}$ \faGraduationCap

    что дает нам желаемый результат:
    $$
    f(x_k) - f^* \leq \frac{2LR^2}{k+1}
    $$

## Нижняя граница для метода Франк-Вульфа ^[[\faFilePdf The Complexity of Large-scale Convex Programming under a Linear Optimization Oracle](https://arxiv.org/abs/1309.5550)] \faGem  \faGem[regular]

:::{.callout-theorem}
Рассмотрим любой алгоритм, который использует только линейный минимизатор (LMO). Пусть диаметр множества $S$ равен $R$. Существует $L$-гладкая сильно выпуклая функция $f : \mathbb{R}^n \to \mathbb{R}$ такая, что этот алгоритм требует по крайней мере
$$
\min \left( \frac{n}{2}, \frac{LR^2}{16 \varepsilon} \right)
$$
итераций (т.е. вызовов LMO) для построения точки $\hat{x} \in S$ с $f(\hat{x}) - \min\limits_{x \in S} f(x) \leq \varepsilon$. Нижняя граница применима как для выпуклых, так и для сильно выпуклых функций.
:::

. . .

::::{.columns}

:::{.column width="50%"}

**Схема доказательства.** Рассмотрим следующую задачу оптимизации:
$$
\begin{aligned}
\min_{x \in S} f(x) &= \min_{x \in S} \frac{1}{2} \|x\|_2^2 \\
S &= \left\{ x \in \mathbb{R}^n \mid x \geq 0,\ \sum_{i=1}^n x_i = 1 \right\}
\end{aligned}
$$  
:::
:::{.column width="50%" .nonincremental}
Обратим внимание, что:

- $f$ является $1$-гладкой;
- диаметр $S$ равен $R = 2$;
- $f$ сильно выпукла.
:::
::::

## Нижняя граница для метода Франк-Вульфа ^[[\faFilePdf The Complexity of Large-scale Convex Programming under a Linear Optimization Oracle](https://arxiv.org/abs/1309.5550)] \faGem \ \faGem

1. Оптимальное решение равно
    $$
    x^* = \frac{1}{n} \mathbf{1} = \frac{1}{n} \sum_{i=1}^n e_i, \quad \text{и} \quad f(x^*) = \frac{1}{2n},
    $$
где $e_i = (0, \dots, 0, \underset{i \text{-ая позиция}}{1}, 0, \dots, 0)^\top$ является $i$-м стандартным базисным вектором.

1. Линейный минимизатор (LMO) над $S$ возвращает вершину $e_i$. После $k$ итераций, метод обнаружит не более $k$ различных базисных векторов $e_{i_1}, \dots, e_{i_k}$. Лучшая выпуклая комбинация, которую можно сформировать, равна
    $$
    \hat{x} = \frac{1}{k} \sum_{j=1}^k e_{i_j}.
    $$

## Нижняя граница для метода Франк-Вульфа ^[[\faFilePdf The Complexity of Large-scale Convex Programming under a Linear Optimization Oracle](https://arxiv.org/abs/1309.5550)] \faGem \ \faGem

1. Оценивая функцию в $\hat{x}$, получаем:
    $$
    f(\hat{x}) - f(x^*) \geq \frac{1}{2} \left( \frac{1}{\min\{k, n\}} - \frac{1}{n} \right).
    $$

1. Чтобы гарантировать, что $f(\hat{x}) - f(x^*) \leq \varepsilon$, необходимо, чтобы (полное доказательство приведено в статье):
    $$
    k \geq \min\left\{ \frac{n}{2}, \frac{1}{4\varepsilon} \right\} = \min\left\{ \frac{n}{2}, \frac{L R^2}{16\varepsilon} \right\}.
    $$

## Summary по методу Франк-Вульфа

* Метод не требует проекций, в некоторых специальных случаях позволяет вычислять итерации в замкнутой форме
* Глобальная скорость сходимости равна $O\left(\frac{1}{k}\right)$ для гладких и выпуклых функций. Сильная выпуклость не улучшает скорость -  это нижняя граница для LMO
* В сравнении с методом проекции градиента, скорость хуже, но итерация может быть дешевле и более разреженной
* Недавно было показано, что для сильно выпуклых множеств, скорость может быть улучшена до $O\left(\frac{1}{k^2}\right)$ ([\faFilePdf \ paper](https://arxiv.org/abs/1406.1305))
* Если мы позволяем steps away, сходимость становится линейной ([\faFilePdf \ paper](https://arxiv.org/abs/1511.05932)) в сильно выпуклом случае
* Недавняя работа показала расширение на негладкий случай ([\faFilePdf \ paper](https://arxiv.org/abs/2010.01848)) с скоростью сходимости $O\left(\frac{1}{\sqrt{k}}\right)$
    

# Численные эксперименты

## 2d пример. Метод Франк-Вульфа

\centering
\includegraphics<1>[width=0.7\textwidth]{fw_2d_0.pdf}
\includegraphics<2>[width=0.7\textwidth]{fw_2d_1.pdf}
\includegraphics<3>[width=0.7\textwidth]{fw_2d_2.pdf}
\includegraphics<4>[width=0.7\textwidth]{fw_2d_3.pdf}
\includegraphics<5>[width=0.7\textwidth]{fw_2d_4.pdf}
\includegraphics<6>[width=0.7\textwidth]{fw_2d_5.pdf}
\includegraphics<7>[width=0.7\textwidth]{fw_2d_6.pdf}
\includegraphics<8>[width=0.7\textwidth]{fw_2d_7.pdf}

## 2d пример. Метод проекции градиента

\centering
\includegraphics<1>[width=0.7\textwidth]{pgd_2d_0.pdf}
\includegraphics<2>[width=0.7\textwidth]{pgd_2d_1.pdf}
\includegraphics<3>[width=0.7\textwidth]{pgd_2d_2.pdf}

## Квадратичная функция. Ограничения на $x$

::::{.columns}

:::{.column width="40%"}

$$
\min_{\substack{x \in \mathbb{R}^n \\ -\mathbf{1} \preceq x \preceq \mathbf{1}}} \frac{1}{2} x^\top A x - b^\top x,
$$

$$
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [\mu; L].
$$

Проекция простая:
$$
\pi_S(x) = \text{clip}(x, -\mathbf{1}, \mathbf{1}).
$$
или
$$
\pi_S(x) = \max\left(-\mathbf{1}, \min(\mathbf{1}, x)\right).
$$

Линейный минимизатор (LMO) равен $y = \operatorname*{argmin}\limits_{z \in S} \langle g, z \rangle$.

Поскольку множество допустимых значений разделяется по координатам, решение вычисляется покоординатно как  
$$
y_i = \begin{cases}
-1, & \text{if } g_i > 0, \\
1,  & \text{if } g_i \le 0.
\end{cases}
$$  
:::
:::{.column width="60%"}

![](conditional_quadratic_80_0_10.pdf)

:::
::::

## Квадратичная функция. Ограничения на $x$

::::{.columns}

:::{.column width="40%"}

$$
\min_{\substack{x \in \mathbb{R}^n \\ -\mathbf{1} \preceq x \preceq \mathbf{1}}} \frac{1}{2} x^\top A x - b^\top x,
$$

$$
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [\mu; L].
$$

Проекция простая:
$$
\pi_S(x) = \text{clip}(x, -\mathbf{1}, \mathbf{1}).
$$
или
$$
\pi_S(x) = \max\left(-\mathbf{1}, \min(\mathbf{1}, x)\right).
$$

Линейный минимизатор (LMO) равен $y = \operatorname*{argmin}\limits_{z \in S} \langle g, z \rangle$.

Поскольку множество допустимых значений разделяется по координатам, решение вычисляется покоординатно как  
$$
y_i = \begin{cases}
-1, & \text{if } g_i > 0, \\
1,  & \text{if } g_i \le 0.
\end{cases}
$$  
:::
:::{.column width="60%"}

![](conditional_quadratic_80_1_10.pdf)

:::
::::

## Квадратичная функция. Ограничения на cимплекс (Задача с диагональной матрицей)

::::{.columns}
:::{.column width="45%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ x \ge 0, \mathbf{1}^T x = 1}} \frac{1}{2} x^T A x, \\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [0; 100].
\end{aligned}
$$

|Метод|Обновление, мс|LMO/проекция, мс|
|:------:|:---------------:|:---------------:|
| PGD    | 0.0069 | 0.0167 |
| FW     | 0.0070 | 0.0066 |

Проекция на единичный симплекс $\pi_S(x)$ может быть выполнена за $\mathcal{O}(n \log n)$ или ожидаемо за $\mathcal{O}(n)$ времени. ^[[\faFilePdf \ проекции на $\ell_1$-шар для обучения в пространствах высокой размерности](https://stanford.edu/~jduchi/projects/DuchiShSiCh08.pdf)]

Линейный минимизатор (LMO) равен $y = \operatorname*{argmin}\limits_{z \in S} \langle g, z \rangle$. Решение соответствует вершине симплекса:
$$
y = e_j \quad \text{where} \quad j = \operatorname*{argmin}_i g_i.
$$

:::
:::{.column width="55%"}
![](fw_simplex_200_mu_0_L_100.0_diag.pdf)

:::
::::

## Квадратичная функция. Ограничения на cимплекс

::::{.columns}
:::{.column width="45%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ x \ge 0, \mathbf{1}^T x = 1}} \frac{1}{2} x^T A x, \\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [0; 100].
\end{aligned}
$$

| Метод | Обновление, мс | LMO/проекция, мс |
|:------:|:---------------:|:---------------:|
| PGD    | 0.0069 | 0.0420 |
| FW     | 0.0069 | 0.0066 |

:::
:::{.column width="55%"}
![](fw_simplex_200_mu_0_L_100.0_rotated.pdf)

:::
::::

## Квадратичная функция. Ограничения на cимплекс

::::{.columns}
:::{.column width="45%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ x \ge 0, \mathbf{1}^T x = 1}} \frac{1}{2} x^T A x, \\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [0; 100].
\end{aligned}
$$

| Метод | Обновление, мс | LMO/проекция, мс |
|:------:|:---------------:|:---------------:|
| PGD    | 0.0068 | 0.0761 |
| FW     | 0.0069 | 0.0070 |

:::
:::{.column width="55%"}
![](fw_simplex_300_mu_0_L_100.0_rotated.pdf)

:::
::::

## Квадратичная функция. Ограничения на cимплекс

::::{.columns}
:::{.column width="45%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ x \ge 0, \mathbf{1}^T x = 1}} \frac{1}{2} x^T A x, \\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [1; 100].
\end{aligned}
$$

| Метод | Обновление, мс | LMO/проекция, мс |
|:------:|:---------------:|:---------------:|
| PGD    | 0.0068 | 0.0752 |
| FW     | 0.0067 | 0.0068 |

:::
:::{.column width="55%"}
![](fw_simplex_300_mu_1_L_100.0_rotated.pdf)

:::
::::

## Метод проекции градиента vs метод Франк-Вульфа

Основное отличие между методами заключается в том, что метод проекции градиента требует проекцию, в то время как метод Франк-Вульфа требует только линейный минимизатор (LMO).

В недавней [книге](https://arxiv.org/pdf/2211.14103) авторы представили следующую таблицу сравнения сложности линейных минимизаций и проекций на некоторые выпуклые множества с точностью до аддитивной ошибки $\epsilon$ в евклидовой норме. 

| **Множество**   | **Линейный минимизатор**  | **Проекция**    |
|---------------------|-----------------|---------|
| $n$-мерный $\ell_p$-шар, $p \neq 1,2,\infty$ | $\mathcal{O}(n)$  | $\tilde{\mathcal{O}}\!\bigl(\tfrac{n}{\epsilon^2}\bigr)$|
| Ядро нормы матрицы $n\times m$ | $\mathcal{O}\!\Bigl(\nu\,\ln(m + n)\,\tfrac{\sqrt{\sigma_1}}{\sqrt{\epsilon}}\Bigr)$    | $\mathcal{O}\!\bigl(m\,n\,\min\{m,n\}\bigr)$   |
| Потоковый многогранник на графе с $m$ вершинами и $n$ ребрами (ограничение на пропускную способность ребер) | $\mathcal{O}\!\Bigl((n \log m)\bigl(n + m\,\log m\bigr)\Bigr)$ | $\tilde{\mathcal{O}}\!\bigl(\tfrac{n}{\epsilon^2}\bigr)\ \text{or}\ \mathcal{O}(n^4\,\log n)$    |
| Многогранник Бirkhoff ($n \times n$ дважды стохастических матриц)   | $\mathcal{O}(n^3)$| $\tilde{\mathcal{O}}\!\bigl(\tfrac{n^2}{\epsilon^2}\bigr)$   |  
Когда $\epsilon$ отсутствует, нет аддитивной ошибки. $\tilde{\mathcal{O}}$ скрывает полилогарифмические факторы в размерностях и полиномиальные факторы в константах, связанных с расстоянием до оптимума. Для ядерной нормы шара, $\nu$ обозначает количество ненулевых элементов, а $\sigma_1$ обозначает наибольшее сингулярное значение проекции матрицы.