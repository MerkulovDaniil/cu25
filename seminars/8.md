---
title: Ускорения градиентного спуска
author: Семинар
institute: Оптимизация для всех! ЦУ
format:
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: false
        incremental: false
        include-in-header: ../files/xeheader.tex  # 
---

# Воспоминания с лекции

## Скорости сходимости градиентного спуска

$$
\min_{x \in \mathbb{R}^n} f(x) \qquad \qquad x_{k+1} = x_k - \alpha_k \nabla f(x_k) \qquad \kappa =\dfrac{L}{\mu}
$$

| | выпуклая и гладкая | выпуклая и сильно выпуклая (или PL) |
|:-----:|:-----:|:--------:|
|Верхняя оценка | $f(x_k) - f^* \approx  \mathcal{O} \left( \dfrac{1}{k} \right)$ | $\|x_k - x^*\|^2 \approx \mathcal{O} \left( \left(\dfrac{\kappa - 1}{\kappa + 1}\right)^k \right)$ |
|Нижняя оценка | $f(x_k) - f^* \approx  \Omega \left( \dfrac{1}{k^2} \right)$ | $\|x_k - x^*\|^2 \approx \Omega \left( \left(\dfrac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^k \right)$ |



## Три схемы обновления

* **Градиентный спуск**
  $$\boldsymbol{x}_k - \alpha_k\nabla f(\boldsymbol{x}_k)$$
  Перемещаем точку $\boldsymbol{x}_k$ в направлении $-\nabla f(\boldsymbol{x}_k)$ на $\alpha_k\Vert\nabla f(\boldsymbol{x}_k)\Vert$ единиц.

. . . 

* **Метод тяжелого шарика**
  $$
  \boldsymbol{x}_k - \alpha_k\nabla f(\boldsymbol{x}_k) + \color{blue}{\beta_k(\boldsymbol{x}_k - \boldsymbol{x}_{k-1})}
  $$
  Выполняем GD, $\color{blue}{\text{перемещаем обновленный }\boldsymbol{x}\text{ в направлении предыдущего шага на }\beta_k\Vert \boldsymbol{x}_k - \boldsymbol{x}_{k-1}\Vert\text{ единиц}}$.

. . . 

* **Ускорение Нестерова**
  $$
  \color{red}{\boldsymbol{x}_k - \alpha_k\nabla f(}\color{blue}{\boldsymbol{x}_k+\beta_k(\boldsymbol{x}_k - \boldsymbol{x}_{k-1})}\color{red}{)}\color{black}{ + }\color{orange}{\beta_k(\boldsymbol{x}_k - \boldsymbol{x}_{k-1})}
  $$
  $\color{blue}{\text{Перемещаем не обновленный }\boldsymbol{x}\text{ в направлении предыдущего шага на }\beta_k\Vert \boldsymbol{x}_k - \boldsymbol{x}_{k-1}\Vert \text{ единиц}}$, $\color{red}\text{выполняем GD на }\color{blue}{\text{сдвинутом }\boldsymbol{x}}\color{black}{\text{, затем }}\color{orange}{\text{перемещаем обновленный }\boldsymbol{x}}\text{ в направлении предыдущего шага на}$ $\color{orange}{\beta_k\Vert \boldsymbol{x}_k-\boldsymbol{x}_{k-1}\Vert}$.

## Черный ящик

Итерация градиентного спуска:
$$
\begin{aligned}
x^{k+1} &= x^k - \alpha^k \nabla f(x^k)\\
&= x^{k-1} - \alpha^{k-1} \nabla f(x^{k-1}) - \alpha^k \nabla f(x^k) \\
& \;\;\vdots \\
&= x^0 - \sum\limits_{i=0}^k \alpha^{k-i} \nabla f(x^{k-i})
\end{aligned}
$$

. . .

Рассмотрим семейство первого порядка, где
$$
\begin{aligned}
x^{k+1} &\in x^0 + \text{span} \left\{\nabla f(x^{0}), \nabla f(x^{1}), \ldots, \nabla f(x^{k})\right\} \; & f \text{ - smooth} \\
x^{k+1} &\in x^0 + \text{span} \left\{g_{0}, g_{1}, \ldots, g_{k}\right\} \text{, where }
g_{i} \in \partial f(x^{i}) \; & f \text{ - non-smooth}
\end{aligned}
$$ {#eq-fom}

. . .

Чтобы построить нижнюю оценку, нам нужно найти функцию $f$ из соответствующего класса, такую что любой [метод из семейства @eq-fom] будет работать не быстрее нижней оценки.


## Гладкий случай

:::{.callout-theorem}
Существует функция $f$, которая является $L$-гладкой и выпуклой, так что любой [метод @eq-fom] для любого $k: 1 \leq k \leq \frac{n-1}{2}$ удовлетворяет:
$$
f(x^k) - f^* \geq \frac{3L \|x^0 - x^*\|_2^2}{32(k+1)^2}
$$
:::


* Не важно, какой метод градиентного спуска вы используете, всегда существует функция $f$, при применении на ней вашего метода градиентного спуска, скорость сходимости нижняя оценка $\mathcal{O}\left(\frac{1}{k^2}\right)$.
* Ключом к доказательству является явное построение специальной функции $f$.
* Обратите внимание, что эта оценка $\mathcal{O}\left(\frac{1}{k^2}\right)$ не совпадает со скоростью сходимости градиентного спуска $\mathcal{O}\left(\frac{1}{k}\right)$. Два возможных варианта:
    a. Нижняя оценка не является точной.
    b. \textbf<7>{Градиентный метод не является оптимальным для этой задачи.}




## Метод тяжелого шарика для квадратичной задачи

:::: {.columns}

::: {.column width="20%"}
:::{.callout-question}
Какая стратегия шага используется для $\color{red}{\text{GD}}$?
:::
:::

::: {.column width="80%"}
[![$\color{red}{\text{GD}}$ vs. $\color{blue}{\text{HBM with fixed }\beta}.$](HBM_exact.pdf){width="90%"}](https://angms.science/doc/teaching/GDLS.pdf)
:::

::::
**Наблюдение:** для хорошей $f$ (со сферическими уровнями), GD уже достаточно хорош, и HBM добавляет небольшой эффект. Однако, для плохой $f$ (с эллиптическими уровнями), HBM лучше в некоторых случаях.



## Метод тяжелого шарика для квадратичной задачи

[![$\color{red}{\text{GD with }\alpha=\dfrac{1}{L}}$ vs. $\color{blue}{\text{HBM with fixed }\beta}.$](HBM_L.pdf){width="70%"}](https://angms.science/doc/teaching/GDLS.pdf)

**Наблюдение:** то же самое. Если хорошая $f$ (с сферическими уровнями), GD уже достаточно хорош. Если плохая $f$ (с эллиптическими уровнями), HBM лучше в некоторых случаях.

# NAG для DL
## NAG как метод импульса

* Начнем с установки $k=0, a_0 = 1, \boldsymbol{x}_{-1}=\boldsymbol{y}_0, \boldsymbol{y}_0$ в произвольном параметре, итерации
  $$
  \text{Обновление градиента } \boldsymbol{x}_k = \boldsymbol{y}_k - \alpha_k\nabla f(\boldsymbol{y}_k)
  $${#eq-grad-update}
  $$
  \text{Вес экстраполяции } a_{k+1} = \dfrac{1+\sqrt{1+4a^2_k}}{2}
  $${#eq-extrap-weight}
  $$
  \text{Экстраполяция } \boldsymbol{y}_{k+1} = \boldsymbol{x}_k + \dfrac{a_k - 1}{a_{k+1}}(\boldsymbol{x}_k-\boldsymbol{x}_{k-1})
  $${#eq-extrap}
  Обратите внимание, что здесь используется фиксированный шаг: $\alpha_k = \dfrac{1}{L} \; \forall k$.
* **Теорема.** Если $f$ является $L$-гладкой и выпуклой, последовательность $\{f(\boldsymbol{x}_k)\}_k$, генерируемая NAG, сходится к оптимальному значению $f^*$ с скоростью $\mathcal{O}(\dfrac{1}{k^2})$ как
  $$
  f(\boldsymbol{x}_k)-f^*\leq\dfrac{4L\Vert \boldsymbol{x}_k-\boldsymbol{x}^*\Vert^2}{(k+2)^2}
  $$
* Вышеуказанное представление трудно понять, поэтому мы перепишем эти уравнения в более интуитивном виде. 



## NAG как метод импульса

Если мы определим
$$
\boldsymbol{v}_k \equiv \boldsymbol{x}_k - \boldsymbol{x}_{k-1}
$${#eq-v}
$$
\beta_k \equiv \dfrac{a_k-1}{a_{k+1}}
$${#eq-mu}
то комбинация @eq-extrap и @eq-mu означает:
$$
\boldsymbol{y}_k = \boldsymbol{x}_{k-1} + \beta_{k-1}\boldsymbol{v}_{k-1}
$$
которое может быть использовано для переписания @eq-grad-update следующим образом, используя $\alpha_k = \alpha_{k-1}$:
$$
\boldsymbol{x}_k = \boldsymbol{x}_{k-1} + \beta_{k-1}\boldsymbol{v}_{k-1}-\alpha_{k-1}\nabla f(\boldsymbol{x}_{k-1} + \beta_{k-1}\boldsymbol{v}_{k-1})
$${#eq-one}
$$
\boldsymbol{v}_k = \beta_{k-1}\boldsymbol{v}_{k-1}-\alpha_{k-1}\nabla f(\boldsymbol{x}_{k-1} + \beta_{k-1}\boldsymbol{v}_{k-1})
$${#eq-two}
где @eq-two является следствием @eq-v. Альтернативно:
$$
\color{blue}{\boldsymbol{v}_{k+1} = \beta_{k}\boldsymbol{v}_{k}-\alpha_{k}\nabla f(\boldsymbol{x}_{k}}\color{red}{+ \beta_{k}\boldsymbol{v}_{k}}\color{blue}{)}
$$
$$
\color{blue}{\boldsymbol{x}_{k+1} = \boldsymbol{x}_k + \boldsymbol{v}_{k+1}}
$$
где $\alpha_k > 0$ является **шагом обучения**, $\beta_k$ является **коэффициентом импульса**. Сравните $\color{blue}{\text{HBM}}$ с $\color{red}{\text{NAG}}$.




## NAG для DL ^[[Ссылка](https://www.cs.toronto.edu/~gdahl/papers/momentumNesterovDeepLearning.pdf)]

![Таблица сообщает о квадратичных ошибках на задачах для каждой комбинации $\beta_{max}$ и типа импульса (NAG=N, HB=M). Когда $\beta_{max}$ равен $0$, выбор между NAG и HB не имеет значения, поэтому ошибки обучения представлены в одном столбце. Для каждого выбора $\beta_{max}$ используется наиболее эффективный шаг обучения. Столбец $\text{SGD}_{C}$ содержит результаты Chapelle & Erhan (2011), которые использовали 1,7 млн. шагов SGD и сети tanh.](../files/NAG_conv.png){width="80%"}

# Скорости сходимости

## Сходимость метода тяжелого шарика ^[[Сходимость метода тяжелого шарика для выпуклой оптимизации, Euhanna Ghadimi et.al.](https://arxiv.org/abs/1412.7457)]

:::{.callout-theorem}
Предположим, что $f$ является гладкой и выпуклой и что

$$
\beta\in[0,1),\quad \alpha\in\biggl(0,\dfrac{2(1-\beta)}{L}\biggr).
$$

Тогда, последовательность $\{x_k\}$, генерируемая итерацией тяжелого шарика, удовлетворяет

$$
f(\overline{x}_T)-f^{\star} \leq  \left\{
\begin{array}[l]{ll}
\frac{\Vert x_{0}-x^\star\Vert^2}{2(T+1)}\biggl(\frac{L\beta}{1-\beta}+\frac{1-\beta}{\alpha}\biggr),\;\;\textup{if}\;\;
\alpha\in\bigl(0,\dfrac{1-\beta}{L}\bigr],\\
\frac{\Vert x_{0}-x^\star\Vert^2}{2(T+1)(2(1-\beta)-\alpha L)}\biggl({L\beta}+\frac{(1-\beta)^2}{\alpha}\biggr),\;\;\textup{if}\;\;
\alpha\in\bigl[\dfrac{1-\beta}{L},\dfrac{2(1-\beta)}{L}\bigr),
\end{array}
\right.
$$

где $\overline{x}_T$ является средним по Чезаро итераций, т.е. 

$$
\overline{x}_T = \frac{1}{T+1}\sum_{k=0}^T x_k.
$$
:::


## Сходимость метода тяжелого шарика ^[[Сходимость метода тяжелого шарика для выпуклой оптимизации, Euhanna Ghadimi et.al.](https://arxiv.org/abs/1412.7457)]

:::{.callout-theorem}
Предположим, что $f$ является гладкой и сильно выпуклой и что

$$
\alpha\in(0,\dfrac{2}{L}),\quad 0\leq  \beta<\dfrac{1}{2}\biggl( \dfrac{\mu \alpha}{2}+\sqrt{\dfrac{\mu^2\alpha^2}{4}+4(1-\frac{\alpha L}{2})} \biggr) .
$$

Тогда, последовательность $\{x_k\}$, генерируемая итерацией тяжелого шарика, сходится линейно к уникальному оптимальному значению $x^\star$. В частности,

$$
f(x_{k})-f^\star \leq q^k (f(x_0)-f^\star),
$$

где $q\in[0,1)$.
:::

## Сходимость NAG

:::{.callout-theorem}
Предположим, что $f : \mathbb{R}^n \rightarrow \mathbb{R}$ является выпуклой и $L$-гладкой. Алгоритм Nesterov Accelerated Gradient Descent (NAG) предназначен для решения задачи минимизации, начиная с начальной точки $x_0 = y_0 \in \mathbb{R}^n$ и $\lambda_0 = 0$. Алгоритм выполняет следующие шаги:
$$
\begin{aligned}
&\textbf{Обновление градиента: } &y_{k+1} &= x_k - \frac{1}{L} \nabla f(x_k) \\
&\textbf{Экстраполяция: } &x_{k+1} &= (1 - \gamma_k)y_{k+1} + \gamma_k y_k \\
&\textbf{Вес экстраполяции: } &\lambda_{k+1} &= \frac{1 + \sqrt{1 + 4\lambda_k^2}}{2} \\
&\textbf{Вес экстраполяции: } &\gamma_k &= \frac{1 - \lambda_k}{\lambda_{k+1}}
\end{aligned}
$$
Последовательности $\{f(y_k)\}_{k\in\mathbb{N}}$, генерируемые алгоритмом, сходятся к оптимальному значению $f^*$ с скоростью $\mathcal{O}\left(\frac{1}{k^2}\right)$, в частности:
$$
f(y_k) - f^* \leq \frac{2L \|x_0 - x^*\|^2}{k^2}
$$
:::

## Сходимость NAG

:::{.callout-theorem}
Предположим, что $f : \mathbb{R}^n \rightarrow \mathbb{R}$ является $\mu$-сильно выпуклой и $L$-гладкой. Алгоритм Нестерова (NAG) предназначен для решения задачи минимизации, начиная с начальной точки $x_0 = y_0 \in \mathbb{R}^n$ и $\lambda_0 = 0$. Алгоритм выполняет следующие шаги:
$$
\begin{aligned}
&\textbf{Обновление градиента: } &y_{k+1} &= x_k - \frac{1}{L} \nabla f(x_k) \\
&\textbf{Экстраполяция: } &x_{k+1} &= (1 + \gamma_k)y_{k+1} - \gamma_k y_k \\
&\textbf{Вес экстраполяции: } &\gamma_k &= \frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}
\end{aligned}
$$
Последовательности $\{f(y_k)\}_{k\in\mathbb{N}}$, генерируемые алгоритмом, сходятся к оптимальному значению $f^*$ линейно:
$$
f(y_k) - f^* \leq \frac{\mu + L}{2}\|x_0 - x^*\|^2_2 \exp \left(-\frac{k}{\sqrt{\kappa}}\right)
$$
:::

# Практика

## Хоббиты

Давайте напишем код! [\faPython  Colab](https://colab.research.google.com/github/MerkulovDaniil/hse25/blob/main/notebooks/s9_hobbit_village.ipynb)

## Логистическая регрессия

Давайте напишем код! [\faPython  Colab](https://colab.research.google.com/github/MerkulovDaniil/hse25/blob/main/notebooks/s9_agd_problems.ipynb) 

## Ссылки и примеры Python

* Изображения для HBM взяты из [презентации](https://angms.science/doc/teaching/GDLS.pdf). Посетите [сайт](https://angms.science) для большего количества туториалов.

* Почему импульс действительно работает. [Ссылка](https://distill.pub/2017/momentum/).

* Запустите код в [\faPython Colab](https://drive.google.com/file/d/1Qtnazye0_wz47Q0tRyaV0w0_EhCMouME/view?usp=sharing). Код взят из [\faGithub](https://github.com/amkatrutsa/optimization_course/blob/master/Spring2022/hb_acc_grad.ipynb).

* Важность инициализации и импульса в глубоком обучении. [Ссылка](https://www.cs.toronto.edu/~gdahl/papers/momentumNesterovDeepLearning.pdf).

# Дополнительно

## NAG для квадратичной задачи

Рассмотрим следующую квадратичную задачу оптимизации:
$$
\label{problem}
\min\limits_{x \in \mathbb{R}^d} q(x) =  \min\limits_{x \in \mathbb{R}^d} \dfrac{1}{2} x^\top  A x - b^\top  x, \text{ where }A \in \mathbb{S}^d_{++}.
$$
Каждая симметричная матрица $A$ имеет разложение на собственные значения
$$
A=Q\text{diag}\,(\lambda_1, \dots, \lambda_n)\,Q^T = Q\Lambda Q^T, \quad Q=[q_1, \dots, q_n].
$$
и, как по соглашению, мы будем считать, что $\lambda_i$'s отсортированы, от наименьшего $\lambda_1$ до наибольшего $\lambda_n$. Очевидно, что $\lambda_i$ соответствует **кривизне** вдоль соответствующих направлений собственных векторов.

Мы можем перепараметризовать $q(x)$ с помощью матричного преобразования $Q$ и оптимизировать $y=Qx$ с помощью целевой функции
$$
p(y) \equiv q(x) = q(Q^\top y) = y^\top Q (Q^\top \Lambda Q) Q^\top y/2 - b^\top Q^\top y = y^\top\Lambda y/2 - c^\top y,
$$
где $c=Qb$. 

Мы можем еще раз переписать $p$ как 
$$
p(y) = \sum_{i=1}^n[p]_i([y]_i),
$$ 
где $[p]_i(t) = \lambda_i t^2/2-[c]_i t$.


## NAG для квадратичной задачи

:::{.callout-tip title="Теорема 2.1 из [[1]](https://www.cs.toronto.edu/~gdahl/papers/momentumNesterovDeepLearning.pdf)."}
Пусть $p(y) = \sum_{i=1}^n[p]_i([y]_i)$ такой, что $[p]_i(t) = \lambda_i t^2/2-[c]_i t$. Пусть $\alpha$ будет произвольным и фиксированным. Обозначим через $\text{HBM}_x(\beta, p, y,v)$ и $\text{HBM}_v(\beta,p,y,v)$ вектор параметров и вектор скорости соответственно, полученные применением одного шага HBM (т.е. уравнения 1 и затем уравнения 2) к функции $p$ в точке $y$, со скоростью $v$, коэффициентом импульса $\beta$ и шагом обучения $\alpha$. Определим $\text{NAG}_x$ и $\text{NAG}_v$ аналогично. Тогда для $z \in  \{x, v\}$:
$$
\text{HBM}_z (\beta,p,y,v) = 
\left[\begin{gathered}
      \text{HBM}_z (\beta,[p]_1,[y]_1, [v]_1)\\
      \vdots\\
      \text{HBM}_z (\beta,[p]_n,[y]_n, [v]_n)\\
    \end{gathered}
\right]
$$
$$
\text{NAG}_z (\beta,p,y,v) = 
\left[\begin{gathered}
      \text{HBM}_z (\beta(1-\alpha\lambda_1),[p]_1,[y]_1, [v]_1)\\
      \vdots\\
      \text{HBM}_z (\beta(1-\alpha\lambda_n),[p]_n,[y]_n, [v]_n)\\
    \end{gathered}
\right]
$$
:::



## NAG для квадратичной задачи. Доказательство (1/2)
**Доказательство:**

Легко показать, что если
$$x_{i+1} = \text{HBM}_x (\beta_i,[q]_i,[x]_i, [v]_i)$$
$$v_{i+1} = \text{HBM}_v (\beta_i,[q]_i,[x]_i, [v]_i)$$
то для $y_i=Qx_i, w_i=Qv_i$
$$y_{i+1} = \text{HBM}_x (\beta_i,[p]_i,[y]_i, [w]_i)$$
$$w_{i+1} = \text{HBM}_v (\beta_i,[p]_i,[y]_i, [w]_i)$$.
Тогда, рассмотрим один шаг $\text{HBM}_v$:
$$\begin{aligned}
& \text{HBM}_v (\beta,p,y,v) = \beta v - \alpha\nabla p(y)\\
& =(\beta[v]_1-\alpha\nabla_{[y]_1}p(y), \dots, \beta[v]_n-\alpha\nabla_{[y]_n}p(y)) \\
& =(\beta[v]_1-\alpha\nabla{[p]_1}([y]_1), \dots, \beta[v]_n-\alpha\nabla{[p]_n}([y]_n)) \\
& = (\text{HBM}_v (\beta_1,[p]_1,[y]_1, [v]_1), \dots, \text{HBM}_v (\beta_i,[p]_i,[y]_i, [v]_i))
\end{aligned}$$
Это показывает, что один шаг $\text{HBM}_v$ на $p$ точно эквивалентен $n$ одновременным применениям $\text{HBM}_v$ к одномерным квадратичным $[p]_i$, все с одним и тем же $\beta$ и $\alpha$. Аналогично, для $\text{HBM}_x$.

## NAG для квадратичной задачи. Доказательство (2/2)

Далее мы покажем, что NAG, примененный к одномерной квадратичной задаче с коэффициентом импульса $\beta$, эквивалентен $\text{HBM}$, примененному к той же квадратичной задаче и с тем же шагом обучения, но с коэффициентом импульса $\beta(1 -\alpha\lambda)$. Мы покажем это, раскрыв $\text{NAG}_v(\beta, [p]_i, y, v)$ (где $y$ и $v$ являются скалярами):
$$\begin{aligned}
\text{NAG}_v (\beta, [p]_i,y,v)
& =\beta v-\alpha\nabla [p]_i(y+\beta v) \\
& =\beta v-\alpha(\lambda_i(y+\beta v) -c_i) \\
& =\beta v-\alpha\lambda_i\beta v - \alpha(\lambda_i y-c_i) \\
& =\beta (1 - \alpha\lambda_i) v - \alpha\nabla[p]_i(y) \\
& =\text{HBM}_v (\beta(1-\alpha\lambda_i), [p]_i,y,v). \\
\end{aligned}$$
ч.т.д.

**Наблюдения**:

* HBM и NAG становятся **эквивалентными** когда $\alpha$ мал (когда $\alpha\lambda \ll 1$ для каждого собственного значения $\lambda$ матрицы A), поэтому NAG и HBM отличаются только когда $\alpha$ достаточно велико.
* Когда $\alpha$ относительно велико, NAG использует меньший эффективный импульс для направлений с высокой кривизной, что **предотвращает колебания** (или расхождение) и, таким образом, позволяет использовать большее $\beta$, что допускает CM при заданном $\alpha$.

