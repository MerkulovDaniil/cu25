---
title: Автоматическое дифференцирование.
author: Семинар
institute: Оптимизация для всех! ЦУ
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: false
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
---

# Напоминание с лекции

# Автоматическое дифференцирование
 
## Прямой режим
![Иллюстрация прямого цепного правила для вычисления производной функции $v_i$ по отношению к $w_k$.](auto_diff_forward.pdf){width=300}

* Использует прямое цепное правило
* Имеет сложность $d \times \mathcal{O}(T)$ операций

## Обратный режим
![Иллюстрация обратного цепного правила для вычисления производной функции $L$ по отношению к узлу $v_i$.](auto_diff_reverse.pdf){width=300}


* Использует обратное цепное правило
* Хранит информацию из прямого прохода
* Имеет сложность $\mathcal{O}(T)$ операций

# Задачи по автоматическому дифференцированию

## Игрушечный пример

::: {.callout-example}
$$
f(x_1, x_2) = x_1 * x_2 + \sin x_1
$$

Давайте вычислим производные $\dfrac{\partial f}{\partial x_i}$ используя прямой и обратный режимы.
:::

. . .

![Иллюстрация вычислительного графа функции $f(x_1, x_2)$.](autograd_example.pdf){width=300}


## Автоматическое дифференцирование с JAX



:::: {.columns}

::: {.column width="50%"}
::: {.callout-tip icon="false" title="Example №1"}
$$f(X) = tr(AX^{-1}B)$$

$$\nabla f = - X^{-T} A^T B^T X^{-T}$$

:::

. . .

:::

::: {.column width="50%"}
::: {.callout-tip icon="false" title="Example №2"}
$$g(x) = 1/3 \cdot ||x||_2^3$$

$$\nabla^2 g = ||x||_2^{-1} x x^T + ||x||_2 I_n$$
:::
:::

::::

. . .

\
\

Давайте вычислим градиенты и гессианы $f$ и $g$ в python [\faPython](https://colab.research.google.com/drive/14FXSFirBR7OI76p1z72n353Ve9LmwL90#scrollTo=61Ryf-1eWeZP&line=1&uniqifier=1)


## Задача 1

::: {.callout-question}
Какой из режимов AD вы бы выбрали (прямой/обратный) для следующего вычислительного графа арифметических операций?
:::

![Какой режим вы бы выбрали для вычисления градиентов?](ad_choose.pdf){width=175}

## Задача 2

:::: {.columns}

::: {.column width="50%"}

Предположим, у нас есть обратимая матрица $A$ и вектор $b$, вектор $x$ является решением системы линейных уравнений $Ax = b$, то есть можно записать аналитическое решение $x = A^{-1}b$.


\
\

::: {.callout-question}

Найдите производные $\dfrac{\partial L}{\partial A}, \dfrac{\partial L}{\partial b}$.

:::

:::

::: {.column width="50%"}

![$x$ может быть найден как решение линейной системы](linear_least_squares_layer.pdf){width=200}


:::
::::

## Распространение градиента через метод наименьших квадратов

:::: {.columns}

::: {.column width="40%"}

![$x$ может быть найден как решение линейной системы](linear_least_squares_layer.pdf)

:::

::: {.column width="60%"}

Предположим, у нас есть обратимая матрица $A$ и вектор $b$, вектор $x$ является решением системы линейных уравнений $Ax = b$, то есть можно записать аналитическое решение $x = A^{-1}b$, в этом примере мы покажем, что вычисление всех производных $\dfrac{\partial L}{\partial A}, \dfrac{\partial L}{\partial b}, \dfrac{\partial L}{\partial x}$, то есть обратный проход, стоит приблизительно столько же, сколько и прямой проход.

. . .

Известно, что дифференциал функции не зависит от параметризации:

$$
dL = \left\langle\dfrac{\partial L}{\partial x}, dx \right\rangle = \left\langle\dfrac{\partial L}{\partial A}, dA \right\rangle + \left\langle\dfrac{\partial L}{\partial b}, db \right\rangle
$$

. . .

Для линейной системы мы имеем:

$$
\begin{split}
Ax &= b \\
dAx + Adx = db &\to dx = A^{-1}(db - dAx)
\end{split}
$$

:::

::::

## Распространение градиента через метод наименьших квадратов

:::: {.columns}

::: {.column width="40%"}

![$x$ может быть найден как решение линейной системы](linear_least_squares_layer.pdf)

:::

::: {.column width="60%"}

Прямая подстановка дает нам:

$$
\left\langle\dfrac{\partial L}{\partial x}, A^{-1}(db - dAx) \right\rangle = \left\langle\dfrac{\partial L}{\partial A}, dA \right\rangle + \left\langle\dfrac{\partial L}{\partial b}, db \right\rangle
$$

. . .

$$
\left\langle -A^{-T}\dfrac{\partial L}{\partial x} x^T, dA \right\rangle + \left\langle A^{-T}\dfrac{\partial L}{\partial x},db \right\rangle = \left\langle\dfrac{\partial L}{\partial A}, dA \right\rangle + \left\langle\dfrac{\partial L}{\partial b}, db \right\rangle
$$

. . .

Следовательно:

$$
\dfrac{\partial L}{\partial A} = -A^{-T}\dfrac{\partial L}{\partial x} x^T \quad \dfrac{\partial L}{\partial b} =  A^{-T}\dfrac{\partial L}{\partial x}
$$

. . .

Интересно, что наиболее вычислительно интенсивная часть здесь - это обратная матрица, которая такая же, как и для прямого прохода. Иногда даже возможно хранить результат сам по себе, что делает обратный проход еще дешевле.

:::

::::

## Задача 3

:::: {.columns}

::: {.column width="50%"}

Предположим, у нас есть прямоугольная матрица $W \in \mathbb{R}^{m \times n}$, которая имеет сингулярное разложение:

\
\

$$
W = U \Sigma V^T, \quad U^TU = I, \quad V^TV = I,
$$ 
$$
\Sigma = \text{diag}(\sigma_1, \ldots, \sigma_{\min(m,n)})
$$

\
\
Регуляризатор $R(W) = \text{tr}(\Sigma)$ в любой функции потерь стимулирует низкоранговые решения. 

::: {.callout-question}

Найдите производную $\dfrac{\partial R}{\partial W}$.

:::
:::

::: {.column width="50%"}

![Вычислительный граф для сингулярного регуляризатора](svd_singular_regularizer_comp_graph.pdf){width=200}

:::
::::

## Распространение градиента через SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

Предположим, у нас есть прямоугольная матрица $W \in \mathbb{R}^{m \times n}$, которая имеет сингулярное разложение:

$$
W = U \Sigma V^T, \quad U^TU = I, \quad V^TV = I, \quad \Sigma = \text{diag}(\sigma_1, \ldots, \sigma_{\min(m,n)})
$$

1. Аналогично предыдущему примеру:

    $$
    \begin{split}
    W &= U \Sigma V^T \\
    dW &= dU \Sigma V^T + U d\Sigma V^T + U \Sigma dV^T \\
    U^T dW V &= U^TdU \Sigma V^TV + U^TU d\Sigma V^TV + U^TU \Sigma dV^TV \\
    U^T dW V &= U^TdU \Sigma + d\Sigma + \Sigma dV^TV
    \end{split}
    $$

:::

::::

## Распространение градиента через SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

2. Обратите внимание, что $U^T U = I \to dU^TU + U^T dU = 0$. Но также $dU^TU = (U^T dU)^T$, что фактически означает, что матрица $U^TdU$ является антисимметричной:

    $$
    (U^T dU)^T +  U^T dU = 0 \quad \to \quad \text{diag}( U^T dU) = (0, \ldots, 0)
    $$

    Та же логика может быть применена к матрице $V$ и

    $$
    \text{diag}(dV^T V) = (0, \ldots, 0)
    $$

3. В то же время, матрица $d \Sigma$ является диагональной, что означает (смотрите 1.) что

    $$
    \text{diag}(U^T dW V) = d \Sigma 
    $$

    Здесь на обеих сторонах мы имеем диагональные матрицы.

:::

::::

## Распространение градиента через SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

4. Теперь мы можем разложить дифференциал функции потерь как функцию $\Sigma$ - такие проблемы возникают в ML задачах, где мы должны ограничить ранг матрицы:

    $$
    \begin{split}
    dL &= \left\langle\dfrac{\partial L}{\partial \Sigma}, d\Sigma \right\rangle \\
    &= \left\langle\dfrac{\partial L}{\partial \Sigma}, \text{diag}(U^T dW V)\right\rangle \\
    &= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T \text{diag}(U^T dW V) \right)
    \end{split}
    $$

:::

::::

## Распространение градиента через SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

5. Как только мы имеем диагональные матрицы внутри произведения, след диагональной части матрицы будет равен следу всей матрицы:

    $$
    \begin{split}
    dL &= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T \text{diag}(U^T dW V) \right) \\
    &= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T U^T dW V \right)  \\
    &= \left\langle\dfrac{\partial L}{\partial \Sigma}, U^T dW V \right\rangle \\
    &= \left\langle U \dfrac{\partial L}{\partial \Sigma} V^T, dW \right\rangle 
    \end{split}
    $$

:::

::::

## Распространение градиента через SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

6. Наконец, используя другую параметризацию дифференциала

    $$
    \left\langle U \dfrac{\partial L}{\partial \Sigma} V^T, dW \right\rangle = \left\langle\dfrac{\partial L}{\partial W}, dW \right\rangle
    $$

    $$
    \dfrac{\partial L}{\partial W} =  U \dfrac{\partial L}{\partial \Sigma} V^T,
    $$

    This nice result allows us to connect the gradients $\dfrac{\partial L}{\partial W}$ and $\dfrac{\partial L}{\partial \Sigma}$.
:::

::::

## Вычислительный эксперимент с JAX

Давайте убедимся численно, что мы правильно вычислили производные в задачах 2-3 [\faPython](https://colab.research.google.com/drive/14FXSFirBR7OI76p1z72n353Ve9LmwL90#scrollTo=LlqwKMaPR0Sf) 


# Контрольные точки градиентов

## Архитектура прямого распространения

![Вычислительный граф для получения градиентов для простого прямого распространения нейронной сети с n слоями. Активации отмечены $f$. Градиент функции потерь по отношению к активациям и параметрам отмечен $b$.](backprop.pdf){width=350}

. . .

::: {.callout-important}

Результаты, полученные для узлов $f$, необходимы для вычисления узлов $b$.

:::

## Обычное обратное распространение

![Вычислительный граф для получения градиентов для простого прямого распространения нейронной сети с n слоями. Фиолетовый цвет указывает узлы, которые хранятся в памяти.](vanilla_backprop.pdf){width=350}

. . .

* Все активации $f$ хранятся в памяти после прямого прохода.

. . .


::: {.callout-tip icon="false" appearance="simple"}

* Оптимально с точки зрения вычислений: оно вычисляет каждый узел только один раз. 

:::

. . .

::: {.callout-important icon="false" appearance="simple"}

* Высокое использование памяти. Использование памяти растет линейно с количеством слоев в нейронной сети. 

:::


## Ограниченное по памятиобратное распространение

![Вычислительный граф для получения градиентов для простого прямого распространения нейронной сети с n слоями. Фиолетовый цвет указывает узлы, которые хранятся в памяти.](poor_mem_backprop.pdf){width=350}

. . .

* Каждая активация $f$ пересчитывается по мере необходимости.

. . .


::: {.callout-tip icon="false" appearance="simple"}

* Оптимально с точки зрения памяти: нет необходимости хранить все активации в памяти.

:::

. . .

::: {.callout-important icon="false" appearance="simple"}

* Вычислительно неэффективно. Количество оценок узлов масштабируется как $n^2$, в то время как в обычном обратном распространении оно масштабируется как $n$: каждый из n узлов пересчитывается порядка $n$ раз.

:::

## Контрольные точки обратного распространения

![Вычислительный граф для получения градиентов для простого прямого распространения нейронной сети с n слоями. Фиолетовый цвет указывает узлы, которые хранятся в памяти.](checkpoint_backprop.pdf){width=350}

. . .

* Компромисс между **обычным** и **ограниченным по памяти** подходами. Стратегия состоит в том, чтобы отметить подмножество активаций нейронной сети как контрольные точки, которые будут храниться в памяти.

. . .


::: {.callout-tip icon="false" appearance="simple"}

* Быстрее пересчитывание активаций $f$. Мы только пересчитываем узлы между узлом $b$ и последней контрольной точкой, предшествующей ему, при вычислении этого узла $b$ во время обратного распространения. 

:::

. . .

::: {.callout-tip icon="false" appearance="simple"}

* Использование памяти зависит от количества контрольных точек. Более эффективно, чем **обычный** подход.

:::

## Визуализация контрольных точек обратного распространения


Анимация вышеуказанных подходов [\faGithub](https://github.com/cybertronai/gradient-checkpointing)


Пример использования контрольных точек градиентов [\faGithub](https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/gradient-checkpointing-nin.ipynb)


## Оценка следа Гессиана ^[[A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines - M.F. Hutchinson, 1990](https://www.tandfonline.com/doi/abs/10.1080/03610919008812866)]

Этот пример иллюстрирует оценку следа Гессиана нейронной сети с использованием метода Hutchinson, который является алгоритмом для получения такой оценки из матрично-векторных произведений:

Пусть $X \in \mathbb{R}^{d \times d}$  и $v \in \mathbb{R}^d$ будет случайным вектором таким, что $\mathbb{E}[vv^T] = I$. Тогда,

:::: {.columns}
::: {.column width="40%"}

$$
\mathrm{Tr}(X) = \mathbb{E}[v^TXv] = \frac{1}{V}\sum_{i=1}^{V}v_i^TXv_i.
$$


 Пример использования оценки следа Гессиана Hutchinson [\faGithub](https://colab.research.google.com/drive/1aLx_-Sv2tTTKz0NCEFcedqQyopBUczJH#scrollTo=DZTgqcHoa8O3)

:::
::: {.column width="60%"}
![Несколько запусков оценки следа Гессиана Hutchinson, инициализированных при разных случайных начальных значениях.](Hutchinson_trace_est.pdf)
:::
::::
