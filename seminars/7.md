---
title: Градиентный спуск. Скорости сходимости
author: Семинар
institute: Оптимизация для всех! ЦУ
format:
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: false
        incremental: false
        include-in-header: ../files/xeheader.tex  # 
---

# Воспоминания с лекции

## Градиентный спуск


Предположим, у нас есть задача минимизации гладкой функции $f(x): \mathbb{R}^n \to \mathbb{R}$:

$$
f(x) \to \min_{x \in \mathbb{R}}
$$


. . . 


Один из методов решения этой задачи — **градиентный спуск**: 

$$
x_{k+1} = x_k - \eta_k\nabla f(x_k)
$$

. . .

Бутылочным горлышком (почти для всех градиентных методов) является выбор шага, который может привести к драматическим различиямв поведении метода.



## Как выбрать шаг

- Одно из теоретических предложений: выбирать шаг, обратно пропорциональный константе Липшица градиента $$\eta_k = \dfrac{1}{L}$$

. . .

- **Линейный поиск с возвратом**. Зафиксируем два параметра: $0<\beta<1$ и $0<\alpha \leq 0.5$. На каждой итерации начинаем с $\mathrm{t}=1$, и пока
$$
f(x_k - t \nabla f(x_k))>f(x_k)-\alpha t\|\nabla f(x_k)\|_2^2,
$$
уменьшаем $t=\beta t$. Иначе выполняем обновление градиентного спуска $x_{k+1}= x_{k} - t \nabla f(x_{k})$.

. . .

- **Точный линейный поиск**. 
$$
\eta_k =\underset{ \eta \geq 0}{\arg \min } f(x_k - \eta \nabla f(x_k))
$$


# Градиентный спуск

## Направление локального наискорейшего спуска

:::: {.columns}
::: {.column width="40%"}

Рассмотрим линейное приближение дифференцируемой функции $f$ вдоль некоторого направления $h, \|h\|_2 = 1$:

. . .

$$
f(x + \alpha h) = f(x) + \alpha \langle f'(x), h \rangle + o(\alpha)
$$

. . .

Мы хотим, чтобы $h$ было убывающим направлением:

$$
f(x + \alpha h) < f(x)
$$

$$
f(x) + \alpha \langle f'(x), h \rangle + o(\alpha) < f(x)
$$

. . .

и переходя к пределу при $\alpha \rightarrow 0$:

$$
\langle f'(x), h \rangle \leq 0
$$

:::

. . .

::: {.column width="60%"}

Также из неравенства Коши — Буняковского — Шварца:

$$
\begin{split}
|\langle f'(x), h \rangle | &\leq \| f'(x) \|_2 \| h \|_2 \\
\langle f'(x), h \rangle &\geq -\| f'(x) \|_2 \| h \|_2 = -\| f'(x) \|_2
\end{split}
$$

. . .

Таким образом, направление антиградиента

$$
h = -\dfrac{f'(x)}{\|f'(x)\|_2}
$$

дает направление **локального наискорейшего** убывания функции $f$.

. . .

Результатом этого метода является

$$
x_{k+1} = x_k - \alpha f'(x_k)
$$

:::
::::


## Точка минимума липшицевой параболы

:::: {.columns}
::: {.column width="50%"}

Если функция $f: \mathbb{R}^n \to \mathbb{R}$ непрерывно дифференцируема и ее градиент удовлетворяет условиям Липшица с константой $L$, то $\forall x,y \in \mathbb{R}^n$:

$$
|f(y) - f(x) - \langle \nabla f(x), y-x \rangle| \leq \frac{L}{2} \|y-x\|^2,
$$

. . .

что геометрически означает, что если мы зафиксируем некоторую точку $x_0 \in \mathbb{R}^n$ и определим две параболы:

$$
\begin{split}
\phi_1(x) &= f(x_0) + \langle \nabla f(x_0), x - x_0 \rangle - \frac{L}{2} \|x-x_0\|^2,\\
\phi_2(x) &= f(x_0) + \langle \nabla f(x_0), x - x_0 \rangle + \frac{L}{2} \|x-x_0\|^2.
\end{split}
$$

. . .

Тогда 

$$
\phi_1(x) \leq f(x) \leq \phi_2(x) \quad \forall x \in \mathbb{R}^n.
$$

. . .

Теперь, если у нас есть глобальная верхняя граница функции в виде параболы, мы можем попробовать сразу попасть в ее минимум.

:::

. . .

::: {.column width="50%"}
![Иллюстрация](lipschitz_parabola.pdf){width=40%}

. . .

$$
\begin{split}
& \nabla \phi_2(x) = 0 \\
& \nabla f(x_0) + L (x^* - x_0) = 0 \\
& x^* = x_0 - \frac{1}{L}\nabla f(x_0) \\
& x_{k+1} = x_k - \frac{1}{L} \nabla f(x_k)
\end{split}
$$

Таким образом, мы получаем шаг $\frac{1}{L}$. Однако часто константа $L$ неизвестна.
:::
::::

## Любая $\mu$-строго выпуклая дифференцируемая функция является PL-функцией

:::{.callout-theorem}
Если функция $f(x)$ дифференцируема и $\mu$-строго выпукла, то она является PL-функцией.
:::

**Доказательство**

:::: {.columns}

::: {.column width="60%"}

По критерию строгой выпуклости первого порядка:
$$
f(y) \geq f(x) + \nabla f(x)^T(y-x) + \dfrac{\mu}{2}\|y-x\|_2^2
$$
Положим $y = x^*$:
$$
\begin{split}
\uncover<+->{ f(x^*) &\geq f(x) + \nabla f(x)^T(x^*-x) + \dfrac{\mu}{2}\|x^*-x\|_2^2 \\ }
\uncover<+->{ f(x) - f(x^*) &\leq \nabla f(x)^T(x-x^*) - \dfrac{\mu}{2}\|x^*-x\|_2^2 = \\ }
\uncover<+->{ &= \left(\nabla f(x)^T - \dfrac{\mu}{2}(x^*-x)\right)^T (x-x^*) = \\ }
\uncover<+->{ &= \frac12 \left(\frac{2}{\sqrt{\mu}}\nabla f(x)^T - \sqrt{\mu}(x^*-x)\right)^T \sqrt{\mu}(x-x^*) \\ }
\end{split}
$$
:::

. . .

::: {.column width="40%"}

Пусть $a = \frac{1}{\sqrt{\mu}}\nabla f(x)$ \\
и $b =\sqrt{\mu}(x-x^*) -\frac{1}{\sqrt{\mu}}\nabla f(x)$ 

. . .

Тогда $a+b = \sqrt{\mu}(x-x^*)$ и $a-b=\frac{2}{\sqrt{\mu}}\nabla f(x)-\sqrt{\mu}(x-x^*)$
:::
::::

## Любая $\mu$-строго выпуклая дифференцируемая функция является PL-функцией

$$
\begin{split}
\uncover<+->{ f(x) - f(x^*) &\leq \frac12 \left(\frac{1}{\mu}\|\nabla f(x)\|^2_2 - \left\|\sqrt{\mu}(x-x^*) -\frac{1}{\sqrt{\mu}}\nabla f(x)\right\|_2^2\right) \\ }
\uncover<+->{ f(x) - f(x^*) &\leq \frac{1}{2\mu}\|\nabla f(x)\|^2_2, \\ }
\end{split}
$$

. . .

что точно соответствует условию PL. Это означает, что мы уже имеем доказательство линейной сходимости для любой строго выпуклой функции.

## Точный линейный поиск, или наискорейший спуск

:::: {.columns}
::: {.column width="80%"}

$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_{k+1}) = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$

Более теоретический, чем практический подход. Он также позволяет анализировать сходимость, но часто точный линейный поиск может быть сложным, если вычисление функции занимает слишком много времени или стоит слишком дорого.

Интересное теоретическое свойство этого метода заключается в том, что каждая следующая итерация ортогональна предыдущей:

$$
\alpha_k = \text{arg}\min_{\alpha \in \mathbb{R^+}} f(x_k - \alpha \nabla f(x_k))
$$

Условия оптимальности:

$$
\nabla f(x_{k+1})^\top \nabla f(x_k) = 0
$$
:::
::: {.column width="20%"}

![Наискорейший спуск](GD_vs_Steepest.pdf)

[Открыть в Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Steepest_descent.ipynb)
:::
::::

## Анализ сходимости. Линейный поиск с возвратом

Предположим, что $f$ выпукла, дифференцируема и имеет константу Липшица $L>0$.

::: {.callout-tip icon="false" title="Theorem"}

 Градиентный спуск с фиксированным шагом $t \leq 1 / L$ удовлетворяет
$$
f\left(x^{(k)}\right)-f^* \leq \frac{\left\|x^{(0)}-x^*\right\|_2^2}{2 t k}
$$

:::

. . .

Покажем, что скорость сходимости для линейного поиска с возвратом не хуже, чем $O(1/k)$

. . .

Поскольку $\nabla f$ непрерывно дифференцируема с константой Липшица $L>0$, мы имеем
$$
f(y) \leq f(x)+\nabla f(x)^T(y-x)+\frac{L}{2}\|y-x\|_2^2, \forall x, y
$$

. . .

Пусть $y=x^{+}=x-t \nabla f(x)$, тогда:
$$
f\left(x^{+}\right) \leq f(x) -\left(1-\frac{L t}{2}\right) t \|\nabla f(x)\|_2^2 \leq f(x) - \dfrac{1}{2 L} \|\nabla f(x)\|_2^2
$$
Это соответствует условию остановки линейного поиска с возвратом при $\alpha=0.5, t=\frac{1}{L}$. Следовательно, при липшицевом градиенте такой поиск гарантирует скорость сходимости $O(1 / k)$.


# Практика!
## Задача

Рассмотрим задачу
$$
    \min_{x \in \mathbb R^n} f(x),
$$
где $f(x)$ выпукла и $L$-гладкая. Найдите скорость сходимости градиентного спуска с оптимальным теоретическим шагом $\eta_k = \frac{1}{L}$ для \textit{усредненной точки} и для \textit{лучшей точки}. 
Другими словами, получите верхние границы на 

- $f(\bar{x}_N) - f^*,\ \text{where}\ \bar{x}_N = \frac{1}{N} \sum_{i=0}^{N-1} x_i$, 
- $\min_{0 \le i \le N-1} f(x_i) - f^*$.

:::{.callout-note}
## Шаг градиентного спуска
$$
x_{k + 1} = \arg \min_{x \in \mathbb R^n} \left\{ \Psi_k(x) \equiv f(x_k) + \langle \nabla f(x_k), x - x_k \rangle + \frac{L}{2} \| x - x_k \|_2^2 \right\}
$$
:::

. . .

:::{.callout-tip}
## Совет
Используйте факт, что $\Psi_k(x)$ является $L$-строго выпуклой из-за квадратичного регуляризатора.
:::

## Код

Примеры: [\faPython code snippet](https://colab.research.google.com/drive/19s2QZKkH2T6uOwUCEMGQsVjVVlapThnj?usp=sharing).
