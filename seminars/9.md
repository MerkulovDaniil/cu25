---
title: Метод сопряженных градиентов
author: Семинар
institute: Оптимизация для всех! ЦУ
format:
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: false
        incremental: false
        include-in-header: ../files/xeheader.tex  #
    beamer-cu:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader_cu.tex   
      header-includes:
        - \newcommand{\cover}{../files/БАК_Методы вып_оптимизации_презентация_9.pdf}
    beamer-cu-maga:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader_cu.tex   
      header-includes:
        - \newcommand{\cover}{../files/Методы вып_оптимизации_презентация_9.pdf}
---


# Повторение лекции
## Сильно выпуклые квадратичные функции

:::: {.columns}

::: {.column width="60%"}

Рассмотрим следующую квадратичную задачу оптимизации:

$$
\min\limits_{x \in \mathbb{R}^d} f(x) =  \min\limits_{x \in \mathbb{R}^d} \dfrac{1}{2} x^\top  A x - b^\top  x + c, \text{  где }A \in \mathbb{S}^d_{++}.
$$
:::

::: {.column width="40%"}
Условия оптимальности:
$$
\nabla f(x^{*}) = Ax^* - b = 0 \iff Ax^* = b
$$
:::
::::

. . .

![](../files/SD_vs_CG.pdf)


## Обзор метода сопряжённых градиентов для квадратичной задачи
 

1) **Инициализация.** $k = 0$ и $x_k = x_0$, $d_k = d_0 = -\nabla f(x_0)$.

. . .

2) **Оптимальная длина шага.** С помощью процедуры одномерного поиска (*line search*) находим оптимальную длину шага. Это означает вычислить $\alpha_{k}$, минимизирующий $f(x_k + \alpha_k d_k)$:

$$
\alpha_k = -\frac{d_k^\top (A x_k - b)}{d_k^\top A d_k}
$$

. . .

3) **Итерация алгоритма.** Обновляем положение $x_k$, двигаясь в направлении $d_k$ с длиной шага $\alpha_k$:

$$
x_{k+1} = x_k + \alpha_k d_k
$$

. . .

4) **Обновление направления.** Обновляем $d_{k+1} = -\nabla f(x_{k+1}) + \beta_k d_k$, где $\beta_k$ вычисляется по формуле:

$$
\beta_k = \frac{\nabla f(x_{k+1})^\top A d_k}{d_k^\top A d_k}.
$$

. . .

5) **Цикл до сходимости.** Повторяем шаги 2–4, пока не построено $n$ направлений, где $n$ — размерность пространства (размерность $x$).

## Оптимальная длина шага

Точный одномерный поиск:
$$
\alpha_k=\arg \min _{\alpha \in \mathbb{R}^{+}} f\left(x_{k+1}\right)=\arg \min _{\alpha \in \mathbb{R}^{+}} f\left(x_k + \alpha d_k\right)
$$

. . .

Найдём аналитическое выражение для шага $\alpha_k$:

$$
f\left(x_k + \alpha d_k\right) = \frac{1}{2} \left(x_k + \alpha d_k\right)^{\top} A\left(x_k+\alpha d_k\right) - b^{\top}\left(x_k + \alpha d_k\right) + c
$$
$$
= \frac{1}{2} \alpha^2 d_k^{\top} A d_k+d_k^{\top}\left(A x_k-b\right) \alpha+\left(\frac{1}{2} x_k^{\top} A x_k+x_k^{\top} d_k+c\right)
$$

. . .

Поскольку $A \in \mathbb{S}^d_{++}$, точка с нулевой производной на этой параболе является минимумом:

$$
\left(d_k^{\top} A d_k\right) \alpha_k+d_k^{\top}\left(A x_k-b\right)=0 \iff \alpha_k=-\frac{d_k^{\top}\left(A x_k-b\right)}{d_k^{\top} A d_k}
$$

## Обновление направления

Обновляем направление так, чтобы следующее направление было $A$‑ортогонально предыдущему:

$$
d_{k+1} \perp_A d_{k} \iff d_{k+1}^{\top} A d_{k} = 0
$$

. . .

Поскольку $d_{k+1} = -\nabla f(x_{k+1}) + \beta_k d_k$, выбираем $\beta_k$ так, чтобы выполнялась $A$‑ортогональность:

$$
d_{k+1}^{\top} A d_{k} = -\nabla f\left(x_{k+1}\right)^{\top} A d_k + \beta_k d_k^{\top} A d_k = 0 \iff \beta_k = \frac{\nabla f\left(x_{k+1}\right)^{\top} A d_k}{d_k^{\top} A d_k}
$$

. . .

::: {.callout-tip title="Лемма 1"}
Все направления, строящиеся по описанной выше процедуре, $A$-ортогональны друг другу:
$$
d_{i}^{\top} A d_{j} = 0, \text{ if } i \neq j
$$
$$
d_{i}^{\top} A d_{j} > 0, \text{ if } i = j
$$
:::


## $A$-ортогональность

[![](A_orthogonality.pdf){#fig-aorth}](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/CG.ipynb)


## Сходимость метода сопряжённых градиентов


::: {.callout-tip title="Лемма 2"}
Пусть решается $n$-мерная квадратичная выпуклая задача оптимизации. Метод сопряжённых направлений:
$$
x_{k+1}=x_0+\sum_{i=0}^k \alpha_i d_i,
$$

где $\alpha_i = -\dfrac{d_i^\top (A x_i - b)}{d_i^\top A d_i}$, взятые из одномерного поиска, обеспечивают сходимость не более чем за $n$ шагов алгоритма.

:::

## Метод сопряжённых градиентов на практике

На практике для шага $\alpha_k$ и коэффициента $\beta_{k}$ обычно используют следующие формулы:

$$
\alpha_k = \dfrac{r^{\top}_k r_k}{d^{\top}_{k}A d_{k}} \qquad \beta_k = \dfrac{r^{\top}_{k + 1} r_{k + 1}}{r^{\top}_{k} r_{k}},
$$

где $r_k = b - Ax_k$, так как $x_{k+1} = x_{k} + \alpha_k d_k$, то $r_{k+1} = r_k - \alpha_k A d_k$. Также, $r_i^T r_k = 0, \forall i \neq k$ (**Лемма 5** из лекции).

. . .

Выведем выражение для $\beta_k$:
$$
\beta_k = \frac{\nabla f\left(x_{k+1}\right)^{\top} A d_k}{d_k^{\top} A d_k} =  -\frac{r_{k+1}^{\top} A d_k}{d_k^{\top} A d_k}
$$

. . .

$\text{Числитель: }r_{k+1}^{\top} A d_k=\frac{1}{\alpha_k} r_{k+1}^{\top}\left(r_k-r_{k+1}\right)= [r_{k+1}^{\top} r_{k} = 0] = -\frac{1}{\alpha_k} r_{k+1}^{\top} r_{k+1}$
$\text{Знаменатель: }d_k^{\top} A d_k=\left(r_k+\beta_{k-1} d_{k-1}\right)^{\top} {A} {d}_k=\frac{1}{\alpha_k} {r}_k^{\top}\left({r}_k-{r}_{k+1}\right)=\frac{1}{\alpha_k} {r}_k^{\top} {r}_k$

. . .

::: {.callout-question}
Почему эта модификация лучше стандартной версии?
:::

## Метод сопряжённых градиентов на практике. Псевдокод

$$
\begin{aligned}
& r_0 := b - A x_0 \\
& \hbox{if } r_{0} \text{ is sufficiently small, then return } x_{0} \text{ as the result}\\
& d_0 := r_0 \\
& k := 0 \\
& \text{repeat} \\
& \qquad \alpha_k := \frac{r_k^\mathsf{T} r_k}{d_k^\mathsf{T} A d_k}  \\
& \qquad x_{k+1} := x_k + \alpha_k d_k \\
& \qquad r_{k+1} := r_k - \alpha_k A d_k \\
& \qquad \hbox{if } r_{k+1} \text{ is sufficiently small, then exit loop} \\
& \qquad \beta_k := \frac{r_{k+1}^\mathsf{T} r_{k+1}}{r_k^\mathsf{T} r_k} \\
& \qquad d_{k+1} := r_{k+1} + \beta_k d_k \\
& \qquad k := k + 1 \\
& \text{end repeat} \\
& \text{return } x_{k+1} \text{ as the result}
\end{aligned}
$$


## Упражнение 1

::: {.callout}
Реализуйте итерации метода сопряжённых градиентов для квадратичной задачи
$$
f(x) = \dfrac{1}{2}x^TAx - b^Tx \longrightarrow \min_{x\in\mathbb{R}^n}
$$
и запустите эксперименты для нескольких матриц $A$. Смотрите код здесь [\faPython](https://colab.research.google.com/github/MerkulovDaniil/hse25/blob/main/notebooks/s10_ex1.ipynb).
:::


## Нелинейный метод сопряжённых градиентов

Если у нас нет аналитического выражения для функции или её градиента, мы, скорее всего, не сможем аналитически решить одномерную задачу минимизации. 
Поэтому $\alpha_k$ подбирается обычной процедурой одномерного поиска. 
Но для выбора $\beta_k$ есть следующий математический трюк:

Для двух последовательных итераций верно:

$$
x_{k+1} - x_k = c d_k,
$$

где $c$ — некоторая константа. Тогда для квадратичного случая имеем:

$$ 
\nabla f(x_{k+1}) - \nabla f(x_k) = (A x_{k+1} - b) - (A x_k - b) = A(x_{k+1}-x_k) = cA d_k
$$

Выражая из этого равенства $Ad_k = \dfrac{1}{c} \left( \nabla f(x_{k+1}) - \nabla f(x_k)\right)$, избавляемся от «знания» функции в определении шага $\beta_k$, тогда пункт 4 переписывается так:

$$
\beta_k = \frac{\nabla f(x_{k+1})^\top (\nabla f(x_{k+1}) - \nabla f(x_k))}{d_k^\top (\nabla f(x_{k+1}) - \nabla f(x_k))}.
$$

Этот метод называется методом Полака—Рибьера.

## Упражнение 2
::: {.callout}
Реализуйте итерации метода Полака—Рибьера и запустите эксперименты для нескольких $\mu$ в бинарной логистической регрессии:
$$
f(x)=\dfrac{\mu}{2}\| x \|^2_2 + \dfrac{1}{m}\sum_{i=1}^{m}log(1+exp(-y_i\langle a_i, x \rangle)) \longrightarrow \min_{x\in\mathbb{R}^n}
$$
Смотрите код здесь [\faPython](https://colab.research.google.com/github/MerkulovDaniil/hse25/blob/main/notebooks/s10_ex2.ipynb).
:::
# Численные эксперименты
## Патологический пример
Пусть $t\in(0, 1)$ и
$$
W =
\begin{bmatrix}
t & \sqrt{t} & & & \\
\sqrt{t} & 1 + t & \sqrt{t} & & \\
& \sqrt{t} & 1 + t & \sqrt{t} & \\
& & \ddots & \ddots & \ddots \\
& & & \sqrt{t} & 1 + t
\end{bmatrix},
\quad
b =
\begin{bmatrix}
1 \\
0 \\
\vdots \\
0
\end{bmatrix}
$$
Так как $W$ невырожденна, существует единственное решение $Wx=b$. Решение методом сопряжённых градиентов даёт довольно плохую сходимость. Во время работы CG ошибка растёт экспоненциально (!), пока внезапно не становится нулевой, когда находится единственное решение.
Невязка $\| Wx_k - b \|^2$ растёт экспоненциально как $(1/t)^k$ до $n$-й итерации, после чего резко падает к нулю.
См. эксперимент здесь [\faPython](https://colab.research.google.com/github/MerkulovDaniil/hse25/blob/main/notebooks/s10_pathological_example.ipynb).
## Другие численные эксперименты
Посмотрим другие примеры здесь [\faPython](https://colab.research.google.com/drive/1N_PH8h8corIpVZSsXDzJ9Utpv7vVp6f6?usp=sharing).
Код взят из [\faGithub](https://github.com/amkatrutsa/optimization_course/blob/master/Spring2022/cg.ipynb).
