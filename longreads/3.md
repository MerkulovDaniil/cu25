---
title: Автоматическое дифференцирование
author: Даня Меркулов
institute: Оптимизация для всех! ЦУ
format:
    pdf:
        include-in-header: ../files/longread_header.tex
        keep-tex: true
number-sections: true
---


# Автоматическое дифференцирование

## {.plain}
![Когда вы поняли идею](autograd_expectations.jpeg){width=65%}

## {.plain}
![Это не autograd](avtograd.jpeg){width=45%}

## Задача

Предположим, что мы хотим решить следующую задачу:

$$
L(w) \to \min_{w \in \mathbb{R}^d}
$$


* Такие задачи обычно возникают в машинном обучении, когда вам нужно найти подходящие параметры $w$ модели (например, обучить нейронную сеть). 
* Вы можете использовать множество алгоритмов для решения этой задачи. Однако, учитывая современный размер задачи, где $d$ может достигать десятков миллиардов, это очень сложно решить без информации о градиентах, используя алгоритмы нулевого порядка. 
* Поэтому было бы полезно уметь вычислять вектор градиента $\nabla_w L = \left( \frac{\partial L}{\partial w_1}, \ldots, \frac{\partial L}{\partial w_d}\right)^T$. 
* Обычно первые методы работают лучше в больших задачах, в то время как вторые методы требуют слишком много памяти.

## Пример: многомерное масштабирование

Предположим, что у нас есть матрица расстояний для $N$ $d$-мерных объектов $D \in \mathbb{R}^{N \times N}$. Учитывая эту матрицу, мы хотим восстановить исходные координаты $W_i \in \mathbb{R}^d, \; i = 1, \ldots, N$.


$$
L(W) = \sum_{i, j = 1}^N \left(\|W_i - W_j\|^2_2 - D_{i,j}\right)^2 \to \min_{W \in \mathbb{R}^{N \times d}}
$$


Ссылка на  наглядную визуализацию [$\clubsuit$](http://www.benfrederickson.com/numerical-optimization/), где можно увидеть, что методы без градиента обрабатывают эту задачу намного медленнее, особенно в пространствах большой размерности.

:::{.callout-question}
Связано ли это с PCA?
:::

## Пример: многомерное масштабирование

![[Ссылка на анимацию](https://fmin.xyz/docs/visualizations/mds.mp4)](mds.png){width=40%}

## Пример: градиентный спуск без градиента

:::: {.columns}
::: {.column width="50%"}
Предположим, что мы хотим решить следующую задачу:

$$
L(w) \to \min_{w \in \mathbb{R}^d}
$$


с помощью алгоритма градиентного спуска (GD):

$$
w_{k+1} = w_k - \alpha_k \nabla_w L(w_k)
$$


Можно ли заменить $\nabla_w L(w_k)$ используя только информацию нулевого порядка? 


Да, но за определенную цену.


Можно рассмотреть оценку 2-точечного градиента^[предлагаю [хорошую](https://scholar.harvard.edu/files/yujietang/files/slides_2019_zero-order_opt_tutorial.pdf) презентацию о методах без градиента] $G$:

$$
G = d\dfrac{L(w + \varepsilon v)- L(w - \varepsilon v)}{2 \varepsilon}v, 
$$

где $v$ сферически симметричен.
:::


::: {.column width="50%"}
!["Иллюстрация двухточечной оценки градиентного спуска"](zgd_2p.pdf)
:::

::::


## Пример: конечные разности

:::: {.columns}
::: {.column width="50%"}

$$
w_{k+1} = w_k - \alpha_k G
$$

 
Можем также рассмотреть идею конечных разностей:

$$
G =  \sum\limits_{i=1}^d\dfrac{L(w+\varepsilon e_i) - L(w-\varepsilon e_i)}{2\varepsilon} e_i
$$

[Открыть в Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Zero_order_GD.ipynb)

:::

::: {.column width="50%"}
!["Иллюстрация оценки конечных разностей градиентного спуска"](zgd_fd.pdf)
:::

::::

## Проклятие размерности для методов нулевого порядка ^[[Оптимальные скорости для нулевого порядка выпуклой оптимизации: сила двух оценок функции](https://arxiv.org/pdf/1312.2139)]

$$
\min_{x \in \mathbb{R}^n} f(x)
$$


$$
\text{GD: } x_{k+1} = x_k - \alpha_k \nabla f(x_k) \qquad \qquad \text{Zero order GD: } x_{k+1} = x_k - \alpha_k G,
$$

где $G$ - оценка градиента 2-точечная или многоточечная.


|  | $f(x)$ - гладкая | $f(x)$ - гладкая и выпуклая | $f(x)$ - гладкая и сильно выпуклая |
|:-:|:---:|:----:|:-------:|
| GD | $\|\nabla f(x_k)\|^2 \approx \mathcal{O} \left( \dfrac{1}{k} \right)$ | $f(x_k) - f^* \approx  \mathcal{O} \left( \dfrac{1}{k} \right)$ | $\|x_k - x^*\|^2 \approx \mathcal{O} \left( \left(1 - \dfrac{\mu}{L}\right)^k \right)$ |
| Zero order GD | $\|\nabla f(x_k)\|^2 \approx \mathcal{O} \left( \dfrac{n}{k} \right)$ | $f(x_k) - f^* \approx  \mathcal{O} \left( \dfrac{n}{k} \right)$ | $\|x_k - x^*\|^2 \approx \mathcal{O} \left( \left(1 - \dfrac{\mu}{n L}\right)^k \right)$ |

Для 2-точечных оценок, вы не можете сделать зависимость лучше, чем на $\sqrt{n}$ !


## Конечные разности

Наивный подход к получению приблизительных значений градиентов - это подход **конечных разностей**. Для каждой координаты, можно вычислить приближенное значение частной производной:

$$
\dfrac{\partial L}{\partial w_k} (w) \approx \dfrac{L(w+\varepsilon e_k) - L(w)}{\varepsilon}, \quad e_k = (0, \ldots, \underset{{\tiny k}}{1}, \ldots, 0)
$$


:::{.callout-question}
Если время, необходимое для одного вычисления $L(w)$ равно $T$, то какое время необходимо для вычисления $\nabla_w L$ с этим подходом?


**Ответ** $2dT$, что очень долго для больших задач. Кроме того, этот точный метод нестабилен, что означает, что вам придется выбирать между точностью и стабильностью.


**Теорема**

Существует алгоритм для вычисления $\nabla_w L$ в $\mathcal{O}(T)$ операциях. ^[Linnainmaa S. The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors.  Master’s Thesis (in Finnish), Univ. Helsinki, 1970.]

:::

## Прямой режим автоматического дифференцирования

Чтобы глубже понять идею автоматического дифференцирования, рассмотрим простую функцию для вычисления производных: 

$$
L(w_1, w_2) = w_2 \log w_1 + \sqrt{w_2 \log w_1}
$$


Давайте нарисуем *вычислительный граф* этой функции:

![Иллюстрация вычислительного графа для функции $L(w_1, w_2)$](comp_graph.pdf)


Давайте пойдем от начала графа к концу и вычислим производную $\dfrac{\partial L}{\partial w_1}$.

## Прямой режим автоматического дифференцирования{.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](comp_graph1.pdf)

:::: {.columns}

::: {.column width="50%"}
### Функция 

$w_1 = w_1, w_2 = w_2$
:::


::: {.column width="50%"}
### Производная

$\dfrac{\partial w_1}{\partial w_1} = 1, \dfrac{\partial w_2}{\partial w_1} = 0$ 
:::

::::




## Прямой режим автоматического дифференцирования{.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](comp_graph2.pdf)



:::: {.columns}

::: {.column width="50%"}
### Функция 

$v_1 = \log w_1$ 
:::



::: {.column width="50%"}
### Производная

$\frac{\partial v_1}{\partial w_1} = \frac{\partial v_1}{\partial w_1} \frac{\partial w_1}{\partial w_1} = \frac{1}{w_1} 1$
:::

::::

## Прямой режим автоматического дифференцирования{.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](comp_graph3.pdf)



:::: {.columns}

::: {.column width="50%"}
### Функция 

$v_2 = w_2 v_1$
:::



::: {.column width="50%"}
### Производная

$\frac{\partial v_2}{\partial w_1} = \frac{\partial v_2}{\partial v_1}\frac{\partial v_1}{\partial w_1} + \frac{\partial v_2}{\partial w_2}\frac{\partial w_2}{\partial w_1} = w_2\frac{\partial v_1}{\partial w_1} + v_1\frac{\partial w_2}{\partial w_1}$
:::

::::

## Прямой режим автоматического дифференцирования{.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](comp_graph4.pdf)



:::: {.columns}

::: {.column width="50%"}
### Функция 

$v_3 = \sqrt{v_2}$
:::



::: {.column width="50%"}
### Производная

$\frac{\partial v_3}{\partial w_1} = \frac{\partial v_3}{\partial v_2}\frac{\partial v_2}{\partial w_1} = \frac{1}{2\sqrt{v_2}}\frac{\partial v_2}{\partial w_1}$
:::

::::

## Прямой режим автоматического дифференцирования{.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](comp_graph5.pdf)



:::: {.columns}

::: {.column width="50%"}
### Функция 

$L = v_2 + v_3$ 
:::



::: {.column width="50%"}
### Производная

$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial v_2}\frac{\partial v_2}{\partial w_1} + \frac{\partial L}{\partial v_3}\frac{\partial v_3}{\partial w_1} = 1\frac{\partial v_2}{\partial w_1} + 1\frac{\partial v_3}{\partial w_1}$
:::

::::

## Сделайте аналогичные вычисления для $\dfrac{\partial L}{\partial w_2}$

![Иллюстрация вычислительного графа для функции $L(w_1, w_2)$](comp_graph.pdf)


## Пример прямого режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](cgraph_ex_1.pdf)

:::: {.columns}

::: {.column width="50%"}
### Функция 

$w_1 = w_1, w_2 = w_2$
:::

::: {.column width="50%"}
### Производная

$\dfrac{\partial w_1}{\partial w_2} = 0, \dfrac{\partial w_2}{\partial w_2} = 1$
:::

::::

## Пример прямого режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](cgraph_ex_2.pdf)

:::: {.columns}

::: {.column width="50%"}
### Функция 

$v_1 = \log w_1$
:::

::: {.column width="50%"}
### Derivative

$\frac{\partial v_1}{\partial w_2} = \frac{\partial v_1}{\partial w_1} \frac{\partial w_1}{\partial w_2}= \frac{1}{w_1} \cdot 0$
:::

::::

## Пример прямого режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](cgraph_ex_3.pdf)

:::: {.columns}

::: {.column width="50%"}
### Функция 

$v_2 = w_2 v_1$
:::

::: {.column width="50%"}
### Производная

$\frac{\partial v_2}{\partial w_2} = \frac{\partial v_2}{\partial v_1}\frac{\partial v_1}{\partial w_2} + \frac{\partial v_2}{\partial w_2}\frac{\partial w_2}{\partial w_2} = w_2\frac{\partial v_1}{\partial w_2} + v_1\frac{\partial w_2}{\partial w_2}$ 
:::

::::

## Пример прямого режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](cgraph_ex_4.pdf)

:::: {.columns}

::: {.column width="50%"}
### Функция 

$v_3 = \sqrt{v_2}$
:::

::: {.column width="50%"}
### Производная

$\frac{\partial v_3}{\partial w_2} = \frac{\partial v_3}{\partial v_2}\frac{\partial v_2}{\partial w_2} = \frac{1}{2\sqrt{v_2}}\frac{\partial v_2}{\partial w_2}$
:::

::::

## Пример прямого режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация прямого режима автоматического дифференцирования](cgraph_ex_5.pdf)

:::: {.columns}

::: {.column width="50%"}
### Функция 

$L = v_2 + v_3$
:::

::: {.column width="50%"}
### Производная

$\frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial v_2}\frac{\partial v_2}{\partial w_2} + \frac{\partial L}{\partial v_3}\frac{\partial v_3}{\partial w_2} = 1\frac{\partial v_2}{\partial w_2} + 1\frac{\partial v_3}{\partial w_2}$
:::

::::

## Алгоритм прямого режима автоматического дифференцирования


:::: {.columns}

::: {.column width="50%"}

Предположим, что у нас есть вычислительный граф $v_i, i \in [1; N]$. Наша цель - вычислить производную выхода этого графа по некоторой входной переменной $w_k$, т.е. $\dfrac{\partial v_N}{\partial w_k}$. Эта идея предполагает распространение градиента по входной переменной от начала к концу, поэтому мы можем ввести обозначение: 


$$
\overline{v_i} = \dfrac{\partial v_i}{\partial w_k}
$$

![Иллюстрация прямого режима автоматического дифференцирования](auto_diff_forward.pdf){width=80%}

:::


::: {.column width="50%"}

* Для $i = 1, \ldots, N$:
    * Вычислить $v_i$ как функцию его родителей (входов) $x_1, \ldots, x_{t_i}$:
 $$
        v_i = v_i(x_1, \ldots, x_{t_i})
        $$
    * Вычислить производную $\overline{v_i}$ используя прямой режим автоматического дифференцирования:
 $$
        \overline{v_i} = \sum_{j = 1}^{t_i}\dfrac{\partial v_i}{\partial x_j}\dfrac{\partial x_j}{\partial w_k}
        $$


Обратите внимание, что этот подход не требует хранения всех промежуточных вычислений, но можно видеть, что для вычисления производной $\dfrac{\partial L}{\partial w_k}$ нам нужно $\mathcal{O}(T)$ операций. Это означает, что для всего градиента, нам нужно $d\mathcal{O}(T)$ операций, что то же самое, что и для конечных разностей, но теперь мы не имеем проблем со стабильностью, или неточностями(формулы выше точны).

:::

::::

## {.plain}
![](yoda.jpg)


## Обратный режим автоматического дифференцирования

Мы рассмотрим ту же функцию с вычислительным графом:

![Иллюстрация вычислительного графа для функции $L(w_1, w_2)$](comp_graph.pdf)



Предположим, что у нас есть некоторые значения параметров $w_1, w_2$ и мы уже выполнили прямой проход (т.е. однократное распространение через вычислительный граф слева направо). Предположим также, что мы как-то сохранили все промежуточные значения $v_i$. Давайте пойдем от конца графа к началу и вычислим производные $\dfrac{\partial L}{\partial w_1}, \dfrac{\partial L}{\partial w_2}$:

## Пример обратного режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация обратного режима автоматического дифференцирования](revad1.pdf)


### Производные


$$
\dfrac{\partial L}{\partial L} = 1
$$

## Пример обратного режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация обратного режима автоматического дифференцирования](revad2.pdf)


### Производные



$$
\begin{aligned}\frac{\partial L}{\partial v_3} &= \frac{\partial L}{\partial L} \frac{\partial L}{\partial v_3}\\ &= \frac{\partial L}{\partial L} 1\end{aligned}
$$ 

## Пример обратного режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация обратного режима автоматического дифференцирования](revad3.pdf)


### Производные


$$
\begin{aligned}\frac{\partial L}{\partial v_2} &= \frac{\partial L}{\partial v_3}\frac{\partial v_3}{\partial v_2} + \frac{\partial L}{\partial L}\frac{\partial L}{\partial v_2} \\&= \frac{\partial L}{\partial v_3}\frac{1}{2\sqrt{v_2}} +  \frac{\partial L}{\partial L}1\end{aligned}
$$

## Пример обратного режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация обратного режима автоматического дифференцирования](revad4.pdf)


### Производные


$$
\begin{aligned}\frac{\partial L}{\partial v_1} &=\frac{\partial L}{\partial v_2}\frac{\partial v_2}{\partial v_1} \\ &= \frac{\partial L}{\partial v_2}w_2\end{aligned}
$$

## Пример обратного режима автоматического дифференцирования {.noframenumbering}

![Иллюстрация обратного режима автоматического дифференцирования](revad5.pdf)


### Производные


$$
\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial v_1}\frac{\partial v_1}{\partial w_1} = \frac{\partial L}{\partial v_1}\frac{1}{w_1} \qquad \qquad \frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial v_2}\frac{\partial v_2}{\partial w_2} = \frac{\partial L}{\partial v_1}v_1
$$


## Обратный (reverse) режим автоматического дифференцирования

:::{.callout-question}
Обратите внимание, что для того же количества вычислений, что и в прямом режиме, мы имеем полный вектор градиента $\nabla_w L$. Это бесплатный обед? Какова стоимость ускорения?


**Ответ** Обратите внимание, что для использования обратного режима AD вам нужно хранить все промежуточные вычисления из прямого прохода. Эта проблема может быть частично смягчена подходом контрольных точек градиента, который включает необходимые повторные вычисления некоторых промежуточных значений. Это может значительно уменьшить объем памяти большой модели машинного обучения.
:::


## Алгоритм обратного режима автоматического дифференцирования


:::: {.columns}

::: {.column width="50%"}

Предположим, что у нас есть вычислительный граф $v_i, i \in [1; N]$. Наша цель - вычислить производную выхода этого графа по всем входным переменным $w$, т.е. $\nabla_w v_N =  \left( \frac{\partial v_N}{\partial w_1}, \ldots, \frac{\partial v_N}{\partial w_d}\right)^T$. Эта идея предполагает распространение градиента функции по промежуточным переменным от конца к началу, поэтому мы можем ввести обозначение: 

$$
\overline{v_i}  = \dfrac{\partial L}{\partial v_i} = \dfrac{\partial v_N}{\partial v_i}
$$

![Иллюстрация обратного режима автоматического дифференцирования](auto_diff_reverse.pdf){width=60%}

:::

::: {.column width="50%"}

* **ПРЯМОЙ ПРОХОД** 

    Для $i = 1, \ldots, N$:

    * Вычислить и сохранить значения $v_i$ как функцию его родителей (входов) 

* **ОБРАТНЫЙ ПРОХОД**
    
    Для $i = N, \ldots, 1$:

    * Вычислить производную $\overline{v_i}$ используя обратный режим автоматического дифференцирования и информацию от всех его детей (выходов) ($x_1, \ldots, x_{t_i}$):
        $$
        \overline{v_i} = \dfrac{\partial L}{\partial v_i} = \sum_{j = 1}^{t_i} \dfrac{\partial L}{\partial x_j} \dfrac{\partial x_j}{\partial v_i}
        $$

:::

::::


## Выберите своего бойца


:::: {.columns}

::: {.column width="40%"}

![Какой режим вы бы выбрали для вычисления градиентов?](ad_choose.pdf)
:::

::: {.column width="60%"}
:::{.callout-question}
Какой из режимов AD вы бы выбрали (прямой/обратный) для следующего вычислительного графа арифметических операций? Предположим, что вам нужно вычислить якобиан $J = \left\{ \dfrac{\partial L_i}{\partial w_j} \right\}_{i,j}$
:::


**Ответ** Обратите внимание, что время вычислений в обратном режиме пропорционально количеству выходов, в то время как прямой режим работает пропорционально количеству входов. Поэтому было бы хорошей идеей рассмотреть прямой режим AD. 

:::



::::

## Выберите своего бойца

![ [$\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Autograd_and_Jax.ipynb) Этот граф хорошо иллюстрирует идею выбора между режимами. Размерность $n = 100$ фиксирована, и граф представляет время, необходимое для вычисления якобиана w.r.t. $x$ для $f(x) = Ax$](forward_vs_reverse_ad.pdf){width=88%}

## Выберите своего бойца

:::: {.columns}

::: {.column width="40%"}

![Какой режим вы бы выбрали для вычисления градиентов?](ad_mixed.pdf){width=60%}

:::

::: {.column width="60%"}
:::{.callout-question}
Какой из режимов AD вы бы выбрали (прямой/обратный) для следующего вычислительного графа арифметических операций? Предположим, что вам нужно вычислить якобиан $J = \left\{ \dfrac{\partial L_i}{\partial w_j} \right\}_{i,j}$. Обратите внимание, что $G$ - это произвольный вычислительный граф
:::

**Ответ** В общем случае невозможно сказать это без некоторого знания о конкретной структуре графа $G$. Обратите внимание, что также есть множество продвинутых подходов для смешивания прямого и обратного режимов AD, основанных на конкретной структуре $G$.

:::


::::

## Архитектура прямого распространения

:::: {.columns}

::: {.column width="40%"}

**ПРЯМОЙ ПРОХОД**

* $v_0 = x$ обычно у нас есть batch данных $x$ здесь в качестве входа.
* Для $k = 1, \ldots, t-1, t$: 
    
    * $v_k = \sigma(v_{k-1}w_k)$. Обратите внимание, что практически говоря, данные имеют размерность $x  \in \mathbb{R}^{b \times d}$, где $b$ - размер батча (для одного данного $b=1$). В то время как матрица весов $w_k$ $k$ слоя имеет размер $n_{k-1} \times n_k$, где $n_k$ - размер внутреннего представления данных. 

* $L = L(v_t)$ - вычислить функцию потерь.

**ОБРАТНЫЙ ПРОХОД**

* $v_{t+1} = L, \dfrac{\partial L}{\partial L} = 1$
* Для $k = t, t-1, \ldots, 1$: 
    
    * $\underset{b \times n_k}{\dfrac{\partial L}{\partial v_k}} = \underset{b \times n_{k+1}}{\dfrac{\partial L}{\partial v_{k+1}}} \underset{n_{k+1} \times n_k}{\dfrac{\partial v_{k+1}}{\partial v_{k}}}$
    * $\underset{b \times n_{k-1} \cdot n_k}{\dfrac{\partial L}{\partial w_k}} = \underset{b \times n_{k+1}}{\dfrac{\partial L}{\partial v_{k+1}}} \cdot  \underset{n_{k+1} \times n_{k-1} \cdot n_k}{\dfrac{\partial v_{k+1}}{\partial w_{k}}}$


:::

::: {.column width="60%"}

![Архитектура прямого распространения нейронной сети](feedforward.pdf)

:::

::::

## Произведение Гессиана на вектор без вычисления самого Гессиана



Когда вам нужна некоторая информация о кривизне функции, обычно вам нужно работать с гессианом. Однако, когда размерность задачи велика, это является вызовом. Для скалярной функции $f : \mathbb{R}^n \to \mathbb{R}$, гессиан в точке $x \in \mathbb{R}^n$ записывается как $\nabla^2 f(x)$. Тогда произведение вектора на гессиан можно оценить


$$
v \mapsto \nabla^2 f(x) \cdot v
$$


для любого вектора $v \in \mathbb{R}^n$. Мы должны использовать тождество
$$
\nabla^2 f (x) v = \nabla [x \mapsto \nabla f(x) \cdot v] = \nabla g(x),
$$
где $g(x) = \nabla f(x)^T \cdot v$ - новая векторная функция, которая умножает градиент $f$ в $x$ на вектор $v$.


```python
import jax.numpy as jnp

def hvp(f, x, v):
    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)
```

## Динамика обучения нейронной сети через спектр Гессиана и hvp ^[[Некоторые исследования в оптимизации нейронных сетей через спектр собственных значений Гессиана](https://arxiv.org/abs/1901.10159)]

![Большие отрицательные собственные значения исчезли после обучения для ResNet-32](ResNet_32_before_After.png)

## Идея Хадчинсона для оценки следа матрицы  ^[[A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines - M.F. Hutchinson, 1990](https://www.tandfonline.com/doi/abs/10.1080/03610919008812866)]

Этот пример иллюстрирует оценку следа Гессиана нейронной сети с помощью метода Hutchinson, который является алгоритмом для получения такой оценки из произведений матрицы на вектор:

Пусть $X \in \mathbb{R}^{d \times d}$ и $v \in \mathbb{R}^d$ - случайный вектор такой, что $\mathbb{E}[vv^T] = I$. Тогда,

:::: {.columns}
::: {.column width="30%"}

$$
\mathrm{Tr}(X) = \mathbb{E}[v^TXv] = \frac{1}{V}\sum_{i=1}^{V}v_i^TXv_i.
$$

:::
::: {.column width="70%"}
![[Источник](https://docs.backpack.pt/en/master/use_cases/example_trace_estimation.html)](Hutchinson_trace_est.pdf)
:::
::::


## Контрольные точки активаций

Анимация вышеуказанных подходов [\faGithub](https://github.com/cybertronai/gradient-checkpointing)

Пример использования контрольных точек градиента [\faGithub](https://colab.research.google.com/github/oseledets/dl2023/blob/main/seminars/seminar-10/Large_model_training_practice.ipynb)


Реальный пример из **GPT-2**^[[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)]: 

   * Активации в простом режиме могут занимать гораздо больше памяти: для последовательности длиной 1K и размера батча $32$, $60$ GB нужно для хранения всех промежуточных активаций. 
   * Контрольные точки активаций могут снизить потребление до 8 GB, перезапустив их (33% дополнительных вычислений) 


## Чем автоматическое дифференцирование (AD) не является:

:::: {.columns}

::: {.column width="40%"}

* AD не является конечными разностями
* AD не является символической производной
* AD не является только цепным правилом
* AD не является только обратным распространением
* AD (обратный режим) является времяэффективным и численно стабильным
* AD (обратный режим) является неэффективным в памяти (вам нужно хранить все промежуточные вычисления из прямого прохода). 

:::

::: {.column width="60%"}

![Различные подходы для взятия производных](differentiation_scheme.pdf)

:::

::::

## Дополнительные материалы

* Я рекомендую прочитать официальную книгу по Jax Autodiff. [Open In Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Autograd_and_Jax.ipynb)
* Распространение градиента через линейные наименьшие квадраты [семинар]
* Распространение градиента через SVD [семинар]
* Контрольные точки активаций [семинар]

# Итоги

## Итоги

:::: {.columns .nonincremental}

::: {.column width="50%"}

### Определения

1. Формула для приближенного вычисления производной функции $f(x): \mathbb{R}^n \to \mathbb{R}$ по $k$-ой координате с помощью метода конечных разностей.
1. Пусть $f = f(x_1(t), \ldots, x_n(t))$. Формула для вычисления $\frac{\partial f}{\partial t}$ через $\frac{\partial x_i}{\partial t}$ (Forward chain rule).
1. Пусть $L$ - функция, возвращающая скаляр, а $v_k$ - функция, возвращающая вектор $x \in \mathbb{R}^t$. Формула для вычисления $\frac{\partial L}{\partial v_k}$ через $\frac{\partial L}{\partial x_i}$ (Backward chain rule).
1. Идея Хатчинсона для оценки следа матрицы с помощью matvec операций.

:::

::: {.column width="50%"}

### Теоремы

1. Автоматическое дифференцирование. Вычислительный граф. Forward/ Backward mode (в этом вопросе нет доказательств, но необходимо подробно описать алгоритмы).

:::
# Задачи 

## Задача 1

::: {.callout-question}
Какой из режимов AD вы бы выбрали (прямой/обратный) для следующего вычислительного графа арифметических операций?
:::

![Какой режим вы бы выбрали для вычисления градиентов?](ad_choose.pdf){width=175}

## Задача 2

:::: {.columns}

::: {.column width="50%"}

Предположим, у нас есть обратимая матрица $A$ и вектор $b$, вектор $x$ является решением системы линейных уравнений $Ax = b$, то есть можно записать аналитическое решение $x = A^{-1}b$.


\
\

::: {.callout-question}

Найдите производные $\dfrac{\partial L}{\partial A}, \dfrac{\partial L}{\partial b}$.

:::

:::

::: {.column width="50%"}

![$x$ может быть найден как решение линейной системы](linear_least_squares_layer.pdf){width=200}

:::
## Задача 3

:::: {.columns}

::: {.column width="50%"}

Предположим, у нас есть прямоугольная матрица $W \in \mathbb{R}^{m \times n}$, которая имеет сингулярное разложение:

\
\

$$
W = U \Sigma V^T, \quad U^TU = I, \quad V^TV = I,
$$ 
$$
\Sigma = \text{diag}(\sigma_1, \ldots, \sigma_{\min(m,n)})
$$

\
\
Регуляризатор $R(W) = \text{tr}(\Sigma)$ в любой функции потерь стимулирует низкоранговые решения. 

::: {.callout-question}

Найдите производную $\dfrac{\partial R}{\partial W}$.

:::
:::

::: {.column width="50%"}

![Вычислительный граф для сингулярного регуляризатора](svd_singular_regularizer_comp_graph.pdf){width=200}

:::
# Задачи на дом 

1. **Бенчмаркинг вычисления произведения Гессиана на вектор (HVP) в нейронной сети через JAX** [22 балла]

    Вам дана простая нейронная сеть (MLP с несколькими скрытыми слоями с нелинейностью, такой как GELU). Параметры модели определяются весами ее слоев. Ваша задача состоит в том, чтобы сравнить различные подходы для вычисления произведения Гессиана на вектор (HVP) по отношению к функции потерь модели и изучить, как время вычисления масштабируется с ростом модели.

    **Определение модели и функции потерь:** [2/22 балла] Вот код для определения модели и функции потерь. Напишите метод `get_params()`, который возвращает сглаженный вектор всех весов модели.

    ```python
    import jax
    import jax.numpy as jnp
    import time
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    from tqdm.auto import tqdm
    from jax.nn import gelu

    # Определение MLP модели
    class MLP:
        def __init__(self, key, layer_sizes):
            self.layer_sizes = layer_sizes
            keys = jax.random.split(key, len(layer_sizes) - 1)
            self.weights = [
                jax.random.normal(k, (layer_sizes[i], layer_sizes[i + 1]))
                for i, k in enumerate(keys)
                ]
        
        def forward(self, x):
            for w in self.weights[:-1]:
                x = gelu(jnp.dot(x, w))
            return jnp.dot(x, self.weights[-1])
        
        def get_params(self):
            ### YOUR CODE HERE ###
            return None
    ```

    **Hessian and HVP Implementations:** [2/22 points] Write a function 
    
    ```python
    # Функция для вычисления Гессиана
    def calculate_hessian(model, params):
        def loss_fn(p):
            x = jnp.ones((1, model.layer_sizes[0]))  # Заглушка входа
            return jnp.sum(model.forward(x))
        
        ### YOUR CODE HERE ###
        #hessian_fn =           
        return hessian_fn(params)
    ```
    который вычисляет полный Гессиан $H$ функции потерь по отношению к параметрам модели с использованием автоматического дифференцирования JAX.
    
    **Naive HVP через полный Гессиан:** [2/22 балла] Напишите функцию ```naive_hvp(hessian, vector)```, которая, учитывая предварительно вычисленный Гессиан $H$ и вектор $v$ (той же формы, что и параметры), вычисляет произведение Гессиана на вектор с использованием прямого матрично-векторного умножения.

    **Эффективное HVP с использованием Autograd:** [4/22 балла] Напишите функцию 
    ```python
     def hvp(f, x, v):
         return jax.grad(lambda x: jnp.vdot(jax.grad(f)(x), v))(x)
    ```
    которая непосредственно вычисляет HVP без явного формирования полного Гессиана. Это использует возможности обратного режима дифференцирования JAX.
     
    **Эксперимент по времени:** Рассмотрим семейство моделей с увеличивающимся количеством скрытых слоев. 

    ```python
    ns = np.linspace(50, 1000, 15, dtype=int)  # The number of hidden layers
    num_runs = 10  # The number of runs for averaging
    ```

    Для каждой конфигурации модели:
    * Сгенерируйте модель и извлеките ее вектор параметров.
    * Сгенерируйте случайный вектор $v$ той же размерности, что и параметры.
    * Измерьте (не забудьте использовать ```.block_until_ready()``` для обеспечения точного измерения времени и правильной синхронизации) следующее:
       1. **Общее время (полный Гессиан + Naive HVP):** Общее время, необходимое для вычисления полного Гессиана и затем выполнения матрично-векторного умножения.
       2. **Время Naive HVP (без вычисления Гессиана):** Время, необходимое для выполнения матрично-векторного умножения, учитывая предварительно вычисленный Гессиан.
       3. **Время эффективного HVP:** Время, необходимое для вычисления HVP с использованием функции на основе autograd.
    * Повторите каждое измерение времени для фиксированного количества запусков (например, 10 запусков) и запишите как среднее, так и стандартное отклонение времени вычисления. 
    
    **Визуализация и анализ:** [12/22 балла]
    * Постройте график результатов времени для трех методов на одном графике. Для каждого метода отобразите ошибки, соответствующие стандартному отклонению по запускам.
    * Четко обозначьте оси (например, "Количество слоев" против "Время вычисления (секунды)") и включите легенду, указывающую, какая кривая соответствует какому методу.
    * Анализируйте масштабирование. Попробуйте аналитически вывести масштабирование методов и сравнить его с экспериментальными результатами.

1. [15 баллов] Мы можем использовать автоматическое дифференцирование не только для вычисления необходимых градиентов, но и для настройки гиперпараметров алгоритма, таких как скорость обучения в градиентном спуске (с градиентным спуском 🤯). Предположим, у нас есть следующая функция $f(x) = \frac{1}{2}\Vert x\Vert^2$, выберите случайную точку $x_0 \in \mathbb{B}^{1000} = \{0 \leq x_i \leq 1 \mid \forall i\}$. Рассмотрим $10$ шагов градиентного спуска, начиная с точки $x_0$:
    $$
    x_{k+1} = x_k - \alpha_k \nabla f(x_k)
    $$
    Ваша цель в этой задаче состоит в том, чтобы написать функцию, которая принимает $10$ скалярных значений $\alpha_i$ и возвращает результат градиентного спуска на функции $L = f(x_{10})$. И оптимизируйте эту функцию с помощью градиентного спуска на $\alpha \in \mathbb{R}^{10}$. Предположим, что каждый из $10$ компонентов $\alpha$ равномерно распределен на $[0; 0.1]$.
    $$
    \alpha_{k+1} = \alpha_k - \beta \frac{\partial L}{\partial \alpha}
    $$
    Выберите любую константу $\beta$ и количество шагов, которое вам нужно. Опишите полученные результаты. Как вы поймете, что полученный график ($\alpha \in \mathbb{R}^{10}$) стал лучше, чем был в начале? Как вы проверите локальную оптимальность этой задачи численно? 

::::